{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1cxGyZhunHDb"
   },
   "source": [
    "# Rede Neural Convolucional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZPp23HYnHDf"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 909,
     "status": "ok",
     "timestamp": 1555804000215,
     "user": {
      "displayName": "Ihan Bender",
      "photoUrl": "",
      "userId": "09326755177320215938"
     },
     "user_tz": 180
    },
    "id": "cGMOJq80nHDs",
    "outputId": "738ead35-2010-4b5a-dc8a-84c0f90d4908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2723,
     "status": "ok",
     "timestamp": 1555804002045,
     "user": {
      "displayName": "Ihan Bender",
      "photoUrl": "",
      "userId": "09326755177320215938"
     },
     "user_tz": 180
    },
    "id": "8XhxVpVFnHD3",
    "outputId": "e79a4a64-9185-4711-f478-eb7f963ce008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Carregar os datasets\n",
    "\n",
    "transform=transforms.Compose([\n",
    "#     transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "dataset_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3984,
     "status": "ok",
     "timestamp": 1555804003324,
     "user": {
      "displayName": "Ihan Bender",
      "photoUrl": "",
      "userId": "09326755177320215938"
     },
     "user_tz": 180
    },
    "id": "QN7Lbm84nHD8",
    "outputId": "b41c6437-9d90-4c6e-f3cd-da0784612dc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "train = DataLoader(dataset=dataset_train, shuffle=True, batch_size=200)\n",
    "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=1000)\n",
    "test, validation = [], []\n",
    "\n",
    "for index, element in enumerate(test_loader):\n",
    "  if index/len(test_loader) < 0.49:\n",
    "    test.append(element)\n",
    "  else:\n",
    "    validation.append(element)\n",
    "\n",
    "test_size, validation_size = len(test) * 1000, len(validation) * 1000\n",
    "print(test_size, validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wb6DKramnHEA"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, 2)\n",
    "        self.conv1_bn = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 32, 2)\n",
    "        self.conv2_bn = nn.BatchNorm2d(32)\n",
    "        self.fc1 = nn.Linear(32*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1_bn(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.conv2_bn(self.conv2(x))))\n",
    "        x = x.view(-1, 32*7*7)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8181,
     "status": "ok",
     "timestamp": 1555804007565,
     "user": {
      "displayName": "Ihan Bender",
      "photoUrl": "",
      "userId": "09326755177320215938"
     },
     "user_tz": 180
    },
    "id": "NuYrQiH3nHEE",
    "outputId": "e19f8341-0c30-4f2d-d904-8eaa95a65533",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv1_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(64, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=1568, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3, weight_decay=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tWUWf9p8nHEJ"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "\n",
    "def one_hot(results):\n",
    "    results = results.cpu().detach().numpy().tolist()\n",
    "    return results.index(max(results))\n",
    "\n",
    "def train_model(model, epochs, train, test):\n",
    "    global test_size\n",
    "    best_model, train_losses, test_acc = model, [], []\n",
    "    min_error = sys.float_info.max\n",
    "    \n",
    "    train_inputs, train_labels = [], []\n",
    "    for _, (inputs, labels) in enumerate(train):\n",
    "        train_inputs.append(inputs.to(device))\n",
    "        train_labels.append(labels.to(device))\n",
    "    \n",
    "    test_inputs, test_labels = [], []\n",
    "    for _, (inputs, labels) in enumerate(test):\n",
    "        test_inputs.append(inputs.to(device))\n",
    "        test_labels.append(labels.to(device))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Set\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(zip(train_inputs, train_labels), 0):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(inputs)\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / (50000/200))\n",
    "        \n",
    "        # Test set\n",
    "        hits = 0\n",
    "        for i, (inputs, labels) in enumerate(zip(test_inputs, test_labels), 0):\n",
    "            y_pred = model(inputs)\n",
    "            for i, (pred, label) in enumerate(zip(y_pred, labels)):\n",
    "                if one_hot(pred)==label.item():\n",
    "                    hits+=1\n",
    "                    \n",
    "        test_acc.append(hits / test_size * 100)\n",
    "        if test_acc[-1] <= min_error:\n",
    "            min_error = test_acc[-1]\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        print(\"Loss at epoch [\"+ str(epoch + 1) +\"]: \"+ str(train_losses[-1]) +\" (Train Set)\")\n",
    "        print(\"Accuracy at epoch [\" + str(epoch + 1) + \"]:\"+str(test_acc[-1])+\"% (Test Set)\")\n",
    "        print('\\n')    \n",
    "            \n",
    "    return best_model, train_losses, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCcA5tlMnHEM"
   },
   "source": [
    "# Trainamento do Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 27217
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 973089,
     "status": "ok",
     "timestamp": 1555804972489,
     "user": {
      "displayName": "Ihan Bender",
      "photoUrl": "",
      "userId": "09326755177320215938"
     },
     "user_tz": 180
    },
    "id": "V6K4tfJ4nHEN",
    "outputId": "3eb00289-8cad-4626-d7b8-b600a821c195",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch [1]: 2.2982462301254274 (Train Set)\n",
      "Accuracy at epoch [1]:16.580000000000002% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [2]: 2.2735381841659548 (Train Set)\n",
      "Accuracy at epoch [2]:20.580000000000002% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [3]: 2.239522029876709 (Train Set)\n",
      "Accuracy at epoch [3]:24.16% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [4]: 2.191661123275757 (Train Set)\n",
      "Accuracy at epoch [4]:28.02% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [5]: 2.1426077003479005 (Train Set)\n",
      "Accuracy at epoch [5]:30.620000000000005% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [6]: 2.09576078414917 (Train Set)\n",
      "Accuracy at epoch [6]:32.48% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [7]: 2.052918100833893 (Train Set)\n",
      "Accuracy at epoch [7]:33.800000000000004% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [8]: 2.010741810798645 (Train Set)\n",
      "Accuracy at epoch [8]:34.96% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [9]: 1.9676825590133666 (Train Set)\n",
      "Accuracy at epoch [9]:35.58% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [10]: 1.9280366883277893 (Train Set)\n",
      "Accuracy at epoch [10]:36.26% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [11]: 1.8838817391395568 (Train Set)\n",
      "Accuracy at epoch [11]:37.36% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [12]: 1.8280602922439575 (Train Set)\n",
      "Accuracy at epoch [12]:38.1% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [13]: 1.7673371844291688 (Train Set)\n",
      "Accuracy at epoch [13]:40.400000000000006% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [14]: 1.7184090809822083 (Train Set)\n",
      "Accuracy at epoch [14]:42.36% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [15]: 1.676267622947693 (Train Set)\n",
      "Accuracy at epoch [15]:43.580000000000005% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [16]: 1.6393429555892944 (Train Set)\n",
      "Accuracy at epoch [16]:44.879999999999995% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [17]: 1.6063205304145813 (Train Set)\n",
      "Accuracy at epoch [17]:46.18% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [18]: 1.5766343474388123 (Train Set)\n",
      "Accuracy at epoch [18]:46.739999999999995% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [19]: 1.549968066215515 (Train Set)\n",
      "Accuracy at epoch [19]:47.56% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [20]: 1.5258615579605102 (Train Set)\n",
      "Accuracy at epoch [20]:48.32% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [21]: 1.5038826818466187 (Train Set)\n",
      "Accuracy at epoch [21]:48.9% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [22]: 1.4837286100387572 (Train Set)\n",
      "Accuracy at epoch [22]:49.7% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [23]: 1.4651112909317017 (Train Set)\n",
      "Accuracy at epoch [23]:50.06% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [24]: 1.4478597841262817 (Train Set)\n",
      "Accuracy at epoch [24]:50.62% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [25]: 1.431829043865204 (Train Set)\n",
      "Accuracy at epoch [25]:51.25999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [26]: 1.41682461643219 (Train Set)\n",
      "Accuracy at epoch [26]:52.019999999999996% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [27]: 1.4026932201385498 (Train Set)\n",
      "Accuracy at epoch [27]:52.44% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [28]: 1.3893124341964722 (Train Set)\n",
      "Accuracy at epoch [28]:52.900000000000006% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [29]: 1.376561475276947 (Train Set)\n",
      "Accuracy at epoch [29]:53.54% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [30]: 1.3643887405395507 (Train Set)\n",
      "Accuracy at epoch [30]:54.02% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [31]: 1.352739001274109 (Train Set)\n",
      "Accuracy at epoch [31]:54.300000000000004% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [32]: 1.3415537257194519 (Train Set)\n",
      "Accuracy at epoch [32]:54.7% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [33]: 1.3307812390327454 (Train Set)\n",
      "Accuracy at epoch [33]:55.17999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [34]: 1.3203557620048523 (Train Set)\n",
      "Accuracy at epoch [34]:55.58% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [35]: 1.3102592339515686 (Train Set)\n",
      "Accuracy at epoch [35]:55.94% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [36]: 1.3004450335502624 (Train Set)\n",
      "Accuracy at epoch [36]:56.18% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [37]: 1.2908945064544677 (Train Set)\n",
      "Accuracy at epoch [37]:56.54% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [38]: 1.2816109285354613 (Train Set)\n",
      "Accuracy at epoch [38]:56.98% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [39]: 1.272553659439087 (Train Set)\n",
      "Accuracy at epoch [39]:57.08% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [40]: 1.263698058128357 (Train Set)\n",
      "Accuracy at epoch [40]:57.379999999999995% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [41]: 1.2550191922187806 (Train Set)\n",
      "Accuracy at epoch [41]:57.64% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [42]: 1.2465292868614197 (Train Set)\n",
      "Accuracy at epoch [42]:57.98% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [43]: 1.2381921434402465 (Train Set)\n",
      "Accuracy at epoch [43]:58.26% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [44]: 1.230029103755951 (Train Set)\n",
      "Accuracy at epoch [44]:58.52% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [45]: 1.2219854745864869 (Train Set)\n",
      "Accuracy at epoch [45]:58.720000000000006% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [46]: 1.2140470612049103 (Train Set)\n",
      "Accuracy at epoch [46]:58.76% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [47]: 1.206206446647644 (Train Set)\n",
      "Accuracy at epoch [47]:58.96% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [48]: 1.1984742925167085 (Train Set)\n",
      "Accuracy at epoch [48]:59.31999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [49]: 1.1908210053443908 (Train Set)\n",
      "Accuracy at epoch [49]:59.64% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [50]: 1.1832438225746156 (Train Set)\n",
      "Accuracy at epoch [50]:60.0% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [51]: 1.1757887575626373 (Train Set)\n",
      "Accuracy at epoch [51]:60.199999999999996% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [52]: 1.1683816478252411 (Train Set)\n",
      "Accuracy at epoch [52]:60.38% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [53]: 1.1610912823677062 (Train Set)\n",
      "Accuracy at epoch [53]:60.68% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [54]: 1.153883080482483 (Train Set)\n",
      "Accuracy at epoch [54]:60.660000000000004% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [55]: 1.1467976098060608 (Train Set)\n",
      "Accuracy at epoch [55]:60.72% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [56]: 1.139816341161728 (Train Set)\n",
      "Accuracy at epoch [56]:60.88% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [57]: 1.1329456334114074 (Train Set)\n",
      "Accuracy at epoch [57]:60.980000000000004% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [58]: 1.1261854860782623 (Train Set)\n",
      "Accuracy at epoch [58]:61.3% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [59]: 1.1195562396049499 (Train Set)\n",
      "Accuracy at epoch [59]:61.519999999999996% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [60]: 1.113052606344223 (Train Set)\n",
      "Accuracy at epoch [60]:61.63999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [61]: 1.1066999170780183 (Train Set)\n",
      "Accuracy at epoch [61]:61.82% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [62]: 1.1004841735363007 (Train Set)\n",
      "Accuracy at epoch [62]:61.919999999999995% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [63]: 1.0944128408432008 (Train Set)\n",
      "Accuracy at epoch [63]:62.2% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [64]: 1.0884900665283204 (Train Set)\n",
      "Accuracy at epoch [64]:62.3% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [65]: 1.082743812084198 (Train Set)\n",
      "Accuracy at epoch [65]:62.46000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [66]: 1.0771408829689026 (Train Set)\n",
      "Accuracy at epoch [66]:62.7% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [67]: 1.0716678733825684 (Train Set)\n",
      "Accuracy at epoch [67]:62.739999999999995% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [68]: 1.0663565752506257 (Train Set)\n",
      "Accuracy at epoch [68]:62.88% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [69]: 1.0611898310184478 (Train Set)\n",
      "Accuracy at epoch [69]:63.06% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [70]: 1.05615536570549 (Train Set)\n",
      "Accuracy at epoch [70]:63.160000000000004% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [71]: 1.05125413107872 (Train Set)\n",
      "Accuracy at epoch [71]:63.32% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [72]: 1.046482514858246 (Train Set)\n",
      "Accuracy at epoch [72]:63.56% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [73]: 1.0418248958587646 (Train Set)\n",
      "Accuracy at epoch [73]:63.800000000000004% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [74]: 1.037291645526886 (Train Set)\n",
      "Accuracy at epoch [74]:63.92% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [75]: 1.0328556406497955 (Train Set)\n",
      "Accuracy at epoch [75]:64.08% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [76]: 1.0285516757965087 (Train Set)\n",
      "Accuracy at epoch [76]:64.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [77]: 1.0243496026992798 (Train Set)\n",
      "Accuracy at epoch [77]:64.4% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [78]: 1.020242041349411 (Train Set)\n",
      "Accuracy at epoch [78]:64.38000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [79]: 1.0162331583499908 (Train Set)\n",
      "Accuracy at epoch [79]:64.58% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [80]: 1.0123177089691162 (Train Set)\n",
      "Accuracy at epoch [80]:64.60000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [81]: 1.0084999651908875 (Train Set)\n",
      "Accuracy at epoch [81]:64.84% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [82]: 1.0047910070419313 (Train Set)\n",
      "Accuracy at epoch [82]:64.92% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [83]: 1.0011653413772583 (Train Set)\n",
      "Accuracy at epoch [83]:65.0% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [84]: 0.9976172535419464 (Train Set)\n",
      "Accuracy at epoch [84]:65.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [85]: 0.9941502659320831 (Train Set)\n",
      "Accuracy at epoch [85]:65.3% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [86]: 0.9907480285167695 (Train Set)\n",
      "Accuracy at epoch [86]:65.42% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [87]: 0.9874026012420655 (Train Set)\n",
      "Accuracy at epoch [87]:65.56% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [88]: 0.9841349024772644 (Train Set)\n",
      "Accuracy at epoch [88]:65.7% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [89]: 0.9809377498626709 (Train Set)\n",
      "Accuracy at epoch [89]:65.74% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [90]: 0.977792774438858 (Train Set)\n",
      "Accuracy at epoch [90]:65.78% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [91]: 0.9746977667808533 (Train Set)\n",
      "Accuracy at epoch [91]:65.92% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [92]: 0.9716673107147217 (Train Set)\n",
      "Accuracy at epoch [92]:66.02% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [93]: 0.9686971867084503 (Train Set)\n",
      "Accuracy at epoch [93]:66.10000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [94]: 0.9657863907814026 (Train Set)\n",
      "Accuracy at epoch [94]:66.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [95]: 0.9629245791435241 (Train Set)\n",
      "Accuracy at epoch [95]:66.4% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [96]: 0.960091465473175 (Train Set)\n",
      "Accuracy at epoch [96]:66.47999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [97]: 0.9573021755218506 (Train Set)\n",
      "Accuracy at epoch [97]:66.53999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [98]: 0.9545792806148529 (Train Set)\n",
      "Accuracy at epoch [98]:66.66% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [99]: 0.9518857066631317 (Train Set)\n",
      "Accuracy at epoch [99]:66.7% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [100]: 0.9492152149677276 (Train Set)\n",
      "Accuracy at epoch [100]:66.88% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [101]: 0.9465838575363159 (Train Set)\n",
      "Accuracy at epoch [101]:66.94% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [102]: 0.9439965913295746 (Train Set)\n",
      "Accuracy at epoch [102]:67.08% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [103]: 0.9414481468200684 (Train Set)\n",
      "Accuracy at epoch [103]:67.10000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [104]: 0.9389411361217499 (Train Set)\n",
      "Accuracy at epoch [104]:67.06% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [105]: 0.9364666163921356 (Train Set)\n",
      "Accuracy at epoch [105]:67.06% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [106]: 0.9340099930763245 (Train Set)\n",
      "Accuracy at epoch [106]:67.06% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [107]: 0.9315986862182617 (Train Set)\n",
      "Accuracy at epoch [107]:67.17999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [108]: 0.9292159690856934 (Train Set)\n",
      "Accuracy at epoch [108]:67.24% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [109]: 0.9268747801780701 (Train Set)\n",
      "Accuracy at epoch [109]:67.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [110]: 0.9245581707954407 (Train Set)\n",
      "Accuracy at epoch [110]:67.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [111]: 0.9222651290893554 (Train Set)\n",
      "Accuracy at epoch [111]:67.32000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [112]: 0.9199871339797974 (Train Set)\n",
      "Accuracy at epoch [112]:67.32000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [113]: 0.9177420651912689 (Train Set)\n",
      "Accuracy at epoch [113]:67.32000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [114]: 0.9155309081077576 (Train Set)\n",
      "Accuracy at epoch [114]:67.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [115]: 0.9133296446800232 (Train Set)\n",
      "Accuracy at epoch [115]:67.24% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [116]: 0.9111641747951508 (Train Set)\n",
      "Accuracy at epoch [116]:67.34% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [117]: 0.9090153369903564 (Train Set)\n",
      "Accuracy at epoch [117]:67.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [118]: 0.9069100129604339 (Train Set)\n",
      "Accuracy at epoch [118]:67.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [119]: 0.9048355774879455 (Train Set)\n",
      "Accuracy at epoch [119]:67.32000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [120]: 0.902777651309967 (Train Set)\n",
      "Accuracy at epoch [120]:67.36% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [121]: 0.900736665725708 (Train Set)\n",
      "Accuracy at epoch [121]:67.4% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [122]: 0.8987146832942963 (Train Set)\n",
      "Accuracy at epoch [122]:67.44% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [123]: 0.8967203180789948 (Train Set)\n",
      "Accuracy at epoch [123]:67.4% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [124]: 0.8947261896133423 (Train Set)\n",
      "Accuracy at epoch [124]:67.5% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [125]: 0.8927598099708557 (Train Set)\n",
      "Accuracy at epoch [125]:67.52% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [126]: 0.890823725938797 (Train Set)\n",
      "Accuracy at epoch [126]:67.5% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [127]: 0.8888917775154114 (Train Set)\n",
      "Accuracy at epoch [127]:67.56% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [128]: 0.8869746603965759 (Train Set)\n",
      "Accuracy at epoch [128]:67.5% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [129]: 0.885074642419815 (Train Set)\n",
      "Accuracy at epoch [129]:67.56% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [130]: 0.8831968607902527 (Train Set)\n",
      "Accuracy at epoch [130]:67.62% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [131]: 0.8813402271270752 (Train Set)\n",
      "Accuracy at epoch [131]:67.72% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [132]: 0.8795019450187683 (Train Set)\n",
      "Accuracy at epoch [132]:67.72% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [133]: 0.8776692926883698 (Train Set)\n",
      "Accuracy at epoch [133]:67.78% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [134]: 0.8758492195606231 (Train Set)\n",
      "Accuracy at epoch [134]:67.82000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [135]: 0.8740299022197724 (Train Set)\n",
      "Accuracy at epoch [135]:67.9% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [136]: 0.8722488634586334 (Train Set)\n",
      "Accuracy at epoch [136]:67.94% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [137]: 0.8704815196990967 (Train Set)\n",
      "Accuracy at epoch [137]:68.0% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [138]: 0.8687183096408844 (Train Set)\n",
      "Accuracy at epoch [138]:68.10000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [139]: 0.866983316898346 (Train Set)\n",
      "Accuracy at epoch [139]:68.16% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [140]: 0.8652498614788056 (Train Set)\n",
      "Accuracy at epoch [140]:68.28% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [141]: 0.8635230855941772 (Train Set)\n",
      "Accuracy at epoch [141]:68.26% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [142]: 0.8618155667781829 (Train Set)\n",
      "Accuracy at epoch [142]:68.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [143]: 0.8601234889030457 (Train Set)\n",
      "Accuracy at epoch [143]:68.16% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [144]: 0.858439884185791 (Train Set)\n",
      "Accuracy at epoch [144]:68.26% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [145]: 0.8567849307060241 (Train Set)\n",
      "Accuracy at epoch [145]:68.26% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [146]: 0.855129450082779 (Train Set)\n",
      "Accuracy at epoch [146]:68.30000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [147]: 0.8534908201694489 (Train Set)\n",
      "Accuracy at epoch [147]:68.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [148]: 0.8518595476150512 (Train Set)\n",
      "Accuracy at epoch [148]:68.28% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [149]: 0.8502494781017303 (Train Set)\n",
      "Accuracy at epoch [149]:68.26% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [150]: 0.8486520273685455 (Train Set)\n",
      "Accuracy at epoch [150]:68.28% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [151]: 0.8470736582279206 (Train Set)\n",
      "Accuracy at epoch [151]:68.32000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [152]: 0.8454938604831695 (Train Set)\n",
      "Accuracy at epoch [152]:68.30000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [153]: 0.8439345335960389 (Train Set)\n",
      "Accuracy at epoch [153]:68.36% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [154]: 0.8423756215572357 (Train Set)\n",
      "Accuracy at epoch [154]:68.46% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [155]: 0.8408426043987274 (Train Set)\n",
      "Accuracy at epoch [155]:68.4% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [156]: 0.8393011965751648 (Train Set)\n",
      "Accuracy at epoch [156]:68.46% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [157]: 0.8377810201644897 (Train Set)\n",
      "Accuracy at epoch [157]:68.46% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [158]: 0.8362699406147003 (Train Set)\n",
      "Accuracy at epoch [158]:68.52000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [159]: 0.8347858550548554 (Train Set)\n",
      "Accuracy at epoch [159]:68.56% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [160]: 0.833286761045456 (Train Set)\n",
      "Accuracy at epoch [160]:68.52000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [161]: 0.8318015139102936 (Train Set)\n",
      "Accuracy at epoch [161]:68.46% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [162]: 0.8303361678123474 (Train Set)\n",
      "Accuracy at epoch [162]:68.52000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [163]: 0.8288796496391296 (Train Set)\n",
      "Accuracy at epoch [163]:68.47999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [164]: 0.8274236483573914 (Train Set)\n",
      "Accuracy at epoch [164]:68.62% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [165]: 0.8259833195209503 (Train Set)\n",
      "Accuracy at epoch [165]:68.78% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [166]: 0.8245328066349029 (Train Set)\n",
      "Accuracy at epoch [166]:68.78% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [167]: 0.823101312160492 (Train Set)\n",
      "Accuracy at epoch [167]:68.82000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [168]: 0.8216883049011231 (Train Set)\n",
      "Accuracy at epoch [168]:68.86% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [169]: 0.8202815868854523 (Train Set)\n",
      "Accuracy at epoch [169]:68.84% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [170]: 0.8188557796478272 (Train Set)\n",
      "Accuracy at epoch [170]:68.88% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [171]: 0.8174539468288422 (Train Set)\n",
      "Accuracy at epoch [171]:68.89999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [172]: 0.8160635950565338 (Train Set)\n",
      "Accuracy at epoch [172]:68.89999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [173]: 0.8146993727684021 (Train Set)\n",
      "Accuracy at epoch [173]:68.84% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [174]: 0.8133173468112945 (Train Set)\n",
      "Accuracy at epoch [174]:68.92% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [175]: 0.8119427897930145 (Train Set)\n",
      "Accuracy at epoch [175]:68.86% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [176]: 0.8105742025375366 (Train Set)\n",
      "Accuracy at epoch [176]:68.89999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [177]: 0.8092210440635681 (Train Set)\n",
      "Accuracy at epoch [177]:68.96% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [178]: 0.8078737018108368 (Train Set)\n",
      "Accuracy at epoch [178]:69.08% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [179]: 0.8065368616580963 (Train Set)\n",
      "Accuracy at epoch [179]:69.02000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [180]: 0.8052039651870727 (Train Set)\n",
      "Accuracy at epoch [180]:69.12% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [181]: 0.8038946220874786 (Train Set)\n",
      "Accuracy at epoch [181]:69.14% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [182]: 0.802569605588913 (Train Set)\n",
      "Accuracy at epoch [182]:69.1% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [183]: 0.8012505221366882 (Train Set)\n",
      "Accuracy at epoch [183]:69.12% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [184]: 0.7999386703968048 (Train Set)\n",
      "Accuracy at epoch [184]:69.16% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [185]: 0.798636234998703 (Train Set)\n",
      "Accuracy at epoch [185]:69.16% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [186]: 0.797350343465805 (Train Set)\n",
      "Accuracy at epoch [186]:69.26% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [187]: 0.7960526375770569 (Train Set)\n",
      "Accuracy at epoch [187]:69.3% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [188]: 0.7947778072357178 (Train Set)\n",
      "Accuracy at epoch [188]:69.44% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [189]: 0.7934842784404754 (Train Set)\n",
      "Accuracy at epoch [189]:69.46% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [190]: 0.7922151184082031 (Train Set)\n",
      "Accuracy at epoch [190]:69.46% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [191]: 0.7909356977939606 (Train Set)\n",
      "Accuracy at epoch [191]:69.52000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [192]: 0.7896755967140198 (Train Set)\n",
      "Accuracy at epoch [192]:69.56% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [193]: 0.7884230000972747 (Train Set)\n",
      "Accuracy at epoch [193]:69.56% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [194]: 0.7871699078083039 (Train Set)\n",
      "Accuracy at epoch [194]:69.54% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [195]: 0.7859280214309693 (Train Set)\n",
      "Accuracy at epoch [195]:69.54% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [196]: 0.7846924107074738 (Train Set)\n",
      "Accuracy at epoch [196]:69.54% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [197]: 0.7834498517513275 (Train Set)\n",
      "Accuracy at epoch [197]:69.5% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [198]: 0.7822175934314728 (Train Set)\n",
      "Accuracy at epoch [198]:69.56% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [199]: 0.78099427318573 (Train Set)\n",
      "Accuracy at epoch [199]:69.6% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [200]: 0.7797865614891052 (Train Set)\n",
      "Accuracy at epoch [200]:69.64% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [201]: 0.7785836834907531 (Train Set)\n",
      "Accuracy at epoch [201]:69.64% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [202]: 0.7773682610988617 (Train Set)\n",
      "Accuracy at epoch [202]:69.64% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [203]: 0.7761826856136322 (Train Set)\n",
      "Accuracy at epoch [203]:69.62% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [204]: 0.7749877829551697 (Train Set)\n",
      "Accuracy at epoch [204]:69.62% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [205]: 0.7737896666526795 (Train Set)\n",
      "Accuracy at epoch [205]:69.66% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [206]: 0.7726067311763763 (Train Set)\n",
      "Accuracy at epoch [206]:69.66% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [207]: 0.7714437341690064 (Train Set)\n",
      "Accuracy at epoch [207]:69.69999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [208]: 0.7702617313861847 (Train Set)\n",
      "Accuracy at epoch [208]:69.69999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [209]: 0.7690653722286225 (Train Set)\n",
      "Accuracy at epoch [209]:69.62% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [210]: 0.7678831558227539 (Train Set)\n",
      "Accuracy at epoch [210]:69.72% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [211]: 0.7667149744033813 (Train Set)\n",
      "Accuracy at epoch [211]:69.74000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [212]: 0.7655585219860077 (Train Set)\n",
      "Accuracy at epoch [212]:69.72% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [213]: 0.7643837516307831 (Train Set)\n",
      "Accuracy at epoch [213]:69.69999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [214]: 0.7632095191478729 (Train Set)\n",
      "Accuracy at epoch [214]:69.72% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [215]: 0.7620577056407929 (Train Set)\n",
      "Accuracy at epoch [215]:69.76% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [216]: 0.7609252016544342 (Train Set)\n",
      "Accuracy at epoch [216]:69.86% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [217]: 0.7597782733440399 (Train Set)\n",
      "Accuracy at epoch [217]:69.84% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [218]: 0.7586306762695313 (Train Set)\n",
      "Accuracy at epoch [218]:69.84% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [219]: 0.7574973773956298 (Train Set)\n",
      "Accuracy at epoch [219]:69.84% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [220]: 0.7563832099437714 (Train Set)\n",
      "Accuracy at epoch [220]:69.88% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [221]: 0.7552440564632416 (Train Set)\n",
      "Accuracy at epoch [221]:69.94% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [222]: 0.7541147565841675 (Train Set)\n",
      "Accuracy at epoch [222]:69.92% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [223]: 0.7529957647323609 (Train Set)\n",
      "Accuracy at epoch [223]:69.94% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [224]: 0.7518769714832306 (Train Set)\n",
      "Accuracy at epoch [224]:70.0% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [225]: 0.7507635912895203 (Train Set)\n",
      "Accuracy at epoch [225]:69.96% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [226]: 0.7496303355693817 (Train Set)\n",
      "Accuracy at epoch [226]:70.06% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [227]: 0.7485140318870545 (Train Set)\n",
      "Accuracy at epoch [227]:70.0% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [228]: 0.7474275133609771 (Train Set)\n",
      "Accuracy at epoch [228]:69.94% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [229]: 0.7463208734989166 (Train Set)\n",
      "Accuracy at epoch [229]:70.0% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [230]: 0.7452034912109375 (Train Set)\n",
      "Accuracy at epoch [230]:70.08% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [231]: 0.7441065411567688 (Train Set)\n",
      "Accuracy at epoch [231]:70.1% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [232]: 0.743009806394577 (Train Set)\n",
      "Accuracy at epoch [232]:70.08% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [233]: 0.7419205985069275 (Train Set)\n",
      "Accuracy at epoch [233]:70.17999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [234]: 0.7408524870872497 (Train Set)\n",
      "Accuracy at epoch [234]:70.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [235]: 0.7397684624195099 (Train Set)\n",
      "Accuracy at epoch [235]:70.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [236]: 0.7387001478672027 (Train Set)\n",
      "Accuracy at epoch [236]:70.16% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [237]: 0.7376329095363617 (Train Set)\n",
      "Accuracy at epoch [237]:70.24000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [238]: 0.7365579934120178 (Train Set)\n",
      "Accuracy at epoch [238]:70.26% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [239]: 0.7354969069957733 (Train Set)\n",
      "Accuracy at epoch [239]:70.3% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [240]: 0.7344601347446441 (Train Set)\n",
      "Accuracy at epoch [240]:70.28% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [241]: 0.7334086635112762 (Train Set)\n",
      "Accuracy at epoch [241]:70.32000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [242]: 0.7323607301712036 (Train Set)\n",
      "Accuracy at epoch [242]:70.26% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [243]: 0.731314228773117 (Train Set)\n",
      "Accuracy at epoch [243]:70.28% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [244]: 0.7302722172737122 (Train Set)\n",
      "Accuracy at epoch [244]:70.34% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [245]: 0.7292372486591339 (Train Set)\n",
      "Accuracy at epoch [245]:70.28% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [246]: 0.7282003247737885 (Train Set)\n",
      "Accuracy at epoch [246]:70.3% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [247]: 0.7271633310317993 (Train Set)\n",
      "Accuracy at epoch [247]:70.32000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [248]: 0.7261371448040008 (Train Set)\n",
      "Accuracy at epoch [248]:70.32000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [249]: 0.7250957176685333 (Train Set)\n",
      "Accuracy at epoch [249]:70.32000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [250]: 0.7240707130432129 (Train Set)\n",
      "Accuracy at epoch [250]:70.39999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [251]: 0.7230576012134552 (Train Set)\n",
      "Accuracy at epoch [251]:70.38% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [252]: 0.7220408189296722 (Train Set)\n",
      "Accuracy at epoch [252]:70.34% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [253]: 0.721020388841629 (Train Set)\n",
      "Accuracy at epoch [253]:70.46% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [254]: 0.720004787683487 (Train Set)\n",
      "Accuracy at epoch [254]:70.44% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [255]: 0.7189938130378724 (Train Set)\n",
      "Accuracy at epoch [255]:70.46% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [256]: 0.7179843745231629 (Train Set)\n",
      "Accuracy at epoch [256]:70.56% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [257]: 0.7169869577884674 (Train Set)\n",
      "Accuracy at epoch [257]:70.54% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [258]: 0.7159827921390534 (Train Set)\n",
      "Accuracy at epoch [258]:70.56% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [259]: 0.7149927082061768 (Train Set)\n",
      "Accuracy at epoch [259]:70.52000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [260]: 0.713977735042572 (Train Set)\n",
      "Accuracy at epoch [260]:70.54% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [261]: 0.7129828584194183 (Train Set)\n",
      "Accuracy at epoch [261]:70.52000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [262]: 0.7120014152526856 (Train Set)\n",
      "Accuracy at epoch [262]:70.54% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [263]: 0.7110038554668426 (Train Set)\n",
      "Accuracy at epoch [263]:70.58% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [264]: 0.7100076141357422 (Train Set)\n",
      "Accuracy at epoch [264]:70.56% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [265]: 0.7090168974399567 (Train Set)\n",
      "Accuracy at epoch [265]:70.62% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [266]: 0.7080241441726685 (Train Set)\n",
      "Accuracy at epoch [266]:70.62% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [267]: 0.707046042919159 (Train Set)\n",
      "Accuracy at epoch [267]:70.7% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [268]: 0.706067845582962 (Train Set)\n",
      "Accuracy at epoch [268]:70.74000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [269]: 0.7050859482288361 (Train Set)\n",
      "Accuracy at epoch [269]:70.67999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [270]: 0.7041170687675476 (Train Set)\n",
      "Accuracy at epoch [270]:70.72% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [271]: 0.7031325061321259 (Train Set)\n",
      "Accuracy at epoch [271]:70.74000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [272]: 0.7021494042873383 (Train Set)\n",
      "Accuracy at epoch [272]:70.76% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [273]: 0.7011889419555664 (Train Set)\n",
      "Accuracy at epoch [273]:70.8% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [274]: 0.7002189366817474 (Train Set)\n",
      "Accuracy at epoch [274]:70.78% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [275]: 0.6992598340511322 (Train Set)\n",
      "Accuracy at epoch [275]:70.76% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [276]: 0.6983225016593934 (Train Set)\n",
      "Accuracy at epoch [276]:70.82000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [277]: 0.6973494663238525 (Train Set)\n",
      "Accuracy at epoch [277]:70.8% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [278]: 0.6964049072265625 (Train Set)\n",
      "Accuracy at epoch [278]:70.8% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [279]: 0.6954654290676117 (Train Set)\n",
      "Accuracy at epoch [279]:70.88% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [280]: 0.6945187511444092 (Train Set)\n",
      "Accuracy at epoch [280]:70.92% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [281]: 0.6935807371139526 (Train Set)\n",
      "Accuracy at epoch [281]:70.88% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [282]: 0.6926443712711334 (Train Set)\n",
      "Accuracy at epoch [282]:70.94% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [283]: 0.691703916311264 (Train Set)\n",
      "Accuracy at epoch [283]:70.92% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [284]: 0.6907794160842895 (Train Set)\n",
      "Accuracy at epoch [284]:70.92% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [285]: 0.6898364706039428 (Train Set)\n",
      "Accuracy at epoch [285]:70.94% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [286]: 0.6888915822505951 (Train Set)\n",
      "Accuracy at epoch [286]:70.96000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [287]: 0.687979376077652 (Train Set)\n",
      "Accuracy at epoch [287]:70.94% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [288]: 0.6870604958534241 (Train Set)\n",
      "Accuracy at epoch [288]:70.94% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [289]: 0.686133278131485 (Train Set)\n",
      "Accuracy at epoch [289]:71.02000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [290]: 0.6852251141071319 (Train Set)\n",
      "Accuracy at epoch [290]:70.96000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [291]: 0.6843097476959229 (Train Set)\n",
      "Accuracy at epoch [291]:71.0% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [292]: 0.6833960268497467 (Train Set)\n",
      "Accuracy at epoch [292]:70.98% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [293]: 0.6824893717765808 (Train Set)\n",
      "Accuracy at epoch [293]:71.0% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [294]: 0.6815851726531983 (Train Set)\n",
      "Accuracy at epoch [294]:71.08% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [295]: 0.6806960229873658 (Train Set)\n",
      "Accuracy at epoch [295]:71.06% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [296]: 0.6797793982028961 (Train Set)\n",
      "Accuracy at epoch [296]:71.04% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [297]: 0.6788689608573913 (Train Set)\n",
      "Accuracy at epoch [297]:71.06% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [298]: 0.6779606997966766 (Train Set)\n",
      "Accuracy at epoch [298]:71.08% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [299]: 0.6770531735420227 (Train Set)\n",
      "Accuracy at epoch [299]:71.08% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [300]: 0.676160358786583 (Train Set)\n",
      "Accuracy at epoch [300]:71.0% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [301]: 0.6752663408517837 (Train Set)\n",
      "Accuracy at epoch [301]:71.1% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [302]: 0.6743668051958084 (Train Set)\n",
      "Accuracy at epoch [302]:71.12% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [303]: 0.6734727754592895 (Train Set)\n",
      "Accuracy at epoch [303]:71.1% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [304]: 0.672583071231842 (Train Set)\n",
      "Accuracy at epoch [304]:71.12% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [305]: 0.671702696442604 (Train Set)\n",
      "Accuracy at epoch [305]:70.98% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [306]: 0.6708045061826706 (Train Set)\n",
      "Accuracy at epoch [306]:71.04% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [307]: 0.6699047477245331 (Train Set)\n",
      "Accuracy at epoch [307]:71.0% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [308]: 0.6690221672058105 (Train Set)\n",
      "Accuracy at epoch [308]:71.0% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [309]: 0.6681335588693619 (Train Set)\n",
      "Accuracy at epoch [309]:71.02000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [310]: 0.6672537838220596 (Train Set)\n",
      "Accuracy at epoch [310]:70.96000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [311]: 0.6663764327764511 (Train Set)\n",
      "Accuracy at epoch [311]:71.02000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [312]: 0.6654900808334351 (Train Set)\n",
      "Accuracy at epoch [312]:71.08% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [313]: 0.6646210668087006 (Train Set)\n",
      "Accuracy at epoch [313]:71.04% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [314]: 0.6637287818193436 (Train Set)\n",
      "Accuracy at epoch [314]:71.1% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [315]: 0.6628621299266815 (Train Set)\n",
      "Accuracy at epoch [315]:71.1% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [316]: 0.6619962445497513 (Train Set)\n",
      "Accuracy at epoch [316]:71.14% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [317]: 0.661108024597168 (Train Set)\n",
      "Accuracy at epoch [317]:71.16% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [318]: 0.66025359582901 (Train Set)\n",
      "Accuracy at epoch [318]:71.17999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [319]: 0.6593718379735947 (Train Set)\n",
      "Accuracy at epoch [319]:71.17999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [320]: 0.6584898945093155 (Train Set)\n",
      "Accuracy at epoch [320]:71.16% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [321]: 0.657633064866066 (Train Set)\n",
      "Accuracy at epoch [321]:71.14% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [322]: 0.6567745246887207 (Train Set)\n",
      "Accuracy at epoch [322]:71.12% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [323]: 0.6559029245376586 (Train Set)\n",
      "Accuracy at epoch [323]:71.14% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [324]: 0.6550359449386597 (Train Set)\n",
      "Accuracy at epoch [324]:71.16% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [325]: 0.6541863601207734 (Train Set)\n",
      "Accuracy at epoch [325]:71.17999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [326]: 0.6533215453624726 (Train Set)\n",
      "Accuracy at epoch [326]:71.17999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [327]: 0.6524753174781799 (Train Set)\n",
      "Accuracy at epoch [327]:71.17999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [328]: 0.651620062828064 (Train Set)\n",
      "Accuracy at epoch [328]:71.16% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [329]: 0.650779272198677 (Train Set)\n",
      "Accuracy at epoch [329]:71.14% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [330]: 0.6499191658496857 (Train Set)\n",
      "Accuracy at epoch [330]:71.17999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [331]: 0.6490743250846863 (Train Set)\n",
      "Accuracy at epoch [331]:71.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [332]: 0.6482219530344009 (Train Set)\n",
      "Accuracy at epoch [332]:71.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [333]: 0.6473681777715683 (Train Set)\n",
      "Accuracy at epoch [333]:71.24000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [334]: 0.6464976297616959 (Train Set)\n",
      "Accuracy at epoch [334]:71.24000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [335]: 0.645643877863884 (Train Set)\n",
      "Accuracy at epoch [335]:71.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [336]: 0.6448127130270004 (Train Set)\n",
      "Accuracy at epoch [336]:71.2% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [337]: 0.6439833011627197 (Train Set)\n",
      "Accuracy at epoch [337]:71.2% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [338]: 0.6431490013599396 (Train Set)\n",
      "Accuracy at epoch [338]:71.12% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [339]: 0.6423006588220597 (Train Set)\n",
      "Accuracy at epoch [339]:71.12% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [340]: 0.6414497839212417 (Train Set)\n",
      "Accuracy at epoch [340]:71.16% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [341]: 0.6405984284877777 (Train Set)\n",
      "Accuracy at epoch [341]:71.16% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [342]: 0.6397561740875244 (Train Set)\n",
      "Accuracy at epoch [342]:71.12% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [343]: 0.6389140465259552 (Train Set)\n",
      "Accuracy at epoch [343]:71.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [344]: 0.6380712926387787 (Train Set)\n",
      "Accuracy at epoch [344]:71.2% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [345]: 0.6372269701957702 (Train Set)\n",
      "Accuracy at epoch [345]:71.2% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [346]: 0.6363806841373444 (Train Set)\n",
      "Accuracy at epoch [346]:71.17999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [347]: 0.6355410487651825 (Train Set)\n",
      "Accuracy at epoch [347]:71.17999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [348]: 0.6347010968923569 (Train Set)\n",
      "Accuracy at epoch [348]:71.16% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [349]: 0.6338670144081116 (Train Set)\n",
      "Accuracy at epoch [349]:71.2% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [350]: 0.6330281583070755 (Train Set)\n",
      "Accuracy at epoch [350]:71.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [351]: 0.6321766028404235 (Train Set)\n",
      "Accuracy at epoch [351]:71.2% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [352]: 0.6313340927362442 (Train Set)\n",
      "Accuracy at epoch [352]:71.28% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [353]: 0.6305014593601227 (Train Set)\n",
      "Accuracy at epoch [353]:71.3% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [354]: 0.6296633661985397 (Train Set)\n",
      "Accuracy at epoch [354]:71.26% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [355]: 0.6288101202249528 (Train Set)\n",
      "Accuracy at epoch [355]:71.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [356]: 0.6279614576101303 (Train Set)\n",
      "Accuracy at epoch [356]:71.26% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [357]: 0.6271370451450348 (Train Set)\n",
      "Accuracy at epoch [357]:71.3% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [358]: 0.6263062214851379 (Train Set)\n",
      "Accuracy at epoch [358]:71.28% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [359]: 0.6254799727201462 (Train Set)\n",
      "Accuracy at epoch [359]:71.28% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [360]: 0.624646661400795 (Train Set)\n",
      "Accuracy at epoch [360]:71.26% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [361]: 0.6238084847927093 (Train Set)\n",
      "Accuracy at epoch [361]:71.2% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [362]: 0.62297960293293 (Train Set)\n",
      "Accuracy at epoch [362]:71.14% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [363]: 0.6221616220474243 (Train Set)\n",
      "Accuracy at epoch [363]:71.12% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [364]: 0.6213442286252976 (Train Set)\n",
      "Accuracy at epoch [364]:71.12% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [365]: 0.6205104931592941 (Train Set)\n",
      "Accuracy at epoch [365]:71.12% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [366]: 0.6196887512207031 (Train Set)\n",
      "Accuracy at epoch [366]:71.1% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [367]: 0.6188755053281784 (Train Set)\n",
      "Accuracy at epoch [367]:71.12% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [368]: 0.6180371741056442 (Train Set)\n",
      "Accuracy at epoch [368]:71.16% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [369]: 0.617205100774765 (Train Set)\n",
      "Accuracy at epoch [369]:71.2% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [370]: 0.6163974792957306 (Train Set)\n",
      "Accuracy at epoch [370]:71.26% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [371]: 0.6155724987983704 (Train Set)\n",
      "Accuracy at epoch [371]:71.2% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [372]: 0.614747895359993 (Train Set)\n",
      "Accuracy at epoch [372]:71.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [373]: 0.6139390219449997 (Train Set)\n",
      "Accuracy at epoch [373]:71.16% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [374]: 0.6131174298524856 (Train Set)\n",
      "Accuracy at epoch [374]:71.17999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [375]: 0.6123073235750198 (Train Set)\n",
      "Accuracy at epoch [375]:71.24000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [376]: 0.6114955053329468 (Train Set)\n",
      "Accuracy at epoch [376]:71.2% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [377]: 0.6106828273534775 (Train Set)\n",
      "Accuracy at epoch [377]:71.26% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [378]: 0.6098684980869293 (Train Set)\n",
      "Accuracy at epoch [378]:71.28% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [379]: 0.6090605585575104 (Train Set)\n",
      "Accuracy at epoch [379]:71.28% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [380]: 0.6082368454933167 (Train Set)\n",
      "Accuracy at epoch [380]:71.32% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [381]: 0.6074252471923828 (Train Set)\n",
      "Accuracy at epoch [381]:71.34% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [382]: 0.6066008437871933 (Train Set)\n",
      "Accuracy at epoch [382]:71.32% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [383]: 0.6057835695743561 (Train Set)\n",
      "Accuracy at epoch [383]:71.32% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [384]: 0.6049877014160157 (Train Set)\n",
      "Accuracy at epoch [384]:71.34% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [385]: 0.6041781449317932 (Train Set)\n",
      "Accuracy at epoch [385]:71.3% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [386]: 0.6033570833206177 (Train Set)\n",
      "Accuracy at epoch [386]:71.28% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [387]: 0.6025462937355042 (Train Set)\n",
      "Accuracy at epoch [387]:71.24000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [388]: 0.6017388336658478 (Train Set)\n",
      "Accuracy at epoch [388]:71.17999999999999% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [389]: 0.6009114028215409 (Train Set)\n",
      "Accuracy at epoch [389]:71.2% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [390]: 0.6001059569120407 (Train Set)\n",
      "Accuracy at epoch [390]:71.24000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [391]: 0.5992856042385102 (Train Set)\n",
      "Accuracy at epoch [391]:71.24000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [392]: 0.5984769854545593 (Train Set)\n",
      "Accuracy at epoch [392]:71.28% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [393]: 0.5976778047084809 (Train Set)\n",
      "Accuracy at epoch [393]:71.24000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [394]: 0.5968824009895325 (Train Set)\n",
      "Accuracy at epoch [394]:71.24000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [395]: 0.5960692019462586 (Train Set)\n",
      "Accuracy at epoch [395]:71.2% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [396]: 0.59526298224926 (Train Set)\n",
      "Accuracy at epoch [396]:71.2% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [397]: 0.5944596801996231 (Train Set)\n",
      "Accuracy at epoch [397]:71.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [398]: 0.5936527014970779 (Train Set)\n",
      "Accuracy at epoch [398]:71.24000000000001% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [399]: 0.5928392902612686 (Train Set)\n",
      "Accuracy at epoch [399]:71.22% (Test Set)\n",
      "\n",
      "\n",
      "Loss at epoch [400]: 0.5920393780469895 (Train Set)\n",
      "Accuracy at epoch [400]:71.26% (Test Set)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Treinamento por 400 pocas\n",
    "best_model, train_losses, test_acc = train_model(model, 400, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 973548,
     "status": "ok",
     "timestamp": 1555804972955,
     "user": {
      "displayName": "Ihan Bender",
      "photoUrl": "",
      "userId": "09326755177320215938"
     },
     "user_tz": 180
    },
    "id": "9i0RiPxGnHER",
    "outputId": "7bb24264-a24a-4234-e2ae-82711cf79b96"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAEYCAYAAAANjbKIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRETimoJKRETi2v8HTnYbntrfgXoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAEYCAYAAAD1bUl/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA5vraZPNVTFQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Cross Entropy durante o Treinamento\")\n",
    "plt.tight_layout()\n",
    "_ = plt.plot(train_losses)\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Accuracia durante o Treinamento\")\n",
    "plt.tight_layout()\n",
    "_ = plt.plot(test_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "01Tt80TJnHEU"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, batch_size):\n",
    "  inputs_list, labels_list = [], []\n",
    "  for _, (inputs, labels) in enumerate(dataset):\n",
    "      inputs_list.append(inputs.to(device))\n",
    "      labels_list.append(labels.to(device))\n",
    "    \n",
    "  hits = 0\n",
    "  for i, (inputs, labels) in enumerate(zip(inputs_list, labels_list), 0):\n",
    "      y_pred = model(inputs)\n",
    "      for i, (pred, label) in enumerate(zip(y_pred, labels)):\n",
    "          if one_hot(pred)==label.item():\n",
    "              hits+=1\n",
    "  return hits / (len(dataset) * batch_size) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORT70P_HnHEa"
   },
   "source": [
    "# Avaliao do Modelo no dataset de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 981477,
     "status": "ok",
     "timestamp": 1555804980895,
     "user": {
      "displayName": "Ihan Bender",
      "photoUrl": "",
      "userId": "09326755177320215938"
     },
     "user_tz": 180
    },
    "id": "67HW1F6qnHEb",
    "outputId": "0da03472-37d8-43f8-d860-8329cfc7a267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.9\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, train, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c8Xu6D4znHEd"
   },
   "source": [
    "# Avaliao do Modelo no dataset de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 981693,
     "status": "ok",
     "timestamp": 1555804981123,
     "user": {
      "displayName": "Ihan Bender",
      "photoUrl": "",
      "userId": "09326755177320215938"
     },
     "user_tz": 180
    },
    "id": "kNDO4K7FnHEf",
    "outputId": "34c44b74-a3bf-4f1e-b01f-891f0fa9ffbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.26\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, test, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VIeUfbSeroSz"
   },
   "source": [
    "# Validao do Modelo (conjunto de validao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 981927,
     "status": "ok",
     "timestamp": 1555804981364,
     "user": {
      "displayName": "Ihan Bender",
      "photoUrl": "",
      "userId": "09326755177320215938"
     },
     "user_tz": 180
    },
    "id": "WSY1Kjdart8J",
    "outputId": "321248fc-65f6-4d31-f142-e56cd3545b18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.6\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, validation, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiRoZ5TeN6BU"
   },
   "source": [
    "<p> Nota-se, portanto que foi possvel obter bons resultados usando batch norm com convolues </p>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "03 - CNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
