{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos deste trabalho\n",
    "- Familiarizar-se com a biblioteca PyTorch\n",
    "- Definir arquiteturas MLP simples em PyTorch\n",
    "- Treinar utilizando CIFAR10, testando diferentes arquiteturas, parâmetros, funções de loss e otimizadores\n",
    "- Comparar os resultados obtidos utilizando apenas Perpceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Carregar os datasets\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "dataset_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=200)\n",
    "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a arquitetura MLP\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.fc1 = nn.Linear(32*32, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        self.activation_function = torch.nn.ReLU()\n",
    "#         self.activation_function = torch.nn.LeakyReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32)\n",
    "        x = self.activation_function(self.fc1(x))\n",
    "        x = self.activation_function(self.fc2(x))\n",
    "        x = self.activation_function(self.fc3(x))\n",
    "        x = self.activation_function(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "MLP(\n",
      "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (activation_function): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP([]).to(device)\n",
    "print(device)\n",
    "print(model)\n",
    "\n",
    "# Definir otimizador e loss\n",
    "# Nota: testar outros otimizadores e funções de loss (em particular cross entropy)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 2.303\n",
      "[2,   200] loss: 2.303\n",
      "[3,   200] loss: 2.302\n",
      "[4,   200] loss: 2.302\n",
      "[5,   200] loss: 2.301\n",
      "[6,   200] loss: 2.301\n",
      "[7,   200] loss: 2.301\n",
      "[8,   200] loss: 2.300\n",
      "[9,   200] loss: 2.300\n",
      "[10,   200] loss: 2.299\n",
      "[11,   200] loss: 2.299\n",
      "[12,   200] loss: 2.299\n",
      "[13,   200] loss: 2.298\n",
      "[14,   200] loss: 2.298\n",
      "[15,   200] loss: 2.297\n",
      "[16,   200] loss: 2.297\n",
      "[17,   200] loss: 2.296\n",
      "[18,   200] loss: 2.296\n",
      "[19,   200] loss: 2.295\n",
      "[20,   200] loss: 2.294\n",
      "[21,   200] loss: 2.294\n",
      "[22,   200] loss: 2.293\n",
      "[23,   200] loss: 2.293\n",
      "[24,   200] loss: 2.292\n",
      "[25,   200] loss: 2.291\n",
      "[26,   200] loss: 2.291\n",
      "[27,   200] loss: 2.290\n",
      "[28,   200] loss: 2.289\n",
      "[29,   200] loss: 2.288\n",
      "[30,   200] loss: 2.288\n",
      "[31,   200] loss: 2.287\n",
      "[32,   200] loss: 2.286\n",
      "[33,   200] loss: 2.285\n",
      "[34,   200] loss: 2.284\n",
      "[35,   200] loss: 2.283\n",
      "[36,   200] loss: 2.282\n",
      "[37,   200] loss: 2.281\n",
      "[38,   200] loss: 2.279\n",
      "[39,   200] loss: 2.278\n",
      "[40,   200] loss: 2.277\n",
      "[41,   200] loss: 2.276\n",
      "[42,   200] loss: 2.274\n",
      "[43,   200] loss: 2.273\n",
      "[44,   200] loss: 2.271\n",
      "[45,   200] loss: 2.270\n",
      "[46,   200] loss: 2.268\n",
      "[47,   200] loss: 2.267\n",
      "[48,   200] loss: 2.265\n",
      "[49,   200] loss: 2.263\n",
      "[50,   200] loss: 2.261\n",
      "[51,   200] loss: 2.259\n",
      "[52,   200] loss: 2.257\n",
      "[53,   200] loss: 2.254\n",
      "[54,   200] loss: 2.252\n",
      "[55,   200] loss: 2.249\n",
      "[56,   200] loss: 2.246\n",
      "[57,   200] loss: 2.244\n",
      "[58,   200] loss: 2.241\n",
      "[59,   200] loss: 2.237\n",
      "[60,   200] loss: 2.234\n",
      "[61,   200] loss: 2.231\n",
      "[62,   200] loss: 2.227\n",
      "[63,   200] loss: 2.223\n",
      "[64,   200] loss: 2.220\n",
      "[65,   200] loss: 2.216\n",
      "[66,   200] loss: 2.212\n",
      "[67,   200] loss: 2.208\n",
      "[68,   200] loss: 2.204\n",
      "[69,   200] loss: 2.200\n",
      "[70,   200] loss: 2.196\n",
      "[71,   200] loss: 2.192\n",
      "[72,   200] loss: 2.188\n",
      "[73,   200] loss: 2.184\n",
      "[74,   200] loss: 2.181\n",
      "[75,   200] loss: 2.178\n",
      "[76,   200] loss: 2.174\n",
      "[77,   200] loss: 2.171\n",
      "[78,   200] loss: 2.168\n",
      "[79,   200] loss: 2.166\n",
      "[80,   200] loss: 2.163\n",
      "[81,   200] loss: 2.161\n",
      "[82,   200] loss: 2.158\n",
      "[83,   200] loss: 2.156\n",
      "[84,   200] loss: 2.154\n",
      "[85,   200] loss: 2.152\n",
      "[86,   200] loss: 2.150\n",
      "[87,   200] loss: 2.148\n",
      "[88,   200] loss: 2.146\n",
      "[89,   200] loss: 2.144\n",
      "[90,   200] loss: 2.142\n",
      "[91,   200] loss: 2.140\n",
      "[92,   200] loss: 2.138\n",
      "[93,   200] loss: 2.136\n",
      "[94,   200] loss: 2.134\n",
      "[95,   200] loss: 2.132\n",
      "[96,   200] loss: 2.130\n",
      "[97,   200] loss: 2.129\n",
      "[98,   200] loss: 2.127\n",
      "[99,   200] loss: 2.125\n",
      "[100,   200] loss: 2.123\n",
      "[101,   200] loss: 2.121\n",
      "[102,   200] loss: 2.119\n",
      "[103,   200] loss: 2.117\n",
      "[104,   200] loss: 2.115\n",
      "[105,   200] loss: 2.112\n",
      "[106,   200] loss: 2.110\n",
      "[107,   200] loss: 2.108\n",
      "[108,   200] loss: 2.106\n",
      "[109,   200] loss: 2.104\n",
      "[110,   200] loss: 2.102\n",
      "[111,   200] loss: 2.100\n",
      "[112,   200] loss: 2.098\n",
      "[113,   200] loss: 2.096\n",
      "[114,   200] loss: 2.094\n",
      "[115,   200] loss: 2.092\n",
      "[116,   200] loss: 2.090\n",
      "[117,   200] loss: 2.088\n",
      "[118,   200] loss: 2.086\n",
      "[119,   200] loss: 2.084\n",
      "[120,   200] loss: 2.082\n",
      "[121,   200] loss: 2.080\n",
      "[122,   200] loss: 2.078\n",
      "[123,   200] loss: 2.076\n",
      "[124,   200] loss: 2.075\n",
      "[125,   200] loss: 2.073\n",
      "[126,   200] loss: 2.071\n",
      "[127,   200] loss: 2.070\n",
      "[128,   200] loss: 2.068\n",
      "[129,   200] loss: 2.067\n",
      "[130,   200] loss: 2.065\n",
      "[131,   200] loss: 2.064\n",
      "[132,   200] loss: 2.063\n",
      "[133,   200] loss: 2.061\n",
      "[134,   200] loss: 2.060\n",
      "[135,   200] loss: 2.059\n",
      "[136,   200] loss: 2.058\n",
      "[137,   200] loss: 2.056\n",
      "[138,   200] loss: 2.055\n",
      "[139,   200] loss: 2.054\n",
      "[140,   200] loss: 2.053\n",
      "[141,   200] loss: 2.052\n",
      "[142,   200] loss: 2.051\n",
      "[143,   200] loss: 2.050\n",
      "[144,   200] loss: 2.049\n",
      "[145,   200] loss: 2.048\n",
      "[146,   200] loss: 2.047\n",
      "[147,   200] loss: 2.046\n",
      "[148,   200] loss: 2.045\n",
      "[149,   200] loss: 2.044\n",
      "[150,   200] loss: 2.043\n",
      "[151,   200] loss: 2.042\n",
      "[152,   200] loss: 2.041\n",
      "[153,   200] loss: 2.041\n",
      "[154,   200] loss: 2.040\n",
      "[155,   200] loss: 2.039\n",
      "[156,   200] loss: 2.038\n",
      "[157,   200] loss: 2.037\n",
      "[158,   200] loss: 2.036\n",
      "[159,   200] loss: 2.035\n",
      "[160,   200] loss: 2.035\n",
      "[161,   200] loss: 2.034\n",
      "[162,   200] loss: 2.033\n",
      "[163,   200] loss: 2.032\n",
      "[164,   200] loss: 2.031\n",
      "[165,   200] loss: 2.030\n",
      "[166,   200] loss: 2.030\n",
      "[167,   200] loss: 2.029\n",
      "[168,   200] loss: 2.028\n",
      "[169,   200] loss: 2.027\n",
      "[170,   200] loss: 2.026\n",
      "[171,   200] loss: 2.025\n",
      "[172,   200] loss: 2.025\n",
      "[173,   200] loss: 2.024\n",
      "[174,   200] loss: 2.023\n",
      "[175,   200] loss: 2.022\n",
      "[176,   200] loss: 2.021\n",
      "[177,   200] loss: 2.020\n",
      "[178,   200] loss: 2.020\n",
      "[179,   200] loss: 2.019\n",
      "[180,   200] loss: 2.018\n",
      "[181,   200] loss: 2.017\n",
      "[182,   200] loss: 2.016\n",
      "[183,   200] loss: 2.015\n",
      "[184,   200] loss: 2.015\n",
      "[185,   200] loss: 2.014\n",
      "[186,   200] loss: 2.013\n",
      "[187,   200] loss: 2.012\n",
      "[188,   200] loss: 2.011\n",
      "[189,   200] loss: 2.010\n",
      "[190,   200] loss: 2.010\n",
      "[191,   200] loss: 2.009\n",
      "[192,   200] loss: 2.008\n",
      "[193,   200] loss: 2.007\n",
      "[194,   200] loss: 2.006\n",
      "[195,   200] loss: 2.005\n",
      "[196,   200] loss: 2.004\n",
      "[197,   200] loss: 2.003\n",
      "[198,   200] loss: 2.003\n",
      "[199,   200] loss: 2.002\n",
      "[200,   200] loss: 2.001\n",
      "[201,   200] loss: 2.000\n",
      "[202,   200] loss: 1.999\n",
      "[203,   200] loss: 1.998\n",
      "[204,   200] loss: 1.997\n",
      "[205,   200] loss: 1.996\n",
      "[206,   200] loss: 1.995\n",
      "[207,   200] loss: 1.995\n",
      "[208,   200] loss: 1.994\n",
      "[209,   200] loss: 1.993\n",
      "[210,   200] loss: 1.992\n",
      "[211,   200] loss: 1.991\n",
      "[212,   200] loss: 1.990\n",
      "[213,   200] loss: 1.989\n",
      "[214,   200] loss: 1.988\n",
      "[215,   200] loss: 1.987\n",
      "[216,   200] loss: 1.987\n",
      "[217,   200] loss: 1.986\n",
      "[218,   200] loss: 1.985\n",
      "[219,   200] loss: 1.984\n",
      "[220,   200] loss: 1.983\n",
      "[221,   200] loss: 1.982\n",
      "[222,   200] loss: 1.981\n",
      "[223,   200] loss: 1.980\n",
      "[224,   200] loss: 1.979\n",
      "[225,   200] loss: 1.978\n",
      "[226,   200] loss: 1.977\n",
      "[227,   200] loss: 1.977\n",
      "[228,   200] loss: 1.976\n",
      "[229,   200] loss: 1.975\n",
      "[230,   200] loss: 1.974\n",
      "[231,   200] loss: 1.973\n",
      "[232,   200] loss: 1.972\n",
      "[233,   200] loss: 1.971\n",
      "[234,   200] loss: 1.970\n",
      "[235,   200] loss: 1.969\n",
      "[236,   200] loss: 1.968\n",
      "[237,   200] loss: 1.967\n",
      "[238,   200] loss: 1.966\n",
      "[239,   200] loss: 1.965\n",
      "[240,   200] loss: 1.965\n",
      "[241,   200] loss: 1.964\n",
      "[242,   200] loss: 1.963\n",
      "[243,   200] loss: 1.962\n",
      "[244,   200] loss: 1.961\n",
      "[245,   200] loss: 1.960\n",
      "[246,   200] loss: 1.959\n",
      "[247,   200] loss: 1.958\n",
      "[248,   200] loss: 1.957\n",
      "[249,   200] loss: 1.956\n",
      "[250,   200] loss: 1.955\n",
      "[251,   200] loss: 1.954\n",
      "[252,   200] loss: 1.953\n",
      "[253,   200] loss: 1.952\n",
      "[254,   200] loss: 1.951\n",
      "[255,   200] loss: 1.950\n",
      "[256,   200] loss: 1.949\n",
      "[257,   200] loss: 1.948\n",
      "[258,   200] loss: 1.947\n",
      "[259,   200] loss: 1.946\n",
      "[260,   200] loss: 1.945\n",
      "[261,   200] loss: 1.944\n",
      "[262,   200] loss: 1.943\n",
      "[263,   200] loss: 1.943\n",
      "[264,   200] loss: 1.942\n",
      "[265,   200] loss: 1.941\n",
      "[266,   200] loss: 1.940\n",
      "[267,   200] loss: 1.939\n",
      "[268,   200] loss: 1.938\n",
      "[269,   200] loss: 1.937\n",
      "[270,   200] loss: 1.936\n",
      "[271,   200] loss: 1.935\n",
      "[272,   200] loss: 1.934\n",
      "[273,   200] loss: 1.933\n",
      "[274,   200] loss: 1.932\n",
      "[275,   200] loss: 1.931\n",
      "[276,   200] loss: 1.930\n",
      "[277,   200] loss: 1.929\n",
      "[278,   200] loss: 1.928\n",
      "[279,   200] loss: 1.927\n",
      "[280,   200] loss: 1.926\n",
      "[281,   200] loss: 1.925\n",
      "[282,   200] loss: 1.924\n",
      "[283,   200] loss: 1.923\n",
      "[284,   200] loss: 1.922\n",
      "[285,   200] loss: 1.921\n",
      "[286,   200] loss: 1.920\n",
      "[287,   200] loss: 1.919\n",
      "[288,   200] loss: 1.918\n",
      "[289,   200] loss: 1.917\n",
      "[290,   200] loss: 1.916\n",
      "[291,   200] loss: 1.915\n",
      "[292,   200] loss: 1.914\n",
      "[293,   200] loss: 1.913\n",
      "[294,   200] loss: 1.912\n",
      "[295,   200] loss: 1.911\n",
      "[296,   200] loss: 1.910\n",
      "[297,   200] loss: 1.909\n",
      "[298,   200] loss: 1.908\n",
      "[299,   200] loss: 1.907\n",
      "[300,   200] loss: 1.906\n",
      "[301,   200] loss: 1.905\n",
      "[302,   200] loss: 1.904\n",
      "[303,   200] loss: 1.903\n",
      "[304,   200] loss: 1.902\n",
      "[305,   200] loss: 1.901\n",
      "[306,   200] loss: 1.900\n",
      "[307,   200] loss: 1.899\n",
      "[308,   200] loss: 1.898\n",
      "[309,   200] loss: 1.897\n",
      "[310,   200] loss: 1.896\n",
      "[311,   200] loss: 1.895\n",
      "[312,   200] loss: 1.894\n",
      "[313,   200] loss: 1.893\n",
      "[314,   200] loss: 1.892\n",
      "[315,   200] loss: 1.891\n",
      "[316,   200] loss: 1.890\n",
      "[317,   200] loss: 1.889\n",
      "[318,   200] loss: 1.888\n",
      "[319,   200] loss: 1.887\n",
      "[320,   200] loss: 1.886\n",
      "[321,   200] loss: 1.885\n",
      "[322,   200] loss: 1.884\n",
      "[323,   200] loss: 1.883\n",
      "[324,   200] loss: 1.882\n",
      "[325,   200] loss: 1.881\n",
      "[326,   200] loss: 1.880\n",
      "[327,   200] loss: 1.879\n",
      "[328,   200] loss: 1.878\n",
      "[329,   200] loss: 1.877\n",
      "[330,   200] loss: 1.876\n",
      "[331,   200] loss: 1.875\n",
      "[332,   200] loss: 1.874\n",
      "[333,   200] loss: 1.873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[334,   200] loss: 1.872\n",
      "[335,   200] loss: 1.871\n",
      "[336,   200] loss: 1.870\n",
      "[337,   200] loss: 1.869\n",
      "[338,   200] loss: 1.868\n",
      "[339,   200] loss: 1.867\n",
      "[340,   200] loss: 1.866\n",
      "[341,   200] loss: 1.865\n",
      "[342,   200] loss: 1.864\n",
      "[343,   200] loss: 1.863\n",
      "[344,   200] loss: 1.862\n",
      "[345,   200] loss: 1.861\n",
      "[346,   200] loss: 1.860\n",
      "[347,   200] loss: 1.858\n",
      "[348,   200] loss: 1.857\n",
      "[349,   200] loss: 1.856\n",
      "[350,   200] loss: 1.855\n",
      "[351,   200] loss: 1.854\n",
      "[352,   200] loss: 1.853\n",
      "[353,   200] loss: 1.852\n",
      "[354,   200] loss: 1.851\n",
      "[355,   200] loss: 1.850\n",
      "[356,   200] loss: 1.849\n",
      "[357,   200] loss: 1.848\n",
      "[358,   200] loss: 1.847\n",
      "[359,   200] loss: 1.846\n",
      "[360,   200] loss: 1.845\n",
      "[361,   200] loss: 1.844\n",
      "[362,   200] loss: 1.843\n",
      "[363,   200] loss: 1.842\n",
      "[364,   200] loss: 1.841\n",
      "[365,   200] loss: 1.840\n",
      "[366,   200] loss: 1.839\n",
      "[367,   200] loss: 1.838\n",
      "[368,   200] loss: 1.837\n",
      "[369,   200] loss: 1.836\n",
      "[370,   200] loss: 1.835\n",
      "[371,   200] loss: 1.834\n",
      "[372,   200] loss: 1.833\n",
      "[373,   200] loss: 1.832\n",
      "[374,   200] loss: 1.831\n",
      "[375,   200] loss: 1.830\n",
      "[376,   200] loss: 1.829\n",
      "[377,   200] loss: 1.828\n",
      "[378,   200] loss: 1.827\n",
      "[379,   200] loss: 1.826\n",
      "[380,   200] loss: 1.825\n",
      "[381,   200] loss: 1.824\n",
      "[382,   200] loss: 1.823\n",
      "[383,   200] loss: 1.822\n",
      "[384,   200] loss: 1.821\n",
      "[385,   200] loss: 1.820\n",
      "[386,   200] loss: 1.819\n",
      "[387,   200] loss: 1.818\n",
      "[388,   200] loss: 1.817\n",
      "[389,   200] loss: 1.816\n",
      "[390,   200] loss: 1.815\n",
      "[391,   200] loss: 1.814\n",
      "[392,   200] loss: 1.813\n",
      "[393,   200] loss: 1.812\n",
      "[394,   200] loss: 1.811\n",
      "[395,   200] loss: 1.810\n",
      "[396,   200] loss: 1.809\n",
      "[397,   200] loss: 1.808\n",
      "[398,   200] loss: 1.807\n",
      "[399,   200] loss: 1.806\n",
      "[400,   200] loss: 1.805\n",
      "[401,   200] loss: 1.804\n",
      "[402,   200] loss: 1.803\n",
      "[403,   200] loss: 1.802\n",
      "[404,   200] loss: 1.801\n",
      "[405,   200] loss: 1.800\n",
      "[406,   200] loss: 1.799\n",
      "[407,   200] loss: 1.798\n",
      "[408,   200] loss: 1.797\n",
      "[409,   200] loss: 1.796\n",
      "[410,   200] loss: 1.795\n",
      "[411,   200] loss: 1.794\n",
      "[412,   200] loss: 1.793\n",
      "[413,   200] loss: 1.792\n",
      "[414,   200] loss: 1.791\n",
      "[415,   200] loss: 1.790\n",
      "[416,   200] loss: 1.790\n",
      "[417,   200] loss: 1.789\n",
      "[418,   200] loss: 1.788\n",
      "[419,   200] loss: 1.787\n",
      "[420,   200] loss: 1.786\n",
      "[421,   200] loss: 1.785\n",
      "[422,   200] loss: 1.784\n",
      "[423,   200] loss: 1.783\n",
      "[424,   200] loss: 1.782\n",
      "[425,   200] loss: 1.781\n",
      "[426,   200] loss: 1.780\n",
      "[427,   200] loss: 1.779\n",
      "[428,   200] loss: 1.778\n",
      "[429,   200] loss: 1.777\n",
      "[430,   200] loss: 1.776\n",
      "[431,   200] loss: 1.775\n",
      "[432,   200] loss: 1.774\n",
      "[433,   200] loss: 1.773\n",
      "[434,   200] loss: 1.773\n",
      "[435,   200] loss: 1.772\n",
      "[436,   200] loss: 1.771\n",
      "[437,   200] loss: 1.770\n",
      "[438,   200] loss: 1.769\n",
      "[439,   200] loss: 1.768\n",
      "[440,   200] loss: 1.767\n",
      "[441,   200] loss: 1.766\n",
      "[442,   200] loss: 1.765\n",
      "[443,   200] loss: 1.764\n",
      "[444,   200] loss: 1.763\n",
      "[445,   200] loss: 1.762\n",
      "[446,   200] loss: 1.761\n",
      "[447,   200] loss: 1.761\n",
      "[448,   200] loss: 1.760\n",
      "[449,   200] loss: 1.759\n",
      "[450,   200] loss: 1.758\n",
      "[451,   200] loss: 1.757\n",
      "[452,   200] loss: 1.756\n",
      "[453,   200] loss: 1.755\n",
      "[454,   200] loss: 1.754\n",
      "[455,   200] loss: 1.753\n",
      "[456,   200] loss: 1.752\n",
      "[457,   200] loss: 1.752\n",
      "[458,   200] loss: 1.751\n",
      "[459,   200] loss: 1.750\n",
      "[460,   200] loss: 1.749\n",
      "[461,   200] loss: 1.748\n",
      "[462,   200] loss: 1.747\n",
      "[463,   200] loss: 1.746\n",
      "[464,   200] loss: 1.745\n",
      "[465,   200] loss: 1.745\n",
      "[466,   200] loss: 1.744\n",
      "[467,   200] loss: 1.743\n",
      "[468,   200] loss: 1.742\n",
      "[469,   200] loss: 1.741\n",
      "[470,   200] loss: 1.740\n",
      "[471,   200] loss: 1.739\n",
      "[472,   200] loss: 1.738\n",
      "[473,   200] loss: 1.737\n",
      "[474,   200] loss: 1.737\n",
      "[475,   200] loss: 1.736\n",
      "[476,   200] loss: 1.735\n",
      "[477,   200] loss: 1.734\n",
      "[478,   200] loss: 1.733\n",
      "[479,   200] loss: 1.732\n",
      "[480,   200] loss: 1.731\n",
      "[481,   200] loss: 1.731\n",
      "[482,   200] loss: 1.730\n",
      "[483,   200] loss: 1.729\n",
      "[484,   200] loss: 1.728\n",
      "[485,   200] loss: 1.727\n",
      "[486,   200] loss: 1.726\n",
      "[487,   200] loss: 1.725\n",
      "[488,   200] loss: 1.724\n",
      "[489,   200] loss: 1.724\n",
      "[490,   200] loss: 1.723\n",
      "[491,   200] loss: 1.722\n",
      "[492,   200] loss: 1.721\n",
      "[493,   200] loss: 1.720\n",
      "[494,   200] loss: 1.719\n",
      "[495,   200] loss: 1.718\n",
      "[496,   200] loss: 1.718\n",
      "[497,   200] loss: 1.717\n",
      "[498,   200] loss: 1.716\n",
      "[499,   200] loss: 1.715\n",
      "[500,   200] loss: 1.714\n",
      "[501,   200] loss: 1.713\n",
      "[502,   200] loss: 1.712\n",
      "[503,   200] loss: 1.712\n",
      "[504,   200] loss: 1.711\n",
      "[505,   200] loss: 1.710\n",
      "[506,   200] loss: 1.709\n",
      "[507,   200] loss: 1.708\n",
      "[508,   200] loss: 1.707\n",
      "[509,   200] loss: 1.706\n",
      "[510,   200] loss: 1.706\n",
      "[511,   200] loss: 1.705\n",
      "[512,   200] loss: 1.704\n",
      "[513,   200] loss: 1.703\n",
      "[514,   200] loss: 1.702\n",
      "[515,   200] loss: 1.701\n",
      "[516,   200] loss: 1.701\n",
      "[517,   200] loss: 1.700\n",
      "[518,   200] loss: 1.699\n",
      "[519,   200] loss: 1.698\n",
      "[520,   200] loss: 1.697\n",
      "[521,   200] loss: 1.696\n",
      "[522,   200] loss: 1.695\n",
      "[523,   200] loss: 1.695\n",
      "[524,   200] loss: 1.694\n",
      "[525,   200] loss: 1.693\n",
      "[526,   200] loss: 1.692\n",
      "[527,   200] loss: 1.691\n",
      "[528,   200] loss: 1.690\n",
      "[529,   200] loss: 1.690\n",
      "[530,   200] loss: 1.689\n",
      "[531,   200] loss: 1.688\n",
      "[532,   200] loss: 1.687\n",
      "[533,   200] loss: 1.686\n",
      "[534,   200] loss: 1.685\n",
      "[535,   200] loss: 1.684\n",
      "[536,   200] loss: 1.684\n",
      "[537,   200] loss: 1.683\n",
      "[538,   200] loss: 1.682\n",
      "[539,   200] loss: 1.681\n",
      "[540,   200] loss: 1.680\n",
      "[541,   200] loss: 1.679\n",
      "[542,   200] loss: 1.678\n",
      "[543,   200] loss: 1.678\n",
      "[544,   200] loss: 1.677\n",
      "[545,   200] loss: 1.676\n",
      "[546,   200] loss: 1.675\n",
      "[547,   200] loss: 1.674\n",
      "[548,   200] loss: 1.673\n",
      "[549,   200] loss: 1.673\n",
      "[550,   200] loss: 1.672\n",
      "[551,   200] loss: 1.671\n",
      "[552,   200] loss: 1.670\n",
      "[553,   200] loss: 1.669\n",
      "[554,   200] loss: 1.668\n",
      "[555,   200] loss: 1.667\n",
      "[556,   200] loss: 1.667\n",
      "[557,   200] loss: 1.666\n",
      "[558,   200] loss: 1.665\n",
      "[559,   200] loss: 1.664\n",
      "[560,   200] loss: 1.663\n",
      "[561,   200] loss: 1.662\n",
      "[562,   200] loss: 1.662\n",
      "[563,   200] loss: 1.661\n",
      "[564,   200] loss: 1.660\n",
      "[565,   200] loss: 1.659\n",
      "[566,   200] loss: 1.658\n",
      "[567,   200] loss: 1.657\n",
      "[568,   200] loss: 1.656\n",
      "[569,   200] loss: 1.656\n",
      "[570,   200] loss: 1.655\n",
      "[571,   200] loss: 1.654\n",
      "[572,   200] loss: 1.653\n",
      "[573,   200] loss: 1.652\n",
      "[574,   200] loss: 1.651\n",
      "[575,   200] loss: 1.650\n",
      "[576,   200] loss: 1.649\n",
      "[577,   200] loss: 1.649\n",
      "[578,   200] loss: 1.648\n",
      "[579,   200] loss: 1.647\n",
      "[580,   200] loss: 1.646\n",
      "[581,   200] loss: 1.645\n",
      "[582,   200] loss: 1.644\n",
      "[583,   200] loss: 1.643\n",
      "[584,   200] loss: 1.643\n",
      "[585,   200] loss: 1.642\n",
      "[586,   200] loss: 1.641\n",
      "[587,   200] loss: 1.640\n",
      "[588,   200] loss: 1.639\n",
      "[589,   200] loss: 1.638\n",
      "[590,   200] loss: 1.637\n",
      "[591,   200] loss: 1.636\n",
      "[592,   200] loss: 1.636\n",
      "[593,   200] loss: 1.635\n",
      "[594,   200] loss: 1.634\n",
      "[595,   200] loss: 1.633\n",
      "[596,   200] loss: 1.632\n",
      "[597,   200] loss: 1.631\n",
      "[598,   200] loss: 1.630\n",
      "[599,   200] loss: 1.629\n",
      "[600,   200] loss: 1.629\n",
      "[601,   200] loss: 1.628\n",
      "[602,   200] loss: 1.627\n",
      "[603,   200] loss: 1.626\n",
      "[604,   200] loss: 1.625\n",
      "[605,   200] loss: 1.624\n",
      "[606,   200] loss: 1.623\n",
      "[607,   200] loss: 1.622\n",
      "[608,   200] loss: 1.622\n",
      "[609,   200] loss: 1.621\n",
      "[610,   200] loss: 1.620\n",
      "[611,   200] loss: 1.619\n",
      "[612,   200] loss: 1.618\n",
      "[613,   200] loss: 1.617\n",
      "[614,   200] loss: 1.616\n",
      "[615,   200] loss: 1.615\n",
      "[616,   200] loss: 1.615\n",
      "[617,   200] loss: 1.614\n",
      "[618,   200] loss: 1.613\n",
      "[619,   200] loss: 1.612\n",
      "[620,   200] loss: 1.611\n",
      "[621,   200] loss: 1.610\n",
      "[622,   200] loss: 1.609\n",
      "[623,   200] loss: 1.608\n",
      "[624,   200] loss: 1.607\n",
      "[625,   200] loss: 1.607\n",
      "[626,   200] loss: 1.606\n",
      "[627,   200] loss: 1.605\n",
      "[628,   200] loss: 1.604\n",
      "[629,   200] loss: 1.603\n",
      "[630,   200] loss: 1.602\n",
      "[631,   200] loss: 1.601\n",
      "[632,   200] loss: 1.600\n",
      "[633,   200] loss: 1.599\n",
      "[634,   200] loss: 1.599\n",
      "[635,   200] loss: 1.598\n",
      "[636,   200] loss: 1.597\n",
      "[637,   200] loss: 1.596\n",
      "[638,   200] loss: 1.595\n",
      "[639,   200] loss: 1.594\n",
      "[640,   200] loss: 1.593\n",
      "[641,   200] loss: 1.592\n",
      "[642,   200] loss: 1.591\n",
      "[643,   200] loss: 1.591\n",
      "[644,   200] loss: 1.590\n",
      "[645,   200] loss: 1.589\n",
      "[646,   200] loss: 1.588\n",
      "[647,   200] loss: 1.587\n",
      "[648,   200] loss: 1.586\n",
      "[649,   200] loss: 1.585\n",
      "[650,   200] loss: 1.584\n",
      "[651,   200] loss: 1.583\n",
      "[652,   200] loss: 1.583\n",
      "[653,   200] loss: 1.582\n",
      "[654,   200] loss: 1.581\n",
      "[655,   200] loss: 1.580\n",
      "[656,   200] loss: 1.579\n",
      "[657,   200] loss: 1.578\n",
      "[658,   200] loss: 1.577\n",
      "[659,   200] loss: 1.576\n",
      "[660,   200] loss: 1.575\n",
      "[661,   200] loss: 1.575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[662,   200] loss: 1.574\n",
      "[663,   200] loss: 1.573\n",
      "[664,   200] loss: 1.572\n",
      "[665,   200] loss: 1.571\n",
      "[666,   200] loss: 1.570\n",
      "[667,   200] loss: 1.569\n",
      "[668,   200] loss: 1.568\n",
      "[669,   200] loss: 1.567\n",
      "[670,   200] loss: 1.567\n",
      "[671,   200] loss: 1.566\n",
      "[672,   200] loss: 1.565\n",
      "[673,   200] loss: 1.564\n",
      "[674,   200] loss: 1.563\n",
      "[675,   200] loss: 1.562\n",
      "[676,   200] loss: 1.561\n",
      "[677,   200] loss: 1.560\n",
      "[678,   200] loss: 1.559\n",
      "[679,   200] loss: 1.559\n",
      "[680,   200] loss: 1.558\n",
      "[681,   200] loss: 1.557\n",
      "[682,   200] loss: 1.556\n",
      "[683,   200] loss: 1.555\n",
      "[684,   200] loss: 1.554\n",
      "[685,   200] loss: 1.553\n",
      "[686,   200] loss: 1.552\n",
      "[687,   200] loss: 1.551\n",
      "[688,   200] loss: 1.551\n",
      "[689,   200] loss: 1.550\n",
      "[690,   200] loss: 1.549\n",
      "[691,   200] loss: 1.548\n",
      "[692,   200] loss: 1.547\n",
      "[693,   200] loss: 1.546\n",
      "[694,   200] loss: 1.545\n",
      "[695,   200] loss: 1.544\n",
      "[696,   200] loss: 1.543\n",
      "[697,   200] loss: 1.543\n",
      "[698,   200] loss: 1.542\n",
      "[699,   200] loss: 1.541\n",
      "[700,   200] loss: 1.540\n",
      "[701,   200] loss: 1.539\n",
      "[702,   200] loss: 1.538\n",
      "[703,   200] loss: 1.537\n",
      "[704,   200] loss: 1.536\n",
      "[705,   200] loss: 1.535\n",
      "[706,   200] loss: 1.535\n",
      "[707,   200] loss: 1.534\n",
      "[708,   200] loss: 1.533\n",
      "[709,   200] loss: 1.532\n",
      "[710,   200] loss: 1.531\n",
      "[711,   200] loss: 1.530\n",
      "[712,   200] loss: 1.529\n",
      "[713,   200] loss: 1.528\n",
      "[714,   200] loss: 1.528\n",
      "[715,   200] loss: 1.527\n",
      "[716,   200] loss: 1.526\n",
      "[717,   200] loss: 1.525\n",
      "[718,   200] loss: 1.524\n",
      "[719,   200] loss: 1.523\n",
      "[720,   200] loss: 1.522\n",
      "[721,   200] loss: 1.522\n",
      "[722,   200] loss: 1.521\n",
      "[723,   200] loss: 1.520\n",
      "[724,   200] loss: 1.519\n",
      "[725,   200] loss: 1.518\n",
      "[726,   200] loss: 1.517\n",
      "[727,   200] loss: 1.516\n",
      "[728,   200] loss: 1.515\n",
      "[729,   200] loss: 1.515\n",
      "[730,   200] loss: 1.514\n",
      "[731,   200] loss: 1.513\n",
      "[732,   200] loss: 1.512\n",
      "[733,   200] loss: 1.511\n",
      "[734,   200] loss: 1.510\n",
      "[735,   200] loss: 1.509\n",
      "[736,   200] loss: 1.509\n",
      "[737,   200] loss: 1.508\n",
      "[738,   200] loss: 1.507\n",
      "[739,   200] loss: 1.506\n",
      "[740,   200] loss: 1.505\n",
      "[741,   200] loss: 1.504\n",
      "[742,   200] loss: 1.504\n",
      "[743,   200] loss: 1.503\n",
      "[744,   200] loss: 1.502\n",
      "[745,   200] loss: 1.501\n",
      "[746,   200] loss: 1.500\n",
      "[747,   200] loss: 1.499\n",
      "[748,   200] loss: 1.498\n",
      "[749,   200] loss: 1.498\n",
      "[750,   200] loss: 1.497\n",
      "[751,   200] loss: 1.496\n",
      "[752,   200] loss: 1.495\n",
      "[753,   200] loss: 1.494\n",
      "[754,   200] loss: 1.493\n",
      "[755,   200] loss: 1.493\n",
      "[756,   200] loss: 1.492\n",
      "[757,   200] loss: 1.491\n",
      "[758,   200] loss: 1.490\n",
      "[759,   200] loss: 1.489\n",
      "[760,   200] loss: 1.488\n",
      "[761,   200] loss: 1.488\n",
      "[762,   200] loss: 1.487\n",
      "[763,   200] loss: 1.486\n",
      "[764,   200] loss: 1.485\n",
      "[765,   200] loss: 1.484\n",
      "[766,   200] loss: 1.483\n",
      "[767,   200] loss: 1.483\n",
      "[768,   200] loss: 1.482\n",
      "[769,   200] loss: 1.481\n",
      "[770,   200] loss: 1.480\n",
      "[771,   200] loss: 1.479\n",
      "[772,   200] loss: 1.478\n",
      "[773,   200] loss: 1.478\n",
      "[774,   200] loss: 1.477\n",
      "[775,   200] loss: 1.476\n",
      "[776,   200] loss: 1.475\n",
      "[777,   200] loss: 1.474\n",
      "[778,   200] loss: 1.473\n",
      "[779,   200] loss: 1.473\n",
      "[780,   200] loss: 1.472\n",
      "[781,   200] loss: 1.471\n",
      "[782,   200] loss: 1.470\n",
      "[783,   200] loss: 1.469\n",
      "[784,   200] loss: 1.469\n",
      "[785,   200] loss: 1.468\n",
      "[786,   200] loss: 1.467\n",
      "[787,   200] loss: 1.466\n",
      "[788,   200] loss: 1.465\n",
      "[789,   200] loss: 1.465\n",
      "[790,   200] loss: 1.464\n",
      "[791,   200] loss: 1.463\n",
      "[792,   200] loss: 1.462\n",
      "[793,   200] loss: 1.461\n",
      "[794,   200] loss: 1.461\n",
      "[795,   200] loss: 1.460\n",
      "[796,   200] loss: 1.459\n",
      "[797,   200] loss: 1.458\n",
      "[798,   200] loss: 1.457\n",
      "[799,   200] loss: 1.457\n",
      "[800,   200] loss: 1.456\n",
      "[801,   200] loss: 1.455\n",
      "[802,   200] loss: 1.454\n",
      "[803,   200] loss: 1.453\n",
      "[804,   200] loss: 1.453\n",
      "[805,   200] loss: 1.452\n",
      "[806,   200] loss: 1.451\n",
      "[807,   200] loss: 1.450\n",
      "[808,   200] loss: 1.449\n",
      "[809,   200] loss: 1.449\n",
      "[810,   200] loss: 1.448\n",
      "[811,   200] loss: 1.447\n",
      "[812,   200] loss: 1.446\n",
      "[813,   200] loss: 1.445\n",
      "[814,   200] loss: 1.445\n",
      "[815,   200] loss: 1.444\n",
      "[816,   200] loss: 1.443\n",
      "[817,   200] loss: 1.442\n",
      "[818,   200] loss: 1.441\n",
      "[819,   200] loss: 1.441\n",
      "[820,   200] loss: 1.440\n",
      "[821,   200] loss: 1.439\n",
      "[822,   200] loss: 1.438\n",
      "[823,   200] loss: 1.437\n",
      "[824,   200] loss: 1.437\n",
      "[825,   200] loss: 1.436\n",
      "[826,   200] loss: 1.435\n",
      "[827,   200] loss: 1.434\n",
      "[828,   200] loss: 1.434\n",
      "[829,   200] loss: 1.433\n",
      "[830,   200] loss: 1.432\n",
      "[831,   200] loss: 1.431\n",
      "[832,   200] loss: 1.430\n",
      "[833,   200] loss: 1.430\n",
      "[834,   200] loss: 1.429\n",
      "[835,   200] loss: 1.428\n",
      "[836,   200] loss: 1.427\n",
      "[837,   200] loss: 1.427\n",
      "[838,   200] loss: 1.426\n",
      "[839,   200] loss: 1.425\n",
      "[840,   200] loss: 1.424\n",
      "[841,   200] loss: 1.424\n",
      "[842,   200] loss: 1.423\n",
      "[843,   200] loss: 1.422\n",
      "[844,   200] loss: 1.421\n",
      "[845,   200] loss: 1.421\n",
      "[846,   200] loss: 1.420\n",
      "[847,   200] loss: 1.419\n",
      "[848,   200] loss: 1.418\n",
      "[849,   200] loss: 1.418\n",
      "[850,   200] loss: 1.417\n",
      "[851,   200] loss: 1.416\n",
      "[852,   200] loss: 1.415\n",
      "[853,   200] loss: 1.415\n",
      "[854,   200] loss: 1.414\n",
      "[855,   200] loss: 1.413\n",
      "[856,   200] loss: 1.412\n",
      "[857,   200] loss: 1.412\n",
      "[858,   200] loss: 1.411\n",
      "[859,   200] loss: 1.410\n",
      "[860,   200] loss: 1.409\n",
      "[861,   200] loss: 1.409\n",
      "[862,   200] loss: 1.408\n",
      "[863,   200] loss: 1.407\n",
      "[864,   200] loss: 1.406\n",
      "[865,   200] loss: 1.406\n",
      "[866,   200] loss: 1.405\n",
      "[867,   200] loss: 1.404\n",
      "[868,   200] loss: 1.404\n",
      "[869,   200] loss: 1.403\n",
      "[870,   200] loss: 1.402\n",
      "[871,   200] loss: 1.401\n",
      "[872,   200] loss: 1.401\n",
      "[873,   200] loss: 1.400\n",
      "[874,   200] loss: 1.399\n",
      "[875,   200] loss: 1.398\n",
      "[876,   200] loss: 1.398\n",
      "[877,   200] loss: 1.397\n",
      "[878,   200] loss: 1.396\n",
      "[879,   200] loss: 1.395\n",
      "[880,   200] loss: 1.395\n",
      "[881,   200] loss: 1.394\n",
      "[882,   200] loss: 1.393\n",
      "[883,   200] loss: 1.393\n",
      "[884,   200] loss: 1.392\n",
      "[885,   200] loss: 1.391\n",
      "[886,   200] loss: 1.390\n",
      "[887,   200] loss: 1.390\n",
      "[888,   200] loss: 1.389\n",
      "[889,   200] loss: 1.388\n",
      "[890,   200] loss: 1.387\n",
      "[891,   200] loss: 1.387\n",
      "[892,   200] loss: 1.386\n",
      "[893,   200] loss: 1.385\n",
      "[894,   200] loss: 1.385\n",
      "[895,   200] loss: 1.384\n",
      "[896,   200] loss: 1.383\n",
      "[897,   200] loss: 1.382\n",
      "[898,   200] loss: 1.382\n",
      "[899,   200] loss: 1.381\n",
      "[900,   200] loss: 1.380\n",
      "[901,   200] loss: 1.380\n",
      "[902,   200] loss: 1.379\n",
      "[903,   200] loss: 1.378\n",
      "[904,   200] loss: 1.377\n",
      "[905,   200] loss: 1.377\n",
      "[906,   200] loss: 1.376\n",
      "[907,   200] loss: 1.375\n",
      "[908,   200] loss: 1.374\n",
      "[909,   200] loss: 1.374\n",
      "[910,   200] loss: 1.373\n",
      "[911,   200] loss: 1.372\n",
      "[912,   200] loss: 1.372\n",
      "[913,   200] loss: 1.371\n",
      "[914,   200] loss: 1.370\n",
      "[915,   200] loss: 1.369\n",
      "[916,   200] loss: 1.369\n",
      "[917,   200] loss: 1.368\n",
      "[918,   200] loss: 1.367\n",
      "[919,   200] loss: 1.367\n",
      "[920,   200] loss: 1.366\n",
      "[921,   200] loss: 1.365\n",
      "[922,   200] loss: 1.364\n",
      "[923,   200] loss: 1.364\n",
      "[924,   200] loss: 1.363\n",
      "[925,   200] loss: 1.362\n",
      "[926,   200] loss: 1.362\n",
      "[927,   200] loss: 1.361\n",
      "[928,   200] loss: 1.360\n",
      "[929,   200] loss: 1.359\n",
      "[930,   200] loss: 1.359\n",
      "[931,   200] loss: 1.358\n",
      "[932,   200] loss: 1.357\n",
      "[933,   200] loss: 1.357\n",
      "[934,   200] loss: 1.356\n",
      "[935,   200] loss: 1.355\n",
      "[936,   200] loss: 1.355\n",
      "[937,   200] loss: 1.354\n",
      "[938,   200] loss: 1.353\n",
      "[939,   200] loss: 1.352\n",
      "[940,   200] loss: 1.352\n",
      "[941,   200] loss: 1.351\n",
      "[942,   200] loss: 1.350\n",
      "[943,   200] loss: 1.350\n",
      "[944,   200] loss: 1.349\n",
      "[945,   200] loss: 1.348\n",
      "[946,   200] loss: 1.348\n",
      "[947,   200] loss: 1.347\n",
      "[948,   200] loss: 1.346\n",
      "[949,   200] loss: 1.345\n",
      "[950,   200] loss: 1.345\n",
      "[951,   200] loss: 1.344\n",
      "[952,   200] loss: 1.343\n",
      "[953,   200] loss: 1.343\n",
      "[954,   200] loss: 1.342\n",
      "[955,   200] loss: 1.341\n",
      "[956,   200] loss: 1.340\n",
      "[957,   200] loss: 1.340\n",
      "[958,   200] loss: 1.339\n",
      "[959,   200] loss: 1.338\n",
      "[960,   200] loss: 1.338\n",
      "[961,   200] loss: 1.337\n",
      "[962,   200] loss: 1.336\n",
      "[963,   200] loss: 1.336\n",
      "[964,   200] loss: 1.335\n",
      "[965,   200] loss: 1.334\n",
      "[966,   200] loss: 1.334\n",
      "[967,   200] loss: 1.333\n",
      "[968,   200] loss: 1.332\n",
      "[969,   200] loss: 1.332\n",
      "[970,   200] loss: 1.331\n",
      "[971,   200] loss: 1.330\n",
      "[972,   200] loss: 1.329\n",
      "[973,   200] loss: 1.329\n",
      "[974,   200] loss: 1.328\n",
      "[975,   200] loss: 1.328\n",
      "[976,   200] loss: 1.327\n",
      "[977,   200] loss: 1.326\n",
      "[978,   200] loss: 1.325\n",
      "[979,   200] loss: 1.325\n",
      "[980,   200] loss: 1.324\n",
      "[981,   200] loss: 1.323\n",
      "[982,   200] loss: 1.323\n",
      "[983,   200] loss: 1.322\n",
      "[984,   200] loss: 1.321\n",
      "[985,   200] loss: 1.320\n",
      "[986,   200] loss: 1.320\n",
      "[987,   200] loss: 1.319\n",
      "[988,   200] loss: 1.318\n",
      "[989,   200] loss: 1.318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[990,   200] loss: 1.316\n",
      "[991,   200] loss: 1.316\n",
      "[992,   200] loss: 1.316\n",
      "[993,   200] loss: 1.315\n",
      "[994,   200] loss: 1.314\n",
      "[995,   200] loss: 1.313\n",
      "[996,   200] loss: 1.313\n",
      "[997,   200] loss: 1.312\n",
      "[998,   200] loss: 1.311\n",
      "[999,   200] loss: 1.311\n",
      "[1000,   200] loss: 1.310\n",
      "[1001,   200] loss: 1.309\n",
      "[1002,   200] loss: 1.308\n",
      "[1003,   200] loss: 1.308\n",
      "[1004,   200] loss: 1.307\n",
      "[1005,   200] loss: 1.306\n",
      "[1006,   200] loss: 1.306\n",
      "[1007,   200] loss: 1.305\n",
      "[1008,   200] loss: 1.305\n",
      "[1009,   200] loss: 1.304\n",
      "[1010,   200] loss: 1.303\n",
      "[1011,   200] loss: 1.302\n",
      "[1012,   200] loss: 1.302\n",
      "[1013,   200] loss: 1.302\n",
      "[1014,   200] loss: 1.301\n",
      "[1015,   200] loss: 1.299\n",
      "[1016,   200] loss: 1.300\n",
      "[1017,   200] loss: 1.299\n",
      "[1018,   200] loss: 1.298\n",
      "[1019,   200] loss: 1.298\n",
      "[1020,   200] loss: 1.296\n",
      "[1021,   200] loss: 1.296\n",
      "[1022,   200] loss: 1.295\n",
      "[1023,   200] loss: 1.295\n",
      "[1024,   200] loss: 1.295\n",
      "[1025,   200] loss: 1.293\n",
      "[1026,   200] loss: 1.292\n",
      "[1027,   200] loss: 1.292\n",
      "[1028,   200] loss: 1.290\n",
      "[1029,   200] loss: 1.290\n",
      "[1030,   200] loss: 1.291\n",
      "[1031,   200] loss: 1.289\n",
      "[1032,   200] loss: 1.288\n",
      "[1033,   200] loss: 1.287\n",
      "[1034,   200] loss: 1.288\n",
      "[1035,   200] loss: 1.287\n",
      "[1036,   200] loss: 1.285\n",
      "[1037,   200] loss: 1.284\n",
      "[1038,   200] loss: 1.286\n",
      "[1039,   200] loss: 1.284\n",
      "[1040,   200] loss: 1.283\n",
      "[1041,   200] loss: 1.283\n",
      "[1042,   200] loss: 1.281\n",
      "[1043,   200] loss: 1.282\n",
      "[1044,   200] loss: 1.280\n",
      "[1045,   200] loss: 1.280\n",
      "[1046,   200] loss: 1.281\n",
      "[1047,   200] loss: 1.280\n",
      "[1048,   200] loss: 1.273\n",
      "[1049,   200] loss: 1.282\n",
      "[1050,   200] loss: 1.278\n",
      "[1051,   200] loss: 1.276\n",
      "[1052,   200] loss: 1.274\n",
      "[1053,   200] loss: 1.268\n",
      "[1054,   200] loss: 1.279\n",
      "[1055,   200] loss: 1.276\n",
      "[1056,   200] loss: 1.271\n",
      "[1057,   200] loss: 1.269\n",
      "[1058,   200] loss: 1.276\n",
      "[1059,   200] loss: 1.272\n",
      "[1060,   200] loss: 1.266\n",
      "[1061,   200] loss: 1.266\n",
      "[1062,   200] loss: 1.270\n",
      "[1063,   200] loss: 1.271\n",
      "[1064,   200] loss: 1.263\n",
      "[1065,   200] loss: 1.264\n",
      "[1066,   200] loss: 1.261\n",
      "[1067,   200] loss: 1.271\n",
      "[1068,   200] loss: 1.266\n",
      "[1069,   200] loss: 1.268\n",
      "[1070,   200] loss: 1.267\n",
      "[1071,   200] loss: 1.257\n",
      "[1072,   200] loss: 1.272\n",
      "[1073,   200] loss: 1.260\n",
      "[1074,   200] loss: 1.266\n",
      "[1075,   200] loss: 1.261\n",
      "[1076,   200] loss: 1.267\n",
      "[1077,   200] loss: 1.268\n",
      "[1078,   200] loss: 1.253\n",
      "[1079,   200] loss: 1.251\n",
      "[1080,   200] loss: 1.268\n",
      "[1081,   200] loss: 1.261\n",
      "[1082,   200] loss: 1.264\n",
      "[1083,   200] loss: 1.260\n",
      "[1084,   200] loss: 1.253\n",
      "[1085,   200] loss: 1.255\n",
      "[1086,   200] loss: 1.263\n",
      "[1087,   200] loss: 1.255\n",
      "[1088,   200] loss: 1.257\n",
      "[1089,   200] loss: 1.249\n",
      "[1090,   200] loss: 1.257\n",
      "[1091,   200] loss: 1.257\n",
      "[1092,   200] loss: 1.252\n",
      "[1093,   200] loss: 1.254\n",
      "[1094,   200] loss: 1.251\n",
      "[1095,   200] loss: 1.253\n",
      "[1096,   200] loss: 1.245\n",
      "[1097,   200] loss: 1.246\n",
      "[1098,   200] loss: 1.249\n",
      "[1099,   200] loss: 1.250\n",
      "[1100,   200] loss: 1.247\n",
      "[1101,   200] loss: 1.249\n",
      "[1102,   200] loss: 1.243\n",
      "[1103,   200] loss: 1.250\n",
      "[1104,   200] loss: 1.247\n",
      "[1105,   200] loss: 1.244\n",
      "[1106,   200] loss: 1.246\n",
      "[1107,   200] loss: 1.242\n",
      "[1108,   200] loss: 1.241\n",
      "[1109,   200] loss: 1.239\n",
      "[1110,   200] loss: 1.244\n",
      "[1111,   200] loss: 1.234\n",
      "[1112,   200] loss: 1.244\n",
      "[1113,   200] loss: 1.235\n",
      "[1114,   200] loss: 1.240\n",
      "[1115,   200] loss: 1.243\n",
      "[1116,   200] loss: 1.238\n",
      "[1117,   200] loss: 1.230\n",
      "[1118,   200] loss: 1.239\n",
      "[1119,   200] loss: 1.242\n",
      "[1120,   200] loss: 1.235\n",
      "[1121,   200] loss: 1.234\n",
      "[1122,   200] loss: 1.235\n",
      "[1123,   200] loss: 1.231\n",
      "[1124,   200] loss: 1.234\n",
      "[1125,   200] loss: 1.234\n",
      "[1126,   200] loss: 1.234\n",
      "[1127,   200] loss: 1.229\n",
      "[1128,   200] loss: 1.230\n",
      "[1129,   200] loss: 1.230\n",
      "[1130,   200] loss: 1.227\n",
      "[1131,   200] loss: 1.221\n",
      "[1132,   200] loss: 1.230\n",
      "[1133,   200] loss: 1.228\n",
      "[1134,   200] loss: 1.225\n",
      "[1135,   200] loss: 1.230\n",
      "[1136,   200] loss: 1.227\n",
      "[1137,   200] loss: 1.228\n",
      "[1138,   200] loss: 1.228\n",
      "[1139,   200] loss: 1.225\n",
      "[1140,   200] loss: 1.220\n",
      "[1141,   200] loss: 1.224\n",
      "[1142,   200] loss: 1.220\n",
      "[1143,   200] loss: 1.225\n",
      "[1144,   200] loss: 1.217\n",
      "[1145,   200] loss: 1.219\n",
      "[1146,   200] loss: 1.216\n",
      "[1147,   200] loss: 1.220\n",
      "[1148,   200] loss: 1.216\n",
      "[1149,   200] loss: 1.216\n",
      "[1150,   200] loss: 1.213\n",
      "[1151,   200] loss: 1.210\n",
      "[1152,   200] loss: 1.222\n",
      "[1153,   200] loss: 1.212\n",
      "[1154,   200] loss: 1.211\n",
      "[1155,   200] loss: 1.218\n",
      "[1156,   200] loss: 1.211\n",
      "[1157,   200] loss: 1.212\n",
      "[1158,   200] loss: 1.207\n",
      "[1159,   200] loss: 1.200\n",
      "[1160,   200] loss: 1.201\n",
      "[1161,   200] loss: 1.209\n",
      "[1162,   200] loss: 1.216\n",
      "[1163,   200] loss: 1.208\n",
      "[1164,   200] loss: 1.207\n",
      "[1165,   200] loss: 1.206\n",
      "[1166,   200] loss: 1.203\n",
      "[1167,   200] loss: 1.197\n",
      "[1168,   200] loss: 1.208\n",
      "[1169,   200] loss: 1.212\n",
      "[1170,   200] loss: 1.206\n",
      "[1171,   200] loss: 1.198\n",
      "[1172,   200] loss: 1.206\n",
      "[1173,   200] loss: 1.194\n",
      "[1174,   200] loss: 1.211\n",
      "[1175,   200] loss: 1.193\n",
      "[1176,   200] loss: 1.197\n",
      "[1177,   200] loss: 1.199\n",
      "[1178,   200] loss: 1.205\n",
      "[1179,   200] loss: 1.199\n",
      "[1180,   200] loss: 1.194\n",
      "[1181,   200] loss: 1.198\n",
      "[1182,   200] loss: 1.200\n",
      "[1183,   200] loss: 1.194\n",
      "[1184,   200] loss: 1.193\n",
      "[1185,   200] loss: 1.193\n",
      "[1186,   200] loss: 1.200\n",
      "[1187,   200] loss: 1.193\n",
      "[1188,   200] loss: 1.192\n",
      "[1189,   200] loss: 1.188\n",
      "[1190,   200] loss: 1.192\n",
      "[1191,   200] loss: 1.185\n",
      "[1192,   200] loss: 1.190\n",
      "[1193,   200] loss: 1.194\n",
      "[1194,   200] loss: 1.182\n",
      "[1195,   200] loss: 1.194\n",
      "[1196,   200] loss: 1.182\n",
      "[1197,   200] loss: 1.180\n",
      "[1198,   200] loss: 1.188\n",
      "[1199,   200] loss: 1.183\n",
      "[1200,   200] loss: 1.184\n",
      "[1201,   200] loss: 1.179\n",
      "[1202,   200] loss: 1.184\n",
      "[1203,   200] loss: 1.182\n",
      "[1204,   200] loss: 1.182\n",
      "[1205,   200] loss: 1.172\n",
      "[1206,   200] loss: 1.173\n",
      "[1207,   200] loss: 1.173\n",
      "[1208,   200] loss: 1.176\n",
      "[1209,   200] loss: 1.176\n",
      "[1210,   200] loss: 1.187\n",
      "[1211,   200] loss: 1.173\n",
      "[1212,   200] loss: 1.179\n",
      "[1213,   200] loss: 1.176\n",
      "[1214,   200] loss: 1.180\n",
      "[1215,   200] loss: 1.173\n",
      "[1216,   200] loss: 1.175\n",
      "[1217,   200] loss: 1.181\n",
      "[1218,   200] loss: 1.168\n",
      "[1219,   200] loss: 1.169\n",
      "[1220,   200] loss: 1.168\n",
      "[1221,   200] loss: 1.170\n",
      "[1222,   200] loss: 1.179\n",
      "[1223,   200] loss: 1.175\n",
      "[1224,   200] loss: 1.170\n",
      "[1225,   200] loss: 1.169\n",
      "[1226,   200] loss: 1.162\n",
      "[1227,   200] loss: 1.165\n",
      "[1228,   200] loss: 1.172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-21db6d5fde4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Realizar o treinamento aqui\n",
    "\n",
    "inputs_list = []\n",
    "labels_list = []\n",
    "for _, (inputs, labels) in enumerate(train_loader):\n",
    "    inputs_list.append(inputs.to(device))\n",
    "    labels_list.append(labels.to(device))\n",
    "        \n",
    "losses = []\n",
    "for epoch in range(5000):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(zip(inputs_list, labels_list), 0):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(inputs)\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Cross Entropy durante o Treinamento\")\n",
    "plt.tight_layout()\n",
    "_ = plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(results):\n",
    "    results = results.cpu().detach().numpy().tolist()[0]\n",
    "    return results.index(max(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.812 %\n",
      "[5426, 4780, 5024, 3938, 5537, 4141, 4988, 6098, 4892, 5176]\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=1)\n",
    "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=1)\n",
    "\n",
    "# Avaliar o modelo aqui (no conjunto de teste)\n",
    "inputs_list = []\n",
    "labels_list = []\n",
    "for _, (inputs, labels) in enumerate(train_loader):\n",
    "    inputs_list.append(inputs.to(device))\n",
    "    labels_list.append(labels.to(device))\n",
    "\n",
    "\n",
    "acuracia = 0\n",
    "results = [0,0,0,0,0,0,0,0,0,0]\n",
    "for i, (inputs, labels) in enumerate(zip(inputs_list, labels_list), 0):\n",
    "    y_pred = model(inputs)\n",
    "    \n",
    "    result = one_hot(y_pred)\n",
    "    \n",
    "    if result == labels.item():\n",
    "        acuracia += 1\n",
    "    \n",
    "    results[result] += 1\n",
    "    \n",
    "print(acuracia / 50000 * 100, \"%\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.882 %\n",
      "[1094, 882, 1010, 747, 1129, 861, 986, 1241, 984, 1066]\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=1)\n",
    "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=1)\n",
    "\n",
    "# Avaliar o modelo aqui (no conjunto de teste)\n",
    "inputs_list = []\n",
    "labels_list = []\n",
    "for _, (inputs, labels) in enumerate(test_loader):\n",
    "    inputs_list.append(inputs.to(device))\n",
    "    labels_list.append(labels.to(device))\n",
    "\n",
    "\n",
    "acuracia = 0\n",
    "results = [0,0,0,0,0,0,0,0,0,0]\n",
    "for i, (inputs, labels) in enumerate(zip(inputs_list, labels_list), 0):\n",
    "    y_pred = model(inputs)\n",
    "    \n",
    "    result = one_hot(y_pred)\n",
    "    \n",
    "    if result == labels.item():\n",
    "        acuracia += 1\n",
    "    \n",
    "    results[result] += 1\n",
    "    \n",
    "print(acuracia / 50000 * 100, \"%\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
