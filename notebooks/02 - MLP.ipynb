{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos deste trabalho\n",
    "- Familiarizar-se com a biblioteca PyTorch\n",
    "- Definir arquiteturas MLP simples em PyTorch\n",
    "- Treinar utilizando CIFAR10, testando diferentes arquiteturas, parâmetros, funções de loss e otimizadores\n",
    "- Comparar os resultados obtidos utilizando apenas Perpceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Carregar os datasets\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "dataset_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=200)\n",
    "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a arquitetura MLP\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.activation_function = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32)\n",
    "        x = self.activation_function(self.fc1(x))\n",
    "        x = self.activation_function(self.fc2(x))\n",
    "        x = self.activation_function(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "MLP(\n",
      "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (activation_function): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "# Definir otimizador e loss\n",
    "# Nota: testar outros otimizadores e funções de loss (em particular cross entropy)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2, weight_decay=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "print(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar o treinamento aqui\n",
    "def train_model(model, epochs, train_loader, losses):\n",
    "    \n",
    "    inputs_list = []\n",
    "    labels_list = []\n",
    "    for _, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs_list.append(inputs.to(device))\n",
    "        labels_list.append(labels.to(device))\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(zip(inputs_list, labels_list), 0):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(inputs)\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "        \n",
    "            running_loss += loss.item()\n",
    "            if i % 250 == 249:\n",
    "                print('[%d] loss: %.3f' %\n",
    "                      (epoch + 1, running_loss / 250))\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainamento do Modelo\n",
    "- Ativação: ReLu\n",
    "- Loss: CrossEntropy\n",
    "- Hidden: 3\n",
    "- Regularizador: L2\n",
    "- Taxa de Aprendizado = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 2.296\n",
      "[2] loss: 2.281\n",
      "[3] loss: 2.259\n",
      "[4] loss: 2.227\n",
      "[5] loss: 2.194\n",
      "[6] loss: 2.171\n",
      "[7] loss: 2.151\n",
      "[8] loss: 2.131\n",
      "[9] loss: 2.114\n",
      "[10] loss: 2.100\n",
      "[11] loss: 2.089\n",
      "[12] loss: 2.079\n",
      "[13] loss: 2.071\n",
      "[14] loss: 2.063\n",
      "[15] loss: 2.056\n",
      "[16] loss: 2.049\n",
      "[17] loss: 2.043\n",
      "[18] loss: 2.037\n",
      "[19] loss: 2.032\n",
      "[20] loss: 2.026\n",
      "[21] loss: 2.020\n",
      "[22] loss: 2.015\n",
      "[23] loss: 2.009\n",
      "[24] loss: 2.004\n",
      "[25] loss: 1.998\n",
      "[26] loss: 1.993\n",
      "[27] loss: 1.987\n",
      "[28] loss: 1.981\n",
      "[29] loss: 1.974\n",
      "[30] loss: 1.968\n",
      "[31] loss: 1.962\n",
      "[32] loss: 1.956\n",
      "[33] loss: 1.949\n",
      "[34] loss: 1.943\n",
      "[35] loss: 1.937\n",
      "[36] loss: 1.931\n",
      "[37] loss: 1.926\n",
      "[38] loss: 1.920\n",
      "[39] loss: 1.915\n",
      "[40] loss: 1.909\n",
      "[41] loss: 1.904\n",
      "[42] loss: 1.899\n",
      "[43] loss: 1.894\n",
      "[44] loss: 1.889\n",
      "[45] loss: 1.884\n",
      "[46] loss: 1.879\n",
      "[47] loss: 1.874\n",
      "[48] loss: 1.869\n",
      "[49] loss: 1.865\n",
      "[50] loss: 1.860\n",
      "[51] loss: 1.855\n",
      "[52] loss: 1.850\n",
      "[53] loss: 1.846\n",
      "[54] loss: 1.841\n",
      "[55] loss: 1.836\n",
      "[56] loss: 1.832\n",
      "[57] loss: 1.827\n",
      "[58] loss: 1.823\n",
      "[59] loss: 1.818\n",
      "[60] loss: 1.814\n",
      "[61] loss: 1.810\n",
      "[62] loss: 1.805\n",
      "[63] loss: 1.801\n",
      "[64] loss: 1.797\n",
      "[65] loss: 1.792\n",
      "[66] loss: 1.745\n",
      "[67] loss: 1.730\n",
      "[68] loss: 1.722\n",
      "[69] loss: 1.716\n",
      "[70] loss: 1.710\n",
      "[71] loss: 1.704\n",
      "[72] loss: 1.699\n",
      "[73] loss: 1.694\n",
      "[74] loss: 1.689\n",
      "[75] loss: 1.684\n",
      "[76] loss: 1.679\n",
      "[77] loss: 1.674\n",
      "[78] loss: 1.669\n",
      "[79] loss: 1.665\n",
      "[80] loss: 1.660\n",
      "[81] loss: 1.656\n",
      "[82] loss: 1.651\n",
      "[83] loss: 1.647\n",
      "[84] loss: 1.643\n",
      "[85] loss: 1.639\n",
      "[86] loss: 1.634\n",
      "[87] loss: 1.630\n",
      "[88] loss: 1.627\n",
      "[89] loss: 1.622\n",
      "[90] loss: 1.619\n",
      "[91] loss: 1.615\n",
      "[92] loss: 1.611\n",
      "[93] loss: 1.607\n",
      "[94] loss: 1.603\n",
      "[95] loss: 1.600\n",
      "[96] loss: 1.596\n",
      "[97] loss: 1.593\n",
      "[98] loss: 1.589\n",
      "[99] loss: 1.586\n",
      "[100] loss: 1.582\n",
      "[101] loss: 1.579\n",
      "[102] loss: 1.575\n",
      "[103] loss: 1.572\n",
      "[104] loss: 1.569\n",
      "[105] loss: 1.565\n",
      "[106] loss: 1.562\n",
      "[107] loss: 1.559\n",
      "[108] loss: 1.556\n",
      "[109] loss: 1.552\n",
      "[110] loss: 1.550\n",
      "[111] loss: 1.546\n",
      "[112] loss: 1.543\n",
      "[113] loss: 1.540\n",
      "[114] loss: 1.537\n",
      "[115] loss: 1.534\n",
      "[116] loss: 1.531\n",
      "[117] loss: 1.528\n",
      "[118] loss: 1.525\n",
      "[119] loss: 1.523\n",
      "[120] loss: 1.520\n",
      "[121] loss: 1.517\n",
      "[122] loss: 1.515\n",
      "[123] loss: 1.511\n",
      "[124] loss: 1.509\n",
      "[125] loss: 1.507\n",
      "[126] loss: 1.504\n",
      "[127] loss: 1.501\n",
      "[128] loss: 1.498\n",
      "[129] loss: 1.495\n",
      "[130] loss: 1.493\n",
      "[131] loss: 1.490\n",
      "[132] loss: 1.488\n",
      "[133] loss: 1.485\n",
      "[134] loss: 1.483\n",
      "[135] loss: 1.480\n",
      "[136] loss: 1.478\n",
      "[137] loss: 1.475\n",
      "[138] loss: 1.473\n",
      "[139] loss: 1.471\n",
      "[140] loss: 1.469\n",
      "[141] loss: 1.466\n",
      "[142] loss: 1.463\n",
      "[143] loss: 1.461\n",
      "[144] loss: 1.458\n",
      "[145] loss: 1.455\n",
      "[146] loss: 1.453\n",
      "[147] loss: 1.450\n",
      "[148] loss: 1.449\n",
      "[149] loss: 1.446\n",
      "[150] loss: 1.443\n",
      "[151] loss: 1.442\n",
      "[152] loss: 1.439\n",
      "[153] loss: 1.436\n",
      "[154] loss: 1.434\n",
      "[155] loss: 1.432\n",
      "[156] loss: 1.429\n",
      "[157] loss: 1.427\n",
      "[158] loss: 1.424\n",
      "[159] loss: 1.423\n",
      "[160] loss: 1.420\n",
      "[161] loss: 1.418\n",
      "[162] loss: 1.416\n",
      "[163] loss: 1.414\n",
      "[164] loss: 1.411\n",
      "[165] loss: 1.409\n",
      "[166] loss: 1.407\n",
      "[167] loss: 1.405\n",
      "[168] loss: 1.403\n",
      "[169] loss: 1.400\n",
      "[170] loss: 1.398\n",
      "[171] loss: 1.396\n",
      "[172] loss: 1.394\n",
      "[173] loss: 1.392\n",
      "[174] loss: 1.390\n",
      "[175] loss: 1.387\n",
      "[176] loss: 1.386\n",
      "[177] loss: 1.383\n",
      "[178] loss: 1.382\n",
      "[179] loss: 1.379\n",
      "[180] loss: 1.378\n",
      "[181] loss: 1.374\n",
      "[182] loss: 1.373\n",
      "[183] loss: 1.370\n",
      "[184] loss: 1.369\n",
      "[185] loss: 1.367\n",
      "[186] loss: 1.365\n",
      "[187] loss: 1.363\n",
      "[188] loss: 1.362\n",
      "[189] loss: 1.359\n",
      "[190] loss: 1.356\n",
      "[191] loss: 1.354\n",
      "[192] loss: 1.352\n",
      "[193] loss: 1.350\n",
      "[194] loss: 1.350\n",
      "[195] loss: 1.347\n",
      "[196] loss: 1.344\n",
      "[197] loss: 1.341\n",
      "[198] loss: 1.339\n",
      "[199] loss: 1.339\n",
      "[200] loss: 1.338\n",
      "[201] loss: 1.335\n",
      "[202] loss: 1.332\n",
      "[203] loss: 1.331\n",
      "[204] loss: 1.327\n",
      "[205] loss: 1.324\n",
      "[206] loss: 1.327\n",
      "[207] loss: 1.323\n",
      "[208] loss: 1.321\n",
      "[209] loss: 1.322\n",
      "[210] loss: 1.317\n",
      "[211] loss: 1.316\n",
      "[212] loss: 1.315\n",
      "[213] loss: 1.310\n",
      "[214] loss: 1.310\n",
      "[215] loss: 1.308\n",
      "[216] loss: 1.307\n",
      "[217] loss: 1.305\n",
      "[218] loss: 1.299\n",
      "[219] loss: 1.298\n",
      "[220] loss: 1.299\n",
      "[221] loss: 1.296\n",
      "[222] loss: 1.293\n",
      "[223] loss: 1.290\n",
      "[224] loss: 1.294\n",
      "[225] loss: 1.291\n",
      "[226] loss: 1.288\n",
      "[227] loss: 1.281\n",
      "[228] loss: 1.285\n",
      "[229] loss: 1.281\n",
      "[230] loss: 1.280\n",
      "[231] loss: 1.276\n",
      "[232] loss: 1.279\n",
      "[233] loss: 1.276\n",
      "[234] loss: 1.273\n",
      "[235] loss: 1.269\n",
      "[236] loss: 1.268\n",
      "[237] loss: 1.268\n",
      "[238] loss: 1.264\n",
      "[239] loss: 1.264\n",
      "[240] loss: 1.259\n",
      "[241] loss: 1.260\n",
      "[242] loss: 1.259\n",
      "[243] loss: 1.255\n",
      "[244] loss: 1.255\n",
      "[245] loss: 1.252\n",
      "[246] loss: 1.247\n",
      "[247] loss: 1.249\n",
      "[248] loss: 1.247\n",
      "[249] loss: 1.247\n",
      "[250] loss: 1.242\n",
      "[251] loss: 1.243\n",
      "[252] loss: 1.236\n",
      "[253] loss: 1.240\n",
      "[254] loss: 1.233\n",
      "[255] loss: 1.239\n",
      "[256] loss: 1.230\n",
      "[257] loss: 1.231\n",
      "[258] loss: 1.229\n",
      "[259] loss: 1.225\n",
      "[260] loss: 1.227\n",
      "[261] loss: 1.221\n",
      "[262] loss: 1.220\n",
      "[263] loss: 1.222\n",
      "[264] loss: 1.220\n",
      "[265] loss: 1.215\n",
      "[266] loss: 1.217\n",
      "[267] loss: 1.214\n",
      "[268] loss: 1.212\n",
      "[269] loss: 1.211\n",
      "[270] loss: 1.209\n",
      "[271] loss: 1.206\n",
      "[272] loss: 1.205\n",
      "[273] loss: 1.203\n",
      "[274] loss: 1.202\n",
      "[275] loss: 1.201\n",
      "[276] loss: 1.199\n",
      "[277] loss: 1.201\n",
      "[278] loss: 1.193\n",
      "[279] loss: 1.193\n",
      "[280] loss: 1.190\n",
      "[281] loss: 1.190\n",
      "[282] loss: 1.191\n",
      "[283] loss: 1.188\n",
      "[284] loss: 1.184\n",
      "[285] loss: 1.185\n",
      "[286] loss: 1.178\n",
      "[287] loss: 1.182\n",
      "[288] loss: 1.179\n",
      "[289] loss: 1.177\n",
      "[290] loss: 1.174\n",
      "[291] loss: 1.175\n",
      "[292] loss: 1.172\n",
      "[293] loss: 1.166\n",
      "[294] loss: 1.173\n",
      "[295] loss: 1.170\n",
      "[296] loss: 1.167\n",
      "[297] loss: 1.162\n",
      "[298] loss: 1.163\n",
      "[299] loss: 1.161\n",
      "[300] loss: 1.157\n",
      "[301] loss: 1.161\n",
      "[302] loss: 1.155\n",
      "[303] loss: 1.147\n",
      "[304] loss: 1.150\n",
      "[305] loss: 1.154\n",
      "[306] loss: 1.155\n",
      "[307] loss: 1.151\n",
      "[308] loss: 1.144\n",
      "[309] loss: 1.143\n",
      "[310] loss: 1.144\n",
      "[311] loss: 1.137\n",
      "[312] loss: 1.138\n",
      "[313] loss: 1.135\n",
      "[314] loss: 1.128\n",
      "[315] loss: 1.138\n",
      "[316] loss: 1.133\n",
      "[317] loss: 1.132\n",
      "[318] loss: 1.133\n",
      "[319] loss: 1.129\n",
      "[320] loss: 1.127\n",
      "[321] loss: 1.125\n",
      "[322] loss: 1.122\n",
      "[323] loss: 1.123\n",
      "[324] loss: 1.121\n",
      "[325] loss: 1.121\n",
      "[326] loss: 1.115\n",
      "[327] loss: 1.113\n",
      "[328] loss: 1.118\n",
      "[329] loss: 1.110\n",
      "[330] loss: 1.110\n",
      "[331] loss: 1.113\n",
      "[332] loss: 1.104\n",
      "[333] loss: 1.105\n",
      "[334] loss: 1.107\n",
      "[335] loss: 1.107\n",
      "[336] loss: 1.103\n",
      "[337] loss: 1.093\n",
      "[338] loss: 1.098\n",
      "[339] loss: 1.105\n",
      "[340] loss: 1.097\n",
      "[341] loss: 1.094\n",
      "[342] loss: 1.098\n",
      "[343] loss: 1.092\n",
      "[344] loss: 1.091\n",
      "[345] loss: 1.091\n",
      "[346] loss: 1.090\n",
      "[347] loss: 1.083\n",
      "[348] loss: 1.091\n",
      "[349] loss: 1.090\n",
      "[350] loss: 1.076\n",
      "[351] loss: 1.079\n",
      "[352] loss: 1.079\n",
      "[353] loss: 1.078\n",
      "[354] loss: 1.074\n",
      "[355] loss: 1.070\n",
      "[356] loss: 1.070\n",
      "[357] loss: 1.074\n",
      "[358] loss: 1.068\n",
      "[359] loss: 1.071\n",
      "[360] loss: 1.067\n",
      "[361] loss: 1.067\n",
      "[362] loss: 1.064\n",
      "[363] loss: 1.058\n",
      "[364] loss: 1.066\n",
      "[365] loss: 1.067\n",
      "[366] loss: 1.056\n",
      "[367] loss: 1.058\n",
      "[368] loss: 1.048\n",
      "[369] loss: 1.054\n",
      "[370] loss: 1.056\n",
      "[371] loss: 1.050\n",
      "[372] loss: 1.052\n",
      "[373] loss: 1.051\n",
      "[374] loss: 1.046\n",
      "[375] loss: 1.037\n",
      "[376] loss: 1.043\n",
      "[377] loss: 1.042\n",
      "[378] loss: 1.041\n",
      "[379] loss: 1.042\n",
      "[380] loss: 1.037\n",
      "[381] loss: 1.031\n",
      "[382] loss: 1.035\n",
      "[383] loss: 1.036\n",
      "[384] loss: 1.032\n",
      "[385] loss: 1.029\n",
      "[386] loss: 1.034\n",
      "[387] loss: 1.029\n",
      "[388] loss: 1.025\n",
      "[389] loss: 1.029\n",
      "[390] loss: 1.020\n",
      "[391] loss: 1.023\n",
      "[392] loss: 1.019\n",
      "[393] loss: 1.021\n",
      "[394] loss: 1.023\n",
      "[395] loss: 1.013\n",
      "[396] loss: 1.016\n",
      "[397] loss: 1.015\n",
      "[398] loss: 1.007\n",
      "[399] loss: 1.004\n",
      "[400] loss: 1.018\n",
      "[401] loss: 1.010\n",
      "[402] loss: 1.002\n",
      "[403] loss: 1.009\n",
      "[404] loss: 1.007\n",
      "[405] loss: 1.005\n",
      "[406] loss: 0.996\n",
      "[407] loss: 0.998\n",
      "[408] loss: 1.000\n",
      "[409] loss: 1.008\n",
      "[410] loss: 0.988\n",
      "[411] loss: 0.999\n",
      "[412] loss: 0.991\n",
      "[413] loss: 0.994\n",
      "[414] loss: 0.990\n",
      "[415] loss: 0.982\n",
      "[416] loss: 0.994\n",
      "[417] loss: 0.990\n",
      "[418] loss: 0.985\n",
      "[419] loss: 0.984\n",
      "[420] loss: 0.981\n",
      "[421] loss: 0.983\n",
      "[422] loss: 0.978\n",
      "[423] loss: 0.980\n",
      "[424] loss: 0.981\n",
      "[425] loss: 0.976\n",
      "[426] loss: 0.974\n",
      "[427] loss: 0.966\n",
      "[428] loss: 0.971\n",
      "[429] loss: 0.967\n",
      "[430] loss: 0.970\n",
      "[431] loss: 0.962\n",
      "[432] loss: 0.972\n",
      "[433] loss: 0.967\n",
      "[434] loss: 0.962\n",
      "[435] loss: 0.971\n",
      "[436] loss: 0.952\n",
      "[437] loss: 0.963\n",
      "[438] loss: 0.957\n",
      "[439] loss: 0.962\n",
      "[440] loss: 0.952\n",
      "[441] loss: 0.956\n",
      "[442] loss: 0.947\n",
      "[443] loss: 0.952\n",
      "[444] loss: 0.955\n",
      "[445] loss: 0.948\n",
      "[446] loss: 0.948\n",
      "[447] loss: 0.941\n",
      "[448] loss: 0.943\n",
      "[449] loss: 0.940\n",
      "[450] loss: 0.944\n",
      "[451] loss: 0.935\n",
      "[452] loss: 0.935\n",
      "[453] loss: 0.930\n",
      "[454] loss: 0.940\n",
      "[455] loss: 0.939\n",
      "[456] loss: 0.936\n",
      "[457] loss: 0.938\n",
      "[458] loss: 0.928\n",
      "[459] loss: 0.921\n",
      "[460] loss: 0.928\n",
      "[461] loss: 0.924\n",
      "[462] loss: 0.915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[463] loss: 0.926\n",
      "[464] loss: 0.929\n",
      "[465] loss: 0.924\n",
      "[466] loss: 0.926\n",
      "[467] loss: 0.922\n",
      "[468] loss: 0.914\n",
      "[469] loss: 0.914\n",
      "[470] loss: 0.913\n",
      "[471] loss: 0.916\n",
      "[472] loss: 0.911\n",
      "[473] loss: 0.912\n",
      "[474] loss: 0.902\n",
      "[475] loss: 0.917\n",
      "[476] loss: 0.909\n",
      "[477] loss: 0.903\n",
      "[478] loss: 0.899\n",
      "[479] loss: 0.907\n",
      "[480] loss: 0.895\n",
      "[481] loss: 0.906\n",
      "[482] loss: 0.912\n",
      "[483] loss: 0.899\n",
      "[484] loss: 0.887\n",
      "[485] loss: 0.892\n",
      "[486] loss: 0.895\n",
      "[487] loss: 0.884\n",
      "[488] loss: 0.902\n",
      "[489] loss: 0.896\n",
      "[490] loss: 0.895\n",
      "[491] loss: 0.885\n",
      "[492] loss: 0.896\n",
      "[493] loss: 0.885\n",
      "[494] loss: 0.883\n",
      "[495] loss: 0.883\n",
      "[496] loss: 0.876\n",
      "[497] loss: 0.878\n",
      "[498] loss: 0.876\n",
      "[499] loss: 0.867\n",
      "[500] loss: 0.877\n",
      "[501] loss: 0.870\n",
      "[502] loss: 0.885\n",
      "[503] loss: 0.861\n",
      "[504] loss: 0.873\n",
      "[505] loss: 0.883\n",
      "[506] loss: 0.875\n",
      "[507] loss: 0.858\n",
      "[508] loss: 0.859\n",
      "[509] loss: 0.868\n",
      "[510] loss: 0.860\n",
      "[511] loss: 0.869\n",
      "[512] loss: 0.863\n",
      "[513] loss: 0.858\n",
      "[514] loss: 0.861\n",
      "[515] loss: 0.860\n",
      "[516] loss: 0.856\n",
      "[517] loss: 0.863\n",
      "[518] loss: 0.853\n",
      "[519] loss: 0.855\n",
      "[520] loss: 0.848\n",
      "[521] loss: 0.854\n",
      "[522] loss: 0.845\n",
      "[523] loss: 0.850\n",
      "[524] loss: 0.857\n",
      "[525] loss: 0.836\n",
      "[526] loss: 0.840\n",
      "[527] loss: 0.840\n",
      "[528] loss: 0.840\n",
      "[529] loss: 0.845\n",
      "[530] loss: 0.841\n",
      "[531] loss: 0.840\n",
      "[532] loss: 0.848\n",
      "[533] loss: 0.834\n",
      "[534] loss: 0.830\n",
      "[535] loss: 0.839\n",
      "[536] loss: 0.829\n",
      "[537] loss: 0.835\n",
      "[538] loss: 0.832\n",
      "[539] loss: 0.825\n",
      "[540] loss: 0.823\n",
      "[541] loss: 0.823\n",
      "[542] loss: 0.823\n",
      "[543] loss: 0.824\n",
      "[544] loss: 0.816\n",
      "[545] loss: 0.833\n",
      "[546] loss: 0.811\n",
      "[547] loss: 0.824\n",
      "[548] loss: 0.818\n",
      "[549] loss: 0.817\n",
      "[550] loss: 0.818\n",
      "[551] loss: 0.817\n",
      "[552] loss: 0.819\n",
      "[553] loss: 0.809\n",
      "[554] loss: 0.807\n",
      "[555] loss: 0.803\n",
      "[556] loss: 0.801\n",
      "[557] loss: 0.812\n",
      "[558] loss: 0.808\n",
      "[559] loss: 0.800\n",
      "[560] loss: 0.804\n",
      "[561] loss: 0.804\n",
      "[562] loss: 0.800\n",
      "[563] loss: 0.795\n",
      "[564] loss: 0.805\n",
      "[565] loss: 0.804\n",
      "[566] loss: 0.795\n",
      "[567] loss: 0.795\n",
      "[568] loss: 0.801\n",
      "[569] loss: 0.792\n",
      "[570] loss: 0.797\n",
      "[571] loss: 0.797\n",
      "[572] loss: 0.789\n",
      "[573] loss: 0.796\n",
      "[574] loss: 0.790\n",
      "[575] loss: 0.796\n",
      "[576] loss: 0.785\n",
      "[577] loss: 0.772\n",
      "[578] loss: 0.781\n",
      "[579] loss: 0.784\n",
      "[580] loss: 0.772\n",
      "[581] loss: 0.781\n",
      "[582] loss: 0.773\n",
      "[583] loss: 0.779\n",
      "[584] loss: 0.776\n",
      "[585] loss: 0.772\n",
      "[586] loss: 0.786\n",
      "[587] loss: 0.761\n",
      "[588] loss: 0.766\n",
      "[589] loss: 0.773\n",
      "[590] loss: 0.772\n",
      "[591] loss: 0.758\n",
      "[592] loss: 0.776\n",
      "[593] loss: 0.770\n",
      "[594] loss: 0.764\n",
      "[595] loss: 0.762\n",
      "[596] loss: 0.761\n",
      "[597] loss: 0.767\n",
      "[598] loss: 0.757\n",
      "[599] loss: 0.763\n",
      "[600] loss: 0.754\n",
      "[601] loss: 0.772\n",
      "[602] loss: 0.751\n",
      "[603] loss: 0.757\n",
      "[604] loss: 0.758\n",
      "[605] loss: 0.755\n",
      "[606] loss: 0.756\n",
      "[607] loss: 0.755\n",
      "[608] loss: 0.744\n",
      "[609] loss: 0.739\n",
      "[610] loss: 0.754\n",
      "[611] loss: 0.741\n",
      "[612] loss: 0.750\n",
      "[613] loss: 0.736\n",
      "[614] loss: 0.746\n",
      "[615] loss: 0.750\n",
      "[616] loss: 0.738\n",
      "[617] loss: 0.731\n",
      "[618] loss: 0.738\n",
      "[619] loss: 0.734\n",
      "[620] loss: 0.744\n",
      "[621] loss: 0.731\n",
      "[622] loss: 0.738\n",
      "[623] loss: 0.745\n",
      "[624] loss: 0.726\n",
      "[625] loss: 0.726\n",
      "[626] loss: 0.718\n",
      "[627] loss: 0.728\n",
      "[628] loss: 0.728\n",
      "[629] loss: 0.735\n",
      "[630] loss: 0.713\n",
      "[631] loss: 0.728\n",
      "[632] loss: 0.711\n",
      "[633] loss: 0.723\n",
      "[634] loss: 0.723\n",
      "[635] loss: 0.727\n",
      "[636] loss: 0.727\n",
      "[637] loss: 0.711\n",
      "[638] loss: 0.725\n",
      "[639] loss: 0.714\n",
      "[640] loss: 0.714\n",
      "[641] loss: 0.703\n",
      "[642] loss: 0.705\n",
      "[643] loss: 0.714\n",
      "[644] loss: 0.711\n",
      "[645] loss: 0.712\n",
      "[646] loss: 0.715\n",
      "[647] loss: 0.697\n",
      "[648] loss: 0.702\n",
      "[649] loss: 0.707\n",
      "[650] loss: 0.695\n",
      "[651] loss: 0.715\n",
      "[652] loss: 0.702\n",
      "[653] loss: 0.690\n",
      "[654] loss: 0.697\n",
      "[655] loss: 0.704\n",
      "[656] loss: 0.710\n",
      "[657] loss: 0.697\n",
      "[658] loss: 0.702\n",
      "[659] loss: 0.692\n",
      "[660] loss: 0.691\n",
      "[661] loss: 0.701\n",
      "[662] loss: 0.682\n",
      "[663] loss: 0.680\n",
      "[664] loss: 0.688\n",
      "[665] loss: 0.703\n",
      "[666] loss: 0.684\n",
      "[667] loss: 0.693\n",
      "[668] loss: 0.685\n",
      "[669] loss: 0.691\n",
      "[670] loss: 0.671\n",
      "[671] loss: 0.668\n",
      "[672] loss: 0.669\n",
      "[673] loss: 0.682\n",
      "[674] loss: 0.674\n",
      "[675] loss: 0.673\n",
      "[676] loss: 0.693\n",
      "[677] loss: 0.665\n",
      "[678] loss: 0.681\n",
      "[679] loss: 0.680\n",
      "[680] loss: 0.673\n",
      "[681] loss: 0.679\n",
      "[682] loss: 0.675\n",
      "[683] loss: 0.678\n",
      "[684] loss: 0.668\n",
      "[685] loss: 0.663\n",
      "[686] loss: 0.660\n",
      "[687] loss: 0.661\n",
      "[688] loss: 0.662\n",
      "[689] loss: 0.663\n",
      "[690] loss: 0.650\n",
      "[691] loss: 0.662\n",
      "[692] loss: 0.658\n",
      "[693] loss: 0.647\n",
      "[694] loss: 0.664\n",
      "[695] loss: 0.675\n",
      "[696] loss: 0.652\n",
      "[697] loss: 0.641\n",
      "[698] loss: 0.665\n",
      "[699] loss: 0.640\n",
      "[700] loss: 0.667\n",
      "[701] loss: 0.644\n",
      "[702] loss: 0.656\n",
      "[703] loss: 0.659\n",
      "[704] loss: 0.645\n",
      "[705] loss: 0.649\n",
      "[706] loss: 0.649\n",
      "[707] loss: 0.652\n",
      "[708] loss: 0.642\n",
      "[709] loss: 0.649\n",
      "[710] loss: 0.642\n",
      "[711] loss: 0.643\n",
      "[712] loss: 0.640\n",
      "[713] loss: 0.649\n",
      "[714] loss: 0.640\n",
      "[715] loss: 0.638\n",
      "[716] loss: 0.633\n",
      "[717] loss: 0.622\n",
      "[718] loss: 0.616\n",
      "[719] loss: 0.650\n",
      "[720] loss: 0.635\n",
      "[721] loss: 0.642\n",
      "[722] loss: 0.628\n",
      "[723] loss: 0.616\n",
      "[724] loss: 0.652\n",
      "[725] loss: 0.633\n",
      "[726] loss: 0.610\n",
      "[727] loss: 0.614\n",
      "[728] loss: 0.623\n",
      "[729] loss: 0.620\n",
      "[730] loss: 0.630\n",
      "[731] loss: 0.606\n",
      "[732] loss: 0.639\n",
      "[733] loss: 0.619\n",
      "[734] loss: 0.607\n",
      "[735] loss: 0.629\n",
      "[736] loss: 0.618\n",
      "[737] loss: 0.616\n",
      "[738] loss: 0.617\n",
      "[739] loss: 0.610\n",
      "[740] loss: 0.616\n",
      "[741] loss: 0.603\n",
      "[742] loss: 0.617\n",
      "[743] loss: 0.637\n",
      "[744] loss: 0.617\n",
      "[745] loss: 0.604\n",
      "[746] loss: 0.608\n",
      "[747] loss: 0.574\n",
      "[748] loss: 0.618\n",
      "[749] loss: 0.602\n",
      "[750] loss: 0.603\n",
      "[751] loss: 0.602\n",
      "[752] loss: 0.619\n",
      "[753] loss: 0.611\n",
      "[754] loss: 0.601\n",
      "[755] loss: 0.587\n",
      "[756] loss: 0.601\n",
      "[757] loss: 0.622\n",
      "[758] loss: 0.577\n",
      "[759] loss: 0.610\n",
      "[760] loss: 0.588\n",
      "[761] loss: 0.588\n",
      "[762] loss: 0.579\n",
      "[763] loss: 0.600\n",
      "[764] loss: 0.584\n",
      "[765] loss: 0.597\n",
      "[766] loss: 0.604\n",
      "[767] loss: 0.587\n",
      "[768] loss: 0.570\n",
      "[769] loss: 0.578\n",
      "[770] loss: 0.570\n",
      "[771] loss: 0.573\n",
      "[772] loss: 0.593\n",
      "[773] loss: 0.590\n",
      "[774] loss: 0.572\n",
      "[775] loss: 0.576\n",
      "[776] loss: 0.597\n",
      "[777] loss: 0.591\n",
      "[778] loss: 0.577\n",
      "[779] loss: 0.573\n",
      "[780] loss: 0.561\n",
      "[781] loss: 0.609\n",
      "[782] loss: 0.575\n",
      "[783] loss: 0.569\n",
      "[784] loss: 0.589\n",
      "[785] loss: 0.571\n",
      "[786] loss: 0.560\n",
      "[787] loss: 0.567\n",
      "[788] loss: 0.583\n",
      "[789] loss: 0.557\n",
      "[790] loss: 0.557\n",
      "[791] loss: 0.583\n",
      "[792] loss: 0.570\n",
      "[793] loss: 0.570\n",
      "[794] loss: 0.559\n",
      "[795] loss: 0.561\n",
      "[796] loss: 0.567\n",
      "[797] loss: 0.568\n",
      "[798] loss: 0.545\n",
      "[799] loss: 0.549\n",
      "[800] loss: 0.551\n",
      "[801] loss: 0.551\n",
      "[802] loss: 0.551\n",
      "[803] loss: 0.551\n",
      "[804] loss: 0.553\n",
      "[805] loss: 0.556\n",
      "[806] loss: 0.556\n",
      "[807] loss: 0.561\n",
      "[808] loss: 0.543\n",
      "[809] loss: 0.540\n",
      "[810] loss: 0.532\n",
      "[811] loss: 0.571\n",
      "[812] loss: 0.525\n",
      "[813] loss: 0.567\n",
      "[814] loss: 0.547\n",
      "[815] loss: 0.527\n",
      "[816] loss: 0.569\n",
      "[817] loss: 0.555\n",
      "[818] loss: 0.526\n",
      "[819] loss: 0.564\n",
      "[820] loss: 0.535\n",
      "[821] loss: 0.530\n",
      "[822] loss: 0.539\n",
      "[823] loss: 0.528\n",
      "[824] loss: 0.537\n",
      "[825] loss: 0.547\n",
      "[826] loss: 0.540\n",
      "[827] loss: 0.551\n",
      "[828] loss: 0.524\n",
      "[829] loss: 0.527\n",
      "[830] loss: 0.541\n",
      "[831] loss: 0.550\n",
      "[832] loss: 0.531\n",
      "[833] loss: 0.526\n",
      "[834] loss: 0.534\n",
      "[835] loss: 0.526\n",
      "[836] loss: 0.520\n",
      "[837] loss: 0.527\n",
      "[838] loss: 0.546\n",
      "[839] loss: 0.516\n",
      "[840] loss: 0.537\n",
      "[841] loss: 0.518\n",
      "[842] loss: 0.527\n",
      "[843] loss: 0.499\n",
      "[844] loss: 0.523\n",
      "[845] loss: 0.525\n",
      "[846] loss: 0.537\n",
      "[847] loss: 0.510\n",
      "[848] loss: 0.501\n",
      "[849] loss: 0.533\n",
      "[850] loss: 0.520\n",
      "[851] loss: 0.512\n",
      "[852] loss: 0.542\n",
      "[853] loss: 0.512\n",
      "[854] loss: 0.523\n",
      "[855] loss: 0.527\n",
      "[856] loss: 0.484\n",
      "[857] loss: 0.487\n",
      "[858] loss: 0.503\n",
      "[859] loss: 0.509\n",
      "[860] loss: 0.542\n",
      "[861] loss: 0.506\n",
      "[862] loss: 0.513\n",
      "[863] loss: 0.507\n",
      "[864] loss: 0.514\n",
      "[865] loss: 0.514\n",
      "[866] loss: 0.458\n",
      "[867] loss: 0.536\n",
      "[868] loss: 0.493\n",
      "[869] loss: 0.525\n",
      "[870] loss: 0.530\n",
      "[871] loss: 0.471\n",
      "[872] loss: 0.503\n",
      "[873] loss: 0.518\n",
      "[874] loss: 0.501\n",
      "[875] loss: 0.463\n",
      "[876] loss: 0.509\n",
      "[877] loss: 0.458\n",
      "[878] loss: 0.490\n",
      "[879] loss: 0.511\n",
      "[880] loss: 0.499\n",
      "[881] loss: 0.511\n",
      "[882] loss: 0.521\n",
      "[883] loss: 0.457\n",
      "[884] loss: 0.495\n",
      "[885] loss: 0.488\n",
      "[886] loss: 0.474\n",
      "[887] loss: 0.458\n",
      "[888] loss: 0.519\n",
      "[889] loss: 0.509\n",
      "[890] loss: 0.453\n",
      "[891] loss: 0.442\n",
      "[892] loss: 0.511\n",
      "[893] loss: 0.506\n",
      "[894] loss: 0.447\n",
      "[895] loss: 0.479\n",
      "[896] loss: 0.533\n",
      "[897] loss: 0.483\n",
      "[898] loss: 0.453\n",
      "[899] loss: 0.512\n",
      "[900] loss: 0.462\n",
      "[901] loss: 0.494\n",
      "[902] loss: 0.471\n",
      "[903] loss: 0.450\n",
      "[904] loss: 0.460\n",
      "[905] loss: 0.485\n",
      "[906] loss: 0.487\n",
      "[907] loss: 0.485\n",
      "[908] loss: 0.492\n",
      "[909] loss: 0.462\n",
      "[910] loss: 0.441\n",
      "[911] loss: 0.450\n",
      "[912] loss: 0.489\n",
      "[913] loss: 0.463\n",
      "[914] loss: 0.463\n",
      "[915] loss: 0.462\n",
      "[916] loss: 0.491\n",
      "[917] loss: 0.470\n",
      "[918] loss: 0.476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[919] loss: 0.490\n",
      "[920] loss: 0.429\n",
      "[921] loss: 0.450\n",
      "[922] loss: 0.439\n",
      "[923] loss: 0.484\n",
      "[924] loss: 0.456\n",
      "[925] loss: 0.465\n",
      "[926] loss: 0.463\n",
      "[927] loss: 0.449\n",
      "[928] loss: 0.443\n",
      "[929] loss: 0.449\n",
      "[930] loss: 0.436\n",
      "[931] loss: 0.449\n",
      "[932] loss: 0.500\n",
      "[933] loss: 0.448\n",
      "[934] loss: 0.446\n",
      "[935] loss: 0.435\n",
      "[936] loss: 0.437\n",
      "[937] loss: 0.460\n",
      "[938] loss: 0.434\n",
      "[939] loss: 0.449\n",
      "[940] loss: 0.432\n",
      "[941] loss: 0.482\n",
      "[942] loss: 0.458\n",
      "[943] loss: 0.437\n",
      "[944] loss: 0.451\n",
      "[945] loss: 0.453\n",
      "[946] loss: 0.464\n",
      "[947] loss: 0.460\n",
      "[948] loss: 0.430\n",
      "[949] loss: 0.435\n",
      "[950] loss: 0.427\n",
      "[951] loss: 0.424\n",
      "[952] loss: 0.437\n",
      "[953] loss: 0.474\n",
      "[954] loss: 0.403\n",
      "[955] loss: 0.418\n",
      "[956] loss: 0.453\n",
      "[957] loss: 0.396\n",
      "[958] loss: 0.443\n",
      "[959] loss: 0.440\n",
      "[960] loss: 0.413\n",
      "[961] loss: 0.448\n",
      "[962] loss: 0.466\n",
      "[963] loss: 0.464\n",
      "[964] loss: 0.407\n",
      "[965] loss: 0.388\n",
      "[966] loss: 0.449\n",
      "[967] loss: 0.401\n",
      "[968] loss: 0.457\n",
      "[969] loss: 0.409\n",
      "[970] loss: 0.429\n",
      "[971] loss: 0.436\n",
      "[972] loss: 0.394\n",
      "[973] loss: 0.404\n",
      "[974] loss: 0.457\n",
      "[975] loss: 0.464\n",
      "[976] loss: 0.386\n",
      "[977] loss: 0.428\n",
      "[978] loss: 0.396\n",
      "[979] loss: 0.424\n",
      "[980] loss: 0.403\n",
      "[981] loss: 0.383\n",
      "[982] loss: 0.424\n",
      "[983] loss: 0.419\n",
      "[984] loss: 0.418\n",
      "[985] loss: 0.442\n",
      "[986] loss: 0.426\n",
      "[987] loss: 0.392\n",
      "[988] loss: 0.454\n",
      "[989] loss: 0.374\n",
      "[990] loss: 0.412\n",
      "[991] loss: 0.422\n",
      "[992] loss: 0.375\n",
      "[993] loss: 0.400\n",
      "[994] loss: 0.386\n",
      "[995] loss: 0.393\n",
      "[996] loss: 0.489\n",
      "[997] loss: 0.427\n",
      "[998] loss: 0.377\n",
      "[999] loss: 0.456\n",
      "[1000] loss: 0.416\n",
      "[1001] loss: 0.387\n",
      "[1002] loss: 0.443\n",
      "[1003] loss: 0.357\n",
      "[1004] loss: 0.450\n",
      "[1005] loss: 0.436\n",
      "[1006] loss: 0.358\n",
      "[1007] loss: 0.460\n",
      "[1008] loss: 0.346\n",
      "[1009] loss: 0.429\n",
      "[1010] loss: 0.413\n",
      "[1011] loss: 0.372\n",
      "[1012] loss: 0.417\n",
      "[1013] loss: 0.348\n",
      "[1014] loss: 0.396\n",
      "[1015] loss: 0.386\n",
      "[1016] loss: 0.418\n",
      "[1017] loss: 0.388\n",
      "[1018] loss: 0.370\n",
      "[1019] loss: 0.405\n",
      "[1020] loss: 0.380\n",
      "[1021] loss: 0.403\n",
      "[1022] loss: 0.377\n",
      "[1023] loss: 0.367\n",
      "[1024] loss: 0.382\n",
      "[1025] loss: 0.410\n",
      "[1026] loss: 0.411\n",
      "[1027] loss: 0.366\n",
      "[1028] loss: 0.385\n",
      "[1029] loss: 0.368\n",
      "[1030] loss: 0.360\n",
      "[1031] loss: 0.394\n",
      "[1032] loss: 0.379\n",
      "[1033] loss: 0.426\n",
      "[1034] loss: 0.386\n",
      "[1035] loss: 0.356\n",
      "[1036] loss: 0.360\n",
      "[1037] loss: 0.366\n",
      "[1038] loss: 0.342\n",
      "[1039] loss: 0.351\n",
      "[1040] loss: 0.427\n",
      "[1041] loss: 0.399\n",
      "[1042] loss: 0.367\n",
      "[1043] loss: 0.397\n",
      "[1044] loss: 0.462\n",
      "[1045] loss: 0.353\n",
      "[1046] loss: 0.396\n",
      "[1047] loss: 0.389\n",
      "[1048] loss: 0.343\n",
      "[1049] loss: 0.350\n",
      "[1050] loss: 0.395\n",
      "[1051] loss: 0.394\n",
      "[1052] loss: 0.285\n",
      "[1053] loss: 0.346\n",
      "[1054] loss: 0.426\n",
      "[1055] loss: 0.386\n",
      "[1056] loss: 0.330\n",
      "[1057] loss: 0.425\n",
      "[1058] loss: 0.420\n",
      "[1059] loss: 0.421\n",
      "[1060] loss: 0.384\n",
      "[1061] loss: 0.351\n",
      "[1062] loss: 0.338\n",
      "[1063] loss: 0.375\n",
      "[1064] loss: 0.367\n",
      "[1065] loss: 0.314\n",
      "[1066] loss: 0.382\n",
      "[1067] loss: 0.387\n",
      "[1068] loss: 0.359\n",
      "[1069] loss: 0.318\n",
      "[1070] loss: 0.285\n",
      "[1071] loss: 0.404\n",
      "[1072] loss: 0.336\n",
      "[1073] loss: 0.320\n",
      "[1074] loss: 0.387\n",
      "[1075] loss: 0.374\n",
      "[1076] loss: 0.378\n",
      "[1077] loss: 0.302\n",
      "[1078] loss: 0.421\n",
      "[1079] loss: 0.313\n",
      "[1080] loss: 0.304\n",
      "[1081] loss: 0.376\n",
      "[1082] loss: 0.404\n",
      "[1083] loss: 0.343\n",
      "[1084] loss: 0.390\n",
      "[1085] loss: 0.357\n",
      "[1086] loss: 0.355\n",
      "[1087] loss: 0.331\n",
      "[1088] loss: 0.313\n",
      "[1089] loss: 0.365\n",
      "[1090] loss: 0.281\n",
      "[1091] loss: 0.368\n",
      "[1092] loss: 0.361\n",
      "[1093] loss: 0.353\n",
      "[1094] loss: 0.310\n",
      "[1095] loss: 0.393\n",
      "[1096] loss: 0.431\n",
      "[1097] loss: 0.313\n",
      "[1098] loss: 0.299\n",
      "[1099] loss: 0.290\n",
      "[1100] loss: 0.374\n",
      "[1101] loss: 0.355\n",
      "[1102] loss: 0.282\n",
      "[1103] loss: 0.302\n",
      "[1104] loss: 0.298\n",
      "[1105] loss: 0.414\n",
      "[1106] loss: 0.454\n",
      "[1107] loss: 0.345\n",
      "[1108] loss: 0.297\n",
      "[1109] loss: 0.404\n",
      "[1110] loss: 0.333\n",
      "[1111] loss: 0.297\n",
      "[1112] loss: 0.260\n",
      "[1113] loss: 0.291\n",
      "[1114] loss: 0.344\n",
      "[1115] loss: 0.367\n",
      "[1116] loss: 0.327\n",
      "[1117] loss: 0.340\n",
      "[1118] loss: 0.404\n",
      "[1119] loss: 0.264\n",
      "[1120] loss: 0.377\n",
      "[1121] loss: 0.409\n",
      "[1122] loss: 0.327\n",
      "[1123] loss: 0.398\n",
      "[1124] loss: 0.254\n",
      "[1125] loss: 0.389\n",
      "[1126] loss: 0.371\n",
      "[1127] loss: 0.325\n",
      "[1128] loss: 0.299\n",
      "[1129] loss: 0.322\n",
      "[1130] loss: 0.365\n",
      "[1131] loss: 0.418\n",
      "[1132] loss: 0.315\n",
      "[1133] loss: 0.296\n",
      "[1134] loss: 0.314\n",
      "[1135] loss: 0.300\n",
      "[1136] loss: 0.370\n",
      "[1137] loss: 0.417\n",
      "[1138] loss: 0.350\n",
      "[1139] loss: 0.266\n",
      "[1140] loss: 0.240\n",
      "[1141] loss: 0.371\n",
      "[1142] loss: 0.327\n",
      "[1143] loss: 0.309\n",
      "[1144] loss: 0.311\n",
      "[1145] loss: 0.275\n",
      "[1146] loss: 0.273\n",
      "[1147] loss: 0.346\n",
      "[1148] loss: 0.360\n",
      "[1149] loss: 0.317\n",
      "[1150] loss: 0.362\n",
      "[1151] loss: 0.391\n",
      "[1152] loss: 0.314\n",
      "[1153] loss: 0.417\n",
      "[1154] loss: 0.320\n",
      "[1155] loss: 0.222\n",
      "[1156] loss: 0.302\n",
      "[1157] loss: 0.222\n",
      "[1158] loss: 0.311\n",
      "[1159] loss: 0.260\n",
      "[1160] loss: 0.420\n",
      "[1161] loss: 0.329\n",
      "[1162] loss: 0.280\n",
      "[1163] loss: 0.372\n",
      "[1164] loss: 0.318\n",
      "[1165] loss: 0.296\n",
      "[1166] loss: 0.224\n",
      "[1167] loss: 0.348\n",
      "[1168] loss: 0.464\n",
      "[1169] loss: 0.324\n",
      "[1170] loss: 0.267\n",
      "[1171] loss: 0.316\n",
      "[1172] loss: 0.337\n",
      "[1173] loss: 0.218\n",
      "[1174] loss: 0.367\n",
      "[1175] loss: 0.239\n",
      "[1176] loss: 0.286\n",
      "[1177] loss: 0.298\n",
      "[1178] loss: 0.329\n",
      "[1179] loss: 0.213\n",
      "[1180] loss: 0.241\n",
      "[1181] loss: 0.396\n",
      "[1182] loss: 0.358\n",
      "[1183] loss: 0.383\n",
      "[1184] loss: 0.331\n",
      "[1185] loss: 0.301\n",
      "[1186] loss: 0.312\n",
      "[1187] loss: 0.226\n",
      "[1188] loss: 0.259\n",
      "[1189] loss: 0.253\n",
      "[1190] loss: 0.292\n",
      "[1191] loss: 0.264\n",
      "[1192] loss: 0.273\n",
      "[1193] loss: 0.411\n",
      "[1194] loss: 0.272\n",
      "[1195] loss: 0.265\n",
      "[1196] loss: 0.284\n",
      "[1197] loss: 0.260\n",
      "[1198] loss: 0.201\n",
      "[1199] loss: 0.360\n",
      "[1200] loss: 0.311\n",
      "[1201] loss: 0.343\n",
      "[1202] loss: 0.214\n",
      "[1203] loss: 0.257\n",
      "[1204] loss: 0.432\n",
      "[1205] loss: 0.298\n",
      "[1206] loss: 0.282\n",
      "[1207] loss: 0.326\n",
      "[1208] loss: 0.331\n",
      "[1209] loss: 0.348\n",
      "[1210] loss: 0.196\n",
      "[1211] loss: 0.330\n",
      "[1212] loss: 0.245\n",
      "[1213] loss: 0.327\n",
      "[1214] loss: 0.294\n",
      "[1215] loss: 0.345\n",
      "[1216] loss: 0.279\n",
      "[1217] loss: 0.397\n",
      "[1218] loss: 0.302\n",
      "[1219] loss: 0.259\n",
      "[1220] loss: 0.341\n",
      "[1221] loss: 0.276\n",
      "[1222] loss: 0.189\n",
      "[1223] loss: 0.186\n",
      "[1224] loss: 0.296\n",
      "[1225] loss: 0.285\n",
      "[1226] loss: 0.352\n",
      "[1227] loss: 0.253\n",
      "[1228] loss: 0.427\n",
      "[1229] loss: 0.315\n",
      "[1230] loss: 0.248\n",
      "[1231] loss: 0.235\n",
      "[1232] loss: 0.287\n",
      "[1233] loss: 0.218\n",
      "[1234] loss: 0.274\n",
      "[1235] loss: 0.281\n",
      "[1236] loss: 0.389\n",
      "[1237] loss: 0.302\n",
      "[1238] loss: 0.322\n",
      "[1239] loss: 0.262\n",
      "[1240] loss: 0.301\n",
      "[1241] loss: 0.349\n",
      "[1242] loss: 0.180\n",
      "[1243] loss: 0.186\n",
      "[1244] loss: 0.358\n",
      "[1245] loss: 0.227\n",
      "[1246] loss: 0.192\n",
      "[1247] loss: 0.373\n",
      "[1248] loss: 0.338\n",
      "[1249] loss: 0.404\n",
      "[1250] loss: 0.317\n",
      "[1251] loss: 0.181\n",
      "[1252] loss: 0.248\n",
      "[1253] loss: 0.226\n",
      "[1254] loss: 0.292\n",
      "[1255] loss: 0.175\n",
      "[1256] loss: 0.246\n",
      "[1257] loss: 0.191\n",
      "[1258] loss: 0.278\n",
      "[1259] loss: 0.350\n",
      "[1260] loss: 0.239\n",
      "[1261] loss: 0.340\n",
      "[1262] loss: 0.176\n",
      "[1263] loss: 0.257\n",
      "[1264] loss: 0.256\n",
      "[1265] loss: 0.209\n",
      "[1266] loss: 0.231\n",
      "[1267] loss: 0.324\n",
      "[1268] loss: 0.233\n",
      "[1269] loss: 0.197\n",
      "[1270] loss: 0.265\n",
      "[1271] loss: 0.372\n",
      "[1272] loss: 0.309\n",
      "[1273] loss: 0.241\n",
      "[1274] loss: 0.229\n",
      "[1275] loss: 0.263\n",
      "[1276] loss: 0.238\n",
      "[1277] loss: 0.295\n",
      "[1278] loss: 0.194\n",
      "[1279] loss: 0.206\n",
      "[1280] loss: 0.240\n",
      "[1281] loss: 0.243\n",
      "[1282] loss: 0.386\n",
      "[1283] loss: 0.212\n",
      "[1284] loss: 0.187\n",
      "[1285] loss: 0.160\n",
      "[1286] loss: 0.326\n",
      "[1287] loss: 0.164\n",
      "[1288] loss: 0.273\n",
      "[1289] loss: 0.343\n",
      "[1290] loss: 0.379\n",
      "[1291] loss: 0.368\n",
      "[1292] loss: 0.268\n",
      "[1293] loss: 0.317\n",
      "[1294] loss: 0.312\n",
      "[1295] loss: 0.165\n",
      "[1296] loss: 0.158\n",
      "[1297] loss: 0.160\n",
      "[1298] loss: 0.503\n",
      "[1299] loss: 0.311\n",
      "[1300] loss: 0.269\n",
      "[1301] loss: 0.234\n",
      "[1302] loss: 0.162\n",
      "[1303] loss: 0.232\n",
      "[1304] loss: 0.275\n",
      "[1305] loss: 0.220\n",
      "[1306] loss: 0.314\n",
      "[1307] loss: 0.161\n",
      "[1308] loss: 0.153\n",
      "[1309] loss: 0.152\n",
      "[1310] loss: 0.266\n",
      "[1311] loss: 0.217\n",
      "[1312] loss: 0.323\n",
      "[1313] loss: 0.154\n",
      "[1314] loss: 0.232\n",
      "[1315] loss: 0.152\n",
      "[1316] loss: 0.354\n",
      "[1317] loss: 0.241\n",
      "[1318] loss: 0.483\n",
      "[1319] loss: 0.161\n",
      "[1320] loss: 0.206\n",
      "[1321] loss: 0.150\n",
      "[1322] loss: 0.150\n",
      "[1323] loss: 0.378\n",
      "[1324] loss: 0.223\n",
      "[1325] loss: 0.223\n",
      "[1326] loss: 0.333\n",
      "[1327] loss: 0.152\n",
      "[1328] loss: 0.503\n",
      "[1329] loss: 0.370\n",
      "[1330] loss: 0.275\n",
      "[1331] loss: 0.160\n",
      "[1332] loss: 0.149\n",
      "[1333] loss: 0.145\n",
      "[1334] loss: 0.144\n",
      "[1335] loss: 0.144\n",
      "[1336] loss: 0.230\n",
      "[1337] loss: 0.354\n",
      "[1338] loss: 0.351\n",
      "[1339] loss: 0.421\n",
      "[1340] loss: 0.230\n",
      "[1341] loss: 0.373\n",
      "[1342] loss: 0.397\n",
      "[1343] loss: 0.211\n",
      "[1344] loss: 0.225\n",
      "[1345] loss: 0.147\n",
      "[1346] loss: 0.215\n",
      "[1347] loss: 0.225\n",
      "[1348] loss: 0.202\n",
      "[1349] loss: 0.222\n",
      "[1350] loss: 0.351\n",
      "[1351] loss: 0.326\n",
      "[1352] loss: 0.147\n",
      "[1353] loss: 0.141\n",
      "[1354] loss: 0.139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1355] loss: 0.138\n",
      "[1356] loss: 0.138\n",
      "[1357] loss: 0.261\n",
      "[1358] loss: 0.204\n",
      "[1359] loss: 0.364\n",
      "[1360] loss: 0.341\n",
      "[1361] loss: 0.338\n",
      "[1362] loss: 0.143\n",
      "[1363] loss: 0.138\n",
      "[1364] loss: 0.387\n",
      "[1365] loss: 0.143\n",
      "[1366] loss: 0.408\n",
      "[1367] loss: 0.241\n",
      "[1368] loss: 0.159\n",
      "[1369] loss: 0.260\n",
      "[1370] loss: 0.140\n",
      "[1371] loss: 0.136\n",
      "[1372] loss: 0.135\n",
      "[1373] loss: 0.210\n",
      "[1374] loss: 0.135\n",
      "[1375] loss: 0.133\n",
      "[1376] loss: 0.334\n",
      "[1377] loss: 0.433\n",
      "[1378] loss: 0.342\n",
      "[1379] loss: 0.139\n",
      "[1380] loss: 0.134\n",
      "[1381] loss: 0.132\n",
      "[1382] loss: 0.687\n",
      "[1383] loss: 0.471\n",
      "[1384] loss: 0.434\n",
      "[1385] loss: 0.232\n",
      "[1386] loss: 0.328\n",
      "[1387] loss: 0.142\n",
      "[1388] loss: 0.136\n",
      "[1389] loss: 0.134\n",
      "[1390] loss: 0.132\n",
      "[1391] loss: 0.133\n",
      "[1392] loss: 0.180\n",
      "[1393] loss: 0.256\n",
      "[1394] loss: 0.397\n",
      "[1395] loss: 0.137\n",
      "[1396] loss: 0.244\n",
      "[1397] loss: 0.131\n",
      "[1398] loss: 0.229\n",
      "[1399] loss: 0.318\n",
      "[1400] loss: 0.132\n",
      "[1401] loss: 0.129\n",
      "[1402] loss: 0.284\n",
      "[1403] loss: 0.219\n",
      "[1404] loss: 0.131\n",
      "[1405] loss: 0.127\n",
      "[1406] loss: 0.126\n",
      "[1407] loss: 0.126\n",
      "[1408] loss: 0.127\n",
      "[1409] loss: 0.125\n",
      "[1410] loss: 0.123\n",
      "[1411] loss: 0.124\n",
      "[1412] loss: 0.371\n",
      "[1413] loss: 0.549\n",
      "[1414] loss: 0.249\n",
      "[1415] loss: 0.260\n",
      "[1416] loss: 0.184\n",
      "[1417] loss: 0.126\n",
      "[1418] loss: 0.124\n",
      "[1419] loss: 0.265\n",
      "[1420] loss: 0.319\n",
      "[1421] loss: 0.235\n",
      "[1422] loss: 0.212\n",
      "[1423] loss: 0.127\n",
      "[1424] loss: 0.134\n",
      "[1425] loss: 0.268\n",
      "[1426] loss: 0.125\n",
      "[1427] loss: 0.307\n",
      "[1428] loss: 0.358\n",
      "[1429] loss: 0.378\n",
      "[1430] loss: 0.407\n",
      "[1431] loss: 0.474\n",
      "[1432] loss: 0.257\n",
      "[1433] loss: 0.129\n",
      "[1434] loss: 0.124\n",
      "[1435] loss: 0.123\n",
      "[1436] loss: 0.122\n",
      "[1437] loss: 0.121\n",
      "[1438] loss: 0.120\n",
      "[1439] loss: 0.119\n",
      "[1440] loss: 0.119\n",
      "[1441] loss: 0.171\n",
      "[1442] loss: 0.394\n",
      "[1443] loss: 0.122\n",
      "[1444] loss: 0.118\n",
      "[1445] loss: 0.118\n",
      "[1446] loss: 0.118\n",
      "[1447] loss: 0.116\n",
      "[1448] loss: 0.115\n",
      "[1449] loss: 0.530\n",
      "[1450] loss: 0.572\n",
      "[1451] loss: 0.417\n",
      "[1452] loss: 0.301\n",
      "[1453] loss: 0.124\n",
      "[1454] loss: 0.119\n",
      "[1455] loss: 0.118\n",
      "[1456] loss: 0.117\n",
      "[1457] loss: 0.116\n",
      "[1458] loss: 0.115\n",
      "[1459] loss: 0.189\n",
      "[1460] loss: 0.675\n",
      "[1461] loss: 0.508\n",
      "[1462] loss: 0.273\n",
      "[1463] loss: 0.165\n",
      "[1464] loss: 0.119\n",
      "[1465] loss: 0.117\n",
      "[1466] loss: 0.116\n",
      "[1467] loss: 0.114\n",
      "[1468] loss: 0.114\n",
      "[1469] loss: 0.113\n",
      "[1470] loss: 0.113\n",
      "[1471] loss: 0.112\n",
      "[1472] loss: 0.325\n",
      "[1473] loss: 0.633\n",
      "[1474] loss: 0.437\n",
      "[1475] loss: 0.201\n",
      "[1476] loss: 0.118\n",
      "[1477] loss: 0.115\n",
      "[1478] loss: 0.114\n",
      "[1479] loss: 0.112\n",
      "[1480] loss: 0.112\n",
      "[1481] loss: 0.111\n",
      "[1482] loss: 0.111\n",
      "[1483] loss: 0.110\n",
      "[1484] loss: 0.109\n",
      "[1485] loss: 0.109\n",
      "[1486] loss: 0.109\n",
      "[1487] loss: 0.644\n",
      "[1488] loss: 0.381\n",
      "[1489] loss: 0.561\n",
      "[1490] loss: 0.391\n",
      "[1491] loss: 0.329\n",
      "[1492] loss: 0.256\n",
      "[1493] loss: 0.200\n",
      "[1494] loss: 0.117\n",
      "[1495] loss: 0.113\n",
      "[1496] loss: 0.111\n",
      "[1497] loss: 0.110\n",
      "[1498] loss: 0.109\n",
      "[1499] loss: 0.109\n",
      "[1500] loss: 0.108\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_model(model, 1500, train_loader, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Cross Entropy durante o Treinamento\")\n",
    "plt.tight_layout()\n",
    "_ = plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(results):\n",
    "    results = results.cpu().detach().numpy().tolist()[0]\n",
    "    return results.index(max(results))\n",
    "\n",
    "def evaluate(model, dataset):\n",
    "    inputs_list = []\n",
    "    labels_list = []\n",
    "    for _, (inputs, labels) in enumerate(dataset):\n",
    "        inputs_list.append(inputs.to(device))\n",
    "        labels_list.append(labels.to(device))\n",
    "\n",
    "    acuracia = 0\n",
    "    results = [0,0,0,0,0,0,0,0,0,0]\n",
    "    for i, (inputs, labels) in enumerate(zip(inputs_list, labels_list), 0):\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        result = one_hot(y_pred)\n",
    "\n",
    "        if result == labels.item():\n",
    "            acuracia += 1\n",
    "\n",
    "        results[result] += 1\n",
    "        \n",
    "    print(acuracia / len(dataset) * 100, \"%\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=1)\n",
    "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do Modelo no dataset de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.496 %\n",
      "[5722, 4959, 4816, 4737, 5216, 4726, 5028, 4868, 5244, 4684]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do Modelo (conjunto de teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.04 %\n",
      "[1344, 938, 986, 912, 1069, 913, 934, 978, 1077, 849]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
