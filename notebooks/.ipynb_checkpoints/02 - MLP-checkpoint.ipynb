{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos deste trabalho\n",
    "- Familiarizar-se com a biblioteca PyTorch\n",
    "- Definir arquiteturas MLP simples em PyTorch\n",
    "- Treinar utilizando CIFAR10, testando diferentes arquiteturas, parâmetros, funções de loss e otimizadores\n",
    "- Comparar os resultados obtidos utilizando apenas Perpceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Carregar os datasets\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "dataset_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=200)\n",
    "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a arquitetura MLP\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.activation_function = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32)\n",
    "        x = self.activation_function(self.fc1(x))\n",
    "        x = self.activation_function(self.fc2(x))\n",
    "        x = self.activation_function(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "MLP(\n",
      "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (activation_function): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "# Definir otimizador e loss\n",
    "# Nota: testar outros otimizadores e funções de loss (em particular cross entropy)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2, weight_decay=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "print(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar o treinamento aqui\n",
    "def train_model(model, epochs, train_loader, losses):\n",
    "    \n",
    "    inputs_list = []\n",
    "    labels_list = []\n",
    "    for _, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs_list.append(inputs.to(device))\n",
    "        labels_list.append(labels.to(device))\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(zip(inputs_list, labels_list), 0):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(inputs)\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "        \n",
    "            running_loss += loss.item()\n",
    "            if i % 250 == 249:\n",
    "                print('[%d] loss: %.3f' %\n",
    "                      (epoch + 1, running_loss / 250))\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainamento do Modelo\n",
    "- Ativação: ReLu\n",
    "- Loss: CrossEntropy\n",
    "- Hidden: 3\n",
    "- Regularizador: L2\n",
    "- Taxa de Aprendizado = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 2.293\n",
      "[2] loss: 2.279\n",
      "[3] loss: 2.263\n",
      "[4] loss: 2.245\n",
      "[5] loss: 2.227\n",
      "[6] loss: 2.205\n",
      "[7] loss: 2.172\n",
      "[8] loss: 2.148\n",
      "[9] loss: 2.117\n",
      "[10] loss: 2.089\n",
      "[11] loss: 2.072\n",
      "[12] loss: 2.059\n",
      "[13] loss: 2.049\n",
      "[14] loss: 2.040\n",
      "[15] loss: 2.032\n",
      "[16] loss: 2.024\n",
      "[17] loss: 2.016\n",
      "[18] loss: 2.009\n",
      "[19] loss: 2.001\n",
      "[20] loss: 1.993\n",
      "[21] loss: 1.986\n",
      "[22] loss: 1.978\n",
      "[23] loss: 1.970\n",
      "[24] loss: 1.962\n",
      "[25] loss: 1.953\n",
      "[26] loss: 1.945\n",
      "[27] loss: 1.937\n",
      "[28] loss: 1.928\n",
      "[29] loss: 1.919\n",
      "[30] loss: 1.911\n",
      "[31] loss: 1.903\n",
      "[32] loss: 1.894\n",
      "[33] loss: 1.886\n",
      "[34] loss: 1.878\n",
      "[35] loss: 1.871\n",
      "[36] loss: 1.863\n",
      "[37] loss: 1.856\n",
      "[38] loss: 1.849\n",
      "[39] loss: 1.842\n",
      "[40] loss: 1.835\n",
      "[41] loss: 1.828\n",
      "[42] loss: 1.821\n",
      "[43] loss: 1.815\n",
      "[44] loss: 1.808\n",
      "[45] loss: 1.802\n",
      "[46] loss: 1.796\n",
      "[47] loss: 1.790\n",
      "[48] loss: 1.784\n",
      "[49] loss: 1.778\n",
      "[50] loss: 1.772\n",
      "[51] loss: 1.767\n",
      "[52] loss: 1.761\n",
      "[53] loss: 1.756\n",
      "[54] loss: 1.750\n",
      "[55] loss: 1.745\n",
      "[56] loss: 1.740\n",
      "[57] loss: 1.734\n",
      "[58] loss: 1.729\n",
      "[59] loss: 1.724\n",
      "[60] loss: 1.719\n",
      "[61] loss: 1.714\n",
      "[62] loss: 1.709\n",
      "[63] loss: 1.704\n",
      "[64] loss: 1.700\n",
      "[65] loss: 1.695\n",
      "[66] loss: 1.690\n",
      "[67] loss: 1.685\n",
      "[68] loss: 1.681\n",
      "[69] loss: 1.676\n",
      "[70] loss: 1.672\n",
      "[71] loss: 1.667\n",
      "[72] loss: 1.662\n",
      "[73] loss: 1.658\n",
      "[74] loss: 1.653\n",
      "[75] loss: 1.649\n",
      "[76] loss: 1.645\n",
      "[77] loss: 1.640\n",
      "[78] loss: 1.636\n",
      "[79] loss: 1.632\n",
      "[80] loss: 1.628\n",
      "[81] loss: 1.623\n",
      "[82] loss: 1.619\n",
      "[83] loss: 1.615\n",
      "[84] loss: 1.611\n",
      "[85] loss: 1.607\n",
      "[86] loss: 1.603\n",
      "[87] loss: 1.599\n",
      "[88] loss: 1.596\n",
      "[89] loss: 1.592\n",
      "[90] loss: 1.588\n",
      "[91] loss: 1.584\n",
      "[92] loss: 1.581\n",
      "[93] loss: 1.577\n",
      "[94] loss: 1.574\n",
      "[95] loss: 1.570\n",
      "[96] loss: 1.567\n",
      "[97] loss: 1.563\n",
      "[98] loss: 1.560\n",
      "[99] loss: 1.556\n",
      "[100] loss: 1.553\n",
      "[101] loss: 1.550\n",
      "[102] loss: 1.546\n",
      "[103] loss: 1.543\n",
      "[104] loss: 1.540\n",
      "[105] loss: 1.537\n",
      "[106] loss: 1.534\n",
      "[107] loss: 1.531\n",
      "[108] loss: 1.528\n",
      "[109] loss: 1.525\n",
      "[110] loss: 1.522\n",
      "[111] loss: 1.519\n",
      "[112] loss: 1.516\n",
      "[113] loss: 1.513\n",
      "[114] loss: 1.510\n",
      "[115] loss: 1.507\n",
      "[116] loss: 1.504\n",
      "[117] loss: 1.501\n",
      "[118] loss: 1.499\n",
      "[119] loss: 1.496\n",
      "[120] loss: 1.493\n",
      "[121] loss: 1.490\n",
      "[122] loss: 1.487\n",
      "[123] loss: 1.485\n",
      "[124] loss: 1.482\n",
      "[125] loss: 1.479\n",
      "[126] loss: 1.477\n",
      "[127] loss: 1.474\n",
      "[128] loss: 1.471\n",
      "[129] loss: 1.469\n",
      "[130] loss: 1.466\n",
      "[131] loss: 1.464\n",
      "[132] loss: 1.461\n",
      "[133] loss: 1.458\n",
      "[134] loss: 1.456\n",
      "[135] loss: 1.453\n",
      "[136] loss: 1.451\n",
      "[137] loss: 1.449\n",
      "[138] loss: 1.446\n",
      "[139] loss: 1.444\n",
      "[140] loss: 1.441\n",
      "[141] loss: 1.439\n",
      "[142] loss: 1.437\n",
      "[143] loss: 1.434\n",
      "[144] loss: 1.432\n",
      "[145] loss: 1.430\n",
      "[146] loss: 1.428\n",
      "[147] loss: 1.425\n",
      "[148] loss: 1.423\n",
      "[149] loss: 1.420\n",
      "[150] loss: 1.418\n",
      "[151] loss: 1.416\n",
      "[152] loss: 1.413\n",
      "[153] loss: 1.411\n",
      "[154] loss: 1.409\n",
      "[155] loss: 1.407\n",
      "[156] loss: 1.405\n",
      "[157] loss: 1.402\n",
      "[158] loss: 1.400\n",
      "[159] loss: 1.397\n",
      "[160] loss: 1.396\n",
      "[161] loss: 1.393\n",
      "[162] loss: 1.391\n",
      "[163] loss: 1.389\n",
      "[164] loss: 1.386\n",
      "[165] loss: 1.384\n",
      "[166] loss: 1.381\n",
      "[167] loss: 1.380\n",
      "[168] loss: 1.377\n",
      "[169] loss: 1.375\n",
      "[170] loss: 1.373\n",
      "[171] loss: 1.370\n",
      "[172] loss: 1.368\n",
      "[173] loss: 1.367\n",
      "[174] loss: 1.363\n",
      "[175] loss: 1.362\n",
      "[176] loss: 1.360\n",
      "[177] loss: 1.358\n",
      "[178] loss: 1.356\n",
      "[179] loss: 1.354\n",
      "[180] loss: 1.353\n",
      "[181] loss: 1.349\n",
      "[182] loss: 1.349\n",
      "[183] loss: 1.346\n",
      "[184] loss: 1.344\n",
      "[185] loss: 1.341\n",
      "[186] loss: 1.340\n",
      "[187] loss: 1.336\n",
      "[188] loss: 1.336\n",
      "[189] loss: 1.334\n",
      "[190] loss: 1.332\n",
      "[191] loss: 1.330\n",
      "[192] loss: 1.327\n",
      "[193] loss: 1.326\n",
      "[194] loss: 1.323\n",
      "[195] loss: 1.322\n",
      "[196] loss: 1.318\n",
      "[197] loss: 1.319\n",
      "[198] loss: 1.314\n",
      "[199] loss: 1.312\n",
      "[200] loss: 1.313\n",
      "[201] loss: 1.310\n",
      "[202] loss: 1.306\n",
      "[203] loss: 1.308\n",
      "[204] loss: 1.302\n",
      "[205] loss: 1.303\n",
      "[206] loss: 1.296\n",
      "[207] loss: 1.300\n",
      "[208] loss: 1.297\n",
      "[209] loss: 1.294\n",
      "[210] loss: 1.290\n",
      "[211] loss: 1.291\n",
      "[212] loss: 1.287\n",
      "[213] loss: 1.287\n",
      "[214] loss: 1.283\n",
      "[215] loss: 1.283\n",
      "[216] loss: 1.279\n",
      "[217] loss: 1.280\n",
      "[218] loss: 1.278\n",
      "[219] loss: 1.272\n",
      "[220] loss: 1.275\n",
      "[221] loss: 1.272\n",
      "[222] loss: 1.270\n",
      "[223] loss: 1.270\n",
      "[224] loss: 1.268\n",
      "[225] loss: 1.266\n",
      "[226] loss: 1.264\n",
      "[227] loss: 1.261\n",
      "[228] loss: 1.256\n",
      "[229] loss: 1.253\n",
      "[230] loss: 1.259\n",
      "[231] loss: 1.255\n",
      "[232] loss: 1.253\n",
      "[233] loss: 1.247\n",
      "[234] loss: 1.251\n",
      "[235] loss: 1.244\n",
      "[236] loss: 1.246\n",
      "[237] loss: 1.246\n",
      "[238] loss: 1.242\n",
      "[239] loss: 1.243\n",
      "[240] loss: 1.239\n",
      "[241] loss: 1.239\n",
      "[242] loss: 1.235\n",
      "[243] loss: 1.234\n",
      "[244] loss: 1.231\n",
      "[245] loss: 1.228\n",
      "[246] loss: 1.231\n",
      "[247] loss: 1.225\n",
      "[248] loss: 1.225\n",
      "[249] loss: 1.223\n",
      "[250] loss: 1.222\n",
      "[251] loss: 1.216\n",
      "[252] loss: 1.216\n",
      "[253] loss: 1.216\n",
      "[254] loss: 1.214\n",
      "[255] loss: 1.209\n",
      "[256] loss: 1.211\n",
      "[257] loss: 1.207\n",
      "[258] loss: 1.205\n",
      "[259] loss: 1.206\n",
      "[260] loss: 1.206\n",
      "[261] loss: 1.203\n",
      "[262] loss: 1.197\n",
      "[263] loss: 1.198\n",
      "[264] loss: 1.197\n",
      "[265] loss: 1.192\n",
      "[266] loss: 1.190\n",
      "[267] loss: 1.194\n",
      "[268] loss: 1.186\n",
      "[269] loss: 1.186\n",
      "[270] loss: 1.184\n",
      "[271] loss: 1.189\n",
      "[272] loss: 1.184\n",
      "[273] loss: 1.180\n",
      "[274] loss: 1.178\n",
      "[275] loss: 1.177\n",
      "[276] loss: 1.179\n",
      "[277] loss: 1.171\n",
      "[278] loss: 1.170\n",
      "[279] loss: 1.170\n",
      "[280] loss: 1.168\n",
      "[281] loss: 1.168\n",
      "[282] loss: 1.170\n",
      "[283] loss: 1.162\n",
      "[284] loss: 1.163\n",
      "[285] loss: 1.159\n",
      "[286] loss: 1.164\n",
      "[287] loss: 1.155\n",
      "[288] loss: 1.156\n",
      "[289] loss: 1.160\n",
      "[290] loss: 1.152\n",
      "[291] loss: 1.149\n",
      "[292] loss: 1.148\n",
      "[293] loss: 1.149\n",
      "[294] loss: 1.150\n",
      "[295] loss: 1.142\n",
      "[296] loss: 1.140\n",
      "[297] loss: 1.142\n",
      "[298] loss: 1.140\n",
      "[299] loss: 1.138\n",
      "[300] loss: 1.141\n",
      "[301] loss: 1.133\n",
      "[302] loss: 1.135\n",
      "[303] loss: 1.129\n",
      "[304] loss: 1.131\n",
      "[305] loss: 1.127\n",
      "[306] loss: 1.127\n",
      "[307] loss: 1.131\n",
      "[308] loss: 1.122\n",
      "[309] loss: 1.125\n",
      "[310] loss: 1.119\n",
      "[311] loss: 1.119\n",
      "[312] loss: 1.119\n",
      "[313] loss: 1.116\n",
      "[314] loss: 1.113\n",
      "[315] loss: 1.114\n",
      "[316] loss: 1.115\n",
      "[317] loss: 1.111\n",
      "[318] loss: 1.103\n",
      "[319] loss: 1.106\n",
      "[320] loss: 1.106\n",
      "[321] loss: 1.103\n",
      "[322] loss: 1.103\n",
      "[323] loss: 1.100\n",
      "[324] loss: 1.097\n",
      "[325] loss: 1.096\n",
      "[326] loss: 1.092\n",
      "[327] loss: 1.103\n",
      "[328] loss: 1.088\n",
      "[329] loss: 1.096\n",
      "[330] loss: 1.085\n",
      "[331] loss: 1.087\n",
      "[332] loss: 1.086\n",
      "[333] loss: 1.086\n",
      "[334] loss: 1.081\n",
      "[335] loss: 1.082\n",
      "[336] loss: 1.084\n",
      "[337] loss: 1.077\n",
      "[338] loss: 1.078\n",
      "[339] loss: 1.075\n",
      "[340] loss: 1.073\n",
      "[341] loss: 1.077\n",
      "[342] loss: 1.071\n",
      "[343] loss: 1.062\n",
      "[344] loss: 1.075\n",
      "[345] loss: 1.061\n",
      "[346] loss: 1.062\n",
      "[347] loss: 1.071\n",
      "[348] loss: 1.058\n",
      "[349] loss: 1.062\n",
      "[350] loss: 1.063\n",
      "[351] loss: 1.054\n",
      "[352] loss: 1.057\n",
      "[353] loss: 1.053\n",
      "[354] loss: 1.049\n",
      "[355] loss: 1.054\n",
      "[356] loss: 1.057\n",
      "[357] loss: 1.048\n",
      "[358] loss: 1.047\n",
      "[359] loss: 1.043\n",
      "[360] loss: 1.044\n",
      "[361] loss: 1.041\n",
      "[362] loss: 1.039\n",
      "[363] loss: 1.040\n",
      "[364] loss: 1.042\n",
      "[365] loss: 1.033\n",
      "[366] loss: 1.040\n",
      "[367] loss: 1.031\n",
      "[368] loss: 1.033\n",
      "[369] loss: 1.035\n",
      "[370] loss: 1.029\n",
      "[371] loss: 1.022\n",
      "[372] loss: 1.026\n",
      "[373] loss: 1.029\n",
      "[374] loss: 1.021\n",
      "[375] loss: 1.024\n",
      "[376] loss: 1.027\n",
      "[377] loss: 1.016\n",
      "[378] loss: 1.018\n",
      "[379] loss: 1.015\n",
      "[380] loss: 1.014\n",
      "[381] loss: 1.018\n",
      "[382] loss: 1.012\n",
      "[383] loss: 1.006\n",
      "[384] loss: 1.014\n",
      "[385] loss: 1.013\n",
      "[386] loss: 1.003\n",
      "[387] loss: 1.010\n",
      "[388] loss: 0.996\n",
      "[389] loss: 1.004\n",
      "[390] loss: 1.000\n",
      "[391] loss: 1.002\n",
      "[392] loss: 1.003\n",
      "[393] loss: 0.996\n",
      "[394] loss: 0.993\n",
      "[395] loss: 0.994\n",
      "[396] loss: 0.997\n",
      "[397] loss: 0.993\n",
      "[398] loss: 0.991\n",
      "[399] loss: 0.991\n",
      "[400] loss: 0.989\n",
      "[401] loss: 0.984\n",
      "[402] loss: 0.982\n",
      "[403] loss: 0.983\n",
      "[404] loss: 0.984\n",
      "[405] loss: 0.972\n",
      "[406] loss: 0.983\n",
      "[407] loss: 0.983\n",
      "[408] loss: 0.976\n",
      "[409] loss: 0.972\n",
      "[410] loss: 0.973\n",
      "[411] loss: 0.967\n",
      "[412] loss: 0.969\n",
      "[413] loss: 0.965\n",
      "[414] loss: 0.968\n",
      "[415] loss: 0.966\n",
      "[416] loss: 0.969\n",
      "[417] loss: 0.959\n",
      "[418] loss: 0.959\n",
      "[419] loss: 0.961\n",
      "[420] loss: 0.961\n",
      "[421] loss: 0.957\n",
      "[422] loss: 0.949\n",
      "[423] loss: 0.952\n",
      "[424] loss: 0.957\n",
      "[425] loss: 0.950\n",
      "[426] loss: 0.961\n",
      "[427] loss: 0.946\n",
      "[428] loss: 0.948\n",
      "[429] loss: 0.950\n",
      "[430] loss: 0.937\n",
      "[431] loss: 0.944\n",
      "[432] loss: 0.946\n",
      "[433] loss: 0.933\n",
      "[434] loss: 0.945\n",
      "[435] loss: 0.939\n",
      "[436] loss: 0.929\n",
      "[437] loss: 0.932\n",
      "[438] loss: 0.935\n",
      "[439] loss: 0.930\n",
      "[440] loss: 0.937\n",
      "[441] loss: 0.922\n",
      "[442] loss: 0.930\n",
      "[443] loss: 0.935\n",
      "[444] loss: 0.919\n",
      "[445] loss: 0.929\n",
      "[446] loss: 0.922\n",
      "[447] loss: 0.928\n",
      "[448] loss: 0.920\n",
      "[449] loss: 0.921\n",
      "[450] loss: 0.918\n",
      "[451] loss: 0.914\n",
      "[452] loss: 0.914\n",
      "[453] loss: 0.913\n",
      "[454] loss: 0.919\n",
      "[455] loss: 0.910\n",
      "[456] loss: 0.905\n",
      "[457] loss: 0.913\n",
      "[458] loss: 0.906\n",
      "[459] loss: 0.902\n",
      "[460] loss: 0.907\n",
      "[461] loss: 0.906\n",
      "[462] loss: 0.902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[463] loss: 0.896\n",
      "[464] loss: 0.898\n",
      "[465] loss: 0.906\n",
      "[466] loss: 0.897\n",
      "[467] loss: 0.890\n",
      "[468] loss: 0.891\n",
      "[469] loss: 0.887\n",
      "[470] loss: 0.888\n",
      "[471] loss: 0.893\n",
      "[472] loss: 0.892\n",
      "[473] loss: 0.895\n",
      "[474] loss: 0.889\n",
      "[475] loss: 0.886\n",
      "[476] loss: 0.889\n",
      "[477] loss: 0.884\n",
      "[478] loss: 0.882\n",
      "[479] loss: 0.877\n",
      "[480] loss: 0.877\n",
      "[481] loss: 0.876\n",
      "[482] loss: 0.873\n",
      "[483] loss: 0.874\n",
      "[484] loss: 0.877\n",
      "[485] loss: 0.876\n",
      "[486] loss: 0.867\n",
      "[487] loss: 0.879\n",
      "[488] loss: 0.869\n",
      "[489] loss: 0.859\n",
      "[490] loss: 0.868\n",
      "[491] loss: 0.870\n",
      "[492] loss: 0.861\n",
      "[493] loss: 0.854\n",
      "[494] loss: 0.857\n",
      "[495] loss: 0.863\n",
      "[496] loss: 0.854\n",
      "[497] loss: 0.856\n",
      "[498] loss: 0.858\n",
      "[499] loss: 0.853\n",
      "[500] loss: 0.857\n",
      "[501] loss: 0.858\n",
      "[502] loss: 0.860\n",
      "[503] loss: 0.856\n",
      "[504] loss: 0.847\n",
      "[505] loss: 0.849\n",
      "[506] loss: 0.847\n",
      "[507] loss: 0.841\n",
      "[508] loss: 0.848\n",
      "[509] loss: 0.835\n",
      "[510] loss: 0.832\n",
      "[511] loss: 0.841\n",
      "[512] loss: 0.839\n",
      "[513] loss: 0.840\n",
      "[514] loss: 0.825\n",
      "[515] loss: 0.840\n",
      "[516] loss: 0.830\n",
      "[517] loss: 0.829\n",
      "[518] loss: 0.826\n",
      "[519] loss: 0.835\n",
      "[520] loss: 0.830\n",
      "[521] loss: 0.830\n",
      "[522] loss: 0.815\n",
      "[523] loss: 0.830\n",
      "[524] loss: 0.823\n",
      "[525] loss: 0.825\n",
      "[526] loss: 0.824\n",
      "[527] loss: 0.816\n",
      "[528] loss: 0.827\n",
      "[529] loss: 0.828\n",
      "[530] loss: 0.814\n",
      "[531] loss: 0.811\n",
      "[532] loss: 0.814\n",
      "[533] loss: 0.806\n",
      "[534] loss: 0.804\n",
      "[535] loss: 0.823\n",
      "[536] loss: 0.829\n",
      "[537] loss: 0.805\n",
      "[538] loss: 0.798\n",
      "[539] loss: 0.811\n",
      "[540] loss: 0.807\n",
      "[541] loss: 0.807\n",
      "[542] loss: 0.811\n",
      "[543] loss: 0.807\n",
      "[544] loss: 0.801\n",
      "[545] loss: 0.784\n",
      "[546] loss: 0.801\n",
      "[547] loss: 0.804\n",
      "[548] loss: 0.793\n",
      "[549] loss: 0.793\n",
      "[550] loss: 0.791\n",
      "[551] loss: 0.796\n",
      "[552] loss: 0.781\n",
      "[553] loss: 0.791\n",
      "[554] loss: 0.793\n",
      "[555] loss: 0.784\n",
      "[556] loss: 0.786\n",
      "[557] loss: 0.793\n",
      "[558] loss: 0.785\n",
      "[559] loss: 0.777\n",
      "[560] loss: 0.786\n",
      "[561] loss: 0.773\n",
      "[562] loss: 0.773\n",
      "[563] loss: 0.769\n",
      "[564] loss: 0.784\n",
      "[565] loss: 0.780\n",
      "[566] loss: 0.774\n",
      "[567] loss: 0.773\n",
      "[568] loss: 0.786\n",
      "[569] loss: 0.759\n",
      "[570] loss: 0.769\n",
      "[571] loss: 0.755\n",
      "[572] loss: 0.758\n",
      "[573] loss: 0.779\n",
      "[574] loss: 0.763\n",
      "[575] loss: 0.759\n",
      "[576] loss: 0.774\n",
      "[577] loss: 0.761\n",
      "[578] loss: 0.768\n",
      "[579] loss: 0.767\n",
      "[580] loss: 0.757\n",
      "[581] loss: 0.755\n",
      "[582] loss: 0.750\n",
      "[583] loss: 0.752\n",
      "[584] loss: 0.762\n",
      "[585] loss: 0.755\n",
      "[586] loss: 0.756\n",
      "[587] loss: 0.750\n",
      "[588] loss: 0.745\n",
      "[589] loss: 0.746\n",
      "[590] loss: 0.751\n",
      "[591] loss: 0.750\n",
      "[592] loss: 0.743\n",
      "[593] loss: 0.737\n",
      "[594] loss: 0.741\n",
      "[595] loss: 0.745\n",
      "[596] loss: 0.730\n",
      "[597] loss: 0.741\n",
      "[598] loss: 0.739\n",
      "[599] loss: 0.750\n",
      "[600] loss: 0.738\n",
      "[601] loss: 0.746\n",
      "[602] loss: 0.729\n",
      "[603] loss: 0.728\n",
      "[604] loss: 0.742\n",
      "[605] loss: 0.718\n",
      "[606] loss: 0.725\n",
      "[607] loss: 0.744\n",
      "[608] loss: 0.743\n",
      "[609] loss: 0.711\n",
      "[610] loss: 0.724\n",
      "[611] loss: 0.734\n",
      "[612] loss: 0.716\n",
      "[613] loss: 0.714\n",
      "[614] loss: 0.727\n",
      "[615] loss: 0.718\n",
      "[616] loss: 0.724\n",
      "[617] loss: 0.706\n",
      "[618] loss: 0.714\n",
      "[619] loss: 0.715\n",
      "[620] loss: 0.724\n",
      "[621] loss: 0.719\n",
      "[622] loss: 0.712\n",
      "[623] loss: 0.712\n",
      "[624] loss: 0.714\n",
      "[625] loss: 0.717\n",
      "[626] loss: 0.704\n",
      "[627] loss: 0.708\n",
      "[628] loss: 0.716\n",
      "[629] loss: 0.693\n",
      "[630] loss: 0.706\n",
      "[631] loss: 0.718\n",
      "[632] loss: 0.692\n",
      "[633] loss: 0.705\n",
      "[634] loss: 0.676\n",
      "[635] loss: 0.709\n",
      "[636] loss: 0.716\n",
      "[637] loss: 0.701\n",
      "[638] loss: 0.685\n",
      "[639] loss: 0.692\n",
      "[640] loss: 0.690\n",
      "[641] loss: 0.694\n",
      "[642] loss: 0.681\n",
      "[643] loss: 0.691\n",
      "[644] loss: 0.686\n",
      "[645] loss: 0.682\n",
      "[646] loss: 0.688\n",
      "[647] loss: 0.675\n",
      "[648] loss: 0.672\n",
      "[649] loss: 0.702\n",
      "[650] loss: 0.671\n",
      "[651] loss: 0.681\n",
      "[652] loss: 0.682\n",
      "[653] loss: 0.685\n",
      "[654] loss: 0.690\n",
      "[655] loss: 0.671\n",
      "[656] loss: 0.668\n",
      "[657] loss: 0.680\n",
      "[658] loss: 0.671\n",
      "[659] loss: 0.689\n",
      "[660] loss: 0.668\n",
      "[661] loss: 0.672\n",
      "[662] loss: 0.675\n",
      "[663] loss: 0.661\n",
      "[664] loss: 0.672\n",
      "[665] loss: 0.684\n",
      "[666] loss: 0.658\n",
      "[667] loss: 0.658\n",
      "[668] loss: 0.679\n",
      "[669] loss: 0.662\n",
      "[670] loss: 0.665\n",
      "[671] loss: 0.649\n",
      "[672] loss: 0.643\n",
      "[673] loss: 0.668\n",
      "[674] loss: 0.646\n",
      "[675] loss: 0.680\n",
      "[676] loss: 0.643\n",
      "[677] loss: 0.645\n",
      "[678] loss: 0.664\n",
      "[679] loss: 0.659\n",
      "[680] loss: 0.644\n",
      "[681] loss: 0.651\n",
      "[682] loss: 0.640\n",
      "[683] loss: 0.657\n",
      "[684] loss: 0.650\n",
      "[685] loss: 0.636\n",
      "[686] loss: 0.633\n",
      "[687] loss: 0.676\n",
      "[688] loss: 0.653\n",
      "[689] loss: 0.636\n",
      "[690] loss: 0.652\n",
      "[691] loss: 0.633\n",
      "[692] loss: 0.642\n",
      "[693] loss: 0.648\n",
      "[694] loss: 0.625\n",
      "[695] loss: 0.651\n",
      "[696] loss: 0.645\n",
      "[697] loss: 0.642\n",
      "[698] loss: 0.630\n",
      "[699] loss: 0.626\n",
      "[700] loss: 0.627\n",
      "[701] loss: 0.618\n",
      "[702] loss: 0.638\n",
      "[703] loss: 0.634\n",
      "[704] loss: 0.628\n",
      "[705] loss: 0.614\n",
      "[706] loss: 0.644\n",
      "[707] loss: 0.626\n",
      "[708] loss: 0.628\n",
      "[709] loss: 0.611\n",
      "[710] loss: 0.637\n",
      "[711] loss: 0.616\n",
      "[712] loss: 0.612\n",
      "[713] loss: 0.614\n",
      "[714] loss: 0.615\n",
      "[715] loss: 0.610\n",
      "[716] loss: 0.626\n",
      "[717] loss: 0.626\n",
      "[718] loss: 0.589\n",
      "[719] loss: 0.611\n",
      "[720] loss: 0.620\n",
      "[721] loss: 0.611\n",
      "[722] loss: 0.589\n",
      "[723] loss: 0.610\n",
      "[724] loss: 0.621\n",
      "[725] loss: 0.617\n",
      "[726] loss: 0.620\n",
      "[727] loss: 0.597\n",
      "[728] loss: 0.596\n",
      "[729] loss: 0.606\n",
      "[730] loss: 0.579\n",
      "[731] loss: 0.619\n",
      "[732] loss: 0.583\n",
      "[733] loss: 0.618\n",
      "[734] loss: 0.594\n",
      "[735] loss: 0.602\n",
      "[736] loss: 0.600\n",
      "[737] loss: 0.615\n",
      "[738] loss: 0.576\n",
      "[739] loss: 0.610\n",
      "[740] loss: 0.579\n",
      "[741] loss: 0.590\n",
      "[742] loss: 0.609\n",
      "[743] loss: 0.591\n",
      "[744] loss: 0.580\n",
      "[745] loss: 0.580\n",
      "[746] loss: 0.632\n",
      "[747] loss: 0.578\n",
      "[748] loss: 0.586\n",
      "[749] loss: 0.575\n",
      "[750] loss: 0.578\n",
      "[751] loss: 0.570\n",
      "[752] loss: 0.596\n",
      "[753] loss: 0.576\n",
      "[754] loss: 0.594\n",
      "[755] loss: 0.569\n",
      "[756] loss: 0.594\n",
      "[757] loss: 0.608\n",
      "[758] loss: 0.568\n",
      "[759] loss: 0.576\n",
      "[760] loss: 0.547\n",
      "[761] loss: 0.570\n",
      "[762] loss: 0.578\n",
      "[763] loss: 0.557\n",
      "[764] loss: 0.570\n",
      "[765] loss: 0.589\n",
      "[766] loss: 0.584\n",
      "[767] loss: 0.556\n",
      "[768] loss: 0.574\n",
      "[769] loss: 0.579\n",
      "[770] loss: 0.550\n",
      "[771] loss: 0.592\n",
      "[772] loss: 0.571\n",
      "[773] loss: 0.574\n",
      "[774] loss: 0.575\n",
      "[775] loss: 0.581\n",
      "[776] loss: 0.552\n",
      "[777] loss: 0.537\n",
      "[778] loss: 0.588\n",
      "[779] loss: 0.535\n",
      "[780] loss: 0.550\n",
      "[781] loss: 0.583\n",
      "[782] loss: 0.561\n",
      "[783] loss: 0.554\n",
      "[784] loss: 0.554\n",
      "[785] loss: 0.558\n",
      "[786] loss: 0.577\n",
      "[787] loss: 0.547\n",
      "[788] loss: 0.535\n",
      "[789] loss: 0.538\n",
      "[790] loss: 0.547\n",
      "[791] loss: 0.559\n",
      "[792] loss: 0.530\n",
      "[793] loss: 0.552\n",
      "[794] loss: 0.534\n",
      "[795] loss: 0.576\n",
      "[796] loss: 0.535\n",
      "[797] loss: 0.559\n",
      "[798] loss: 0.534\n",
      "[799] loss: 0.536\n",
      "[800] loss: 0.528\n",
      "[801] loss: 0.551\n",
      "[802] loss: 0.540\n",
      "[803] loss: 0.567\n",
      "[804] loss: 0.529\n",
      "[805] loss: 0.525\n",
      "[806] loss: 0.546\n",
      "[807] loss: 0.538\n",
      "[808] loss: 0.516\n",
      "[809] loss: 0.534\n",
      "[810] loss: 0.502\n",
      "[811] loss: 0.551\n",
      "[812] loss: 0.548\n",
      "[813] loss: 0.524\n",
      "[814] loss: 0.564\n",
      "[815] loss: 0.519\n",
      "[816] loss: 0.509\n",
      "[817] loss: 0.544\n",
      "[818] loss: 0.550\n",
      "[819] loss: 0.501\n",
      "[820] loss: 0.516\n",
      "[821] loss: 0.536\n",
      "[822] loss: 0.527\n",
      "[823] loss: 0.497\n",
      "[824] loss: 0.550\n",
      "[825] loss: 0.482\n",
      "[826] loss: 0.540\n",
      "[827] loss: 0.500\n",
      "[828] loss: 0.509\n",
      "[829] loss: 0.492\n",
      "[830] loss: 0.544\n",
      "[831] loss: 0.537\n",
      "[832] loss: 0.502\n",
      "[833] loss: 0.507\n",
      "[834] loss: 0.502\n",
      "[835] loss: 0.523\n",
      "[836] loss: 0.508\n",
      "[837] loss: 0.528\n",
      "[838] loss: 0.491\n",
      "[839] loss: 0.512\n",
      "[840] loss: 0.517\n",
      "[841] loss: 0.470\n",
      "[842] loss: 0.565\n",
      "[843] loss: 0.505\n",
      "[844] loss: 0.512\n",
      "[845] loss: 0.479\n",
      "[846] loss: 0.513\n",
      "[847] loss: 0.500\n",
      "[848] loss: 0.508\n",
      "[849] loss: 0.452\n",
      "[850] loss: 0.485\n",
      "[851] loss: 0.516\n",
      "[852] loss: 0.486\n",
      "[853] loss: 0.474\n",
      "[854] loss: 0.508\n",
      "[855] loss: 0.554\n",
      "[856] loss: 0.511\n",
      "[857] loss: 0.467\n",
      "[858] loss: 0.476\n",
      "[859] loss: 0.500\n",
      "[860] loss: 0.473\n",
      "[861] loss: 0.486\n",
      "[862] loss: 0.474\n",
      "[863] loss: 0.481\n",
      "[864] loss: 0.482\n",
      "[865] loss: 0.470\n",
      "[866] loss: 0.503\n",
      "[867] loss: 0.465\n",
      "[868] loss: 0.465\n",
      "[869] loss: 0.481\n",
      "[870] loss: 0.480\n",
      "[871] loss: 0.453\n",
      "[872] loss: 0.510\n",
      "[873] loss: 0.480\n",
      "[874] loss: 0.512\n",
      "[875] loss: 0.484\n",
      "[876] loss: 0.482\n",
      "[877] loss: 0.470\n",
      "[878] loss: 0.503\n",
      "[879] loss: 0.492\n",
      "[880] loss: 0.489\n",
      "[881] loss: 0.512\n",
      "[882] loss: 0.482\n",
      "[883] loss: 0.451\n",
      "[884] loss: 0.480\n",
      "[885] loss: 0.464\n",
      "[886] loss: 0.498\n",
      "[887] loss: 0.434\n",
      "[888] loss: 0.493\n",
      "[889] loss: 0.445\n",
      "[890] loss: 0.444\n",
      "[891] loss: 0.498\n",
      "[892] loss: 0.445\n",
      "[893] loss: 0.454\n",
      "[894] loss: 0.514\n",
      "[895] loss: 0.456\n",
      "[896] loss: 0.453\n",
      "[897] loss: 0.442\n",
      "[898] loss: 0.461\n",
      "[899] loss: 0.489\n",
      "[900] loss: 0.470\n",
      "[901] loss: 0.484\n",
      "[902] loss: 0.442\n",
      "[903] loss: 0.457\n",
      "[904] loss: 0.440\n",
      "[905] loss: 0.448\n",
      "[906] loss: 0.486\n",
      "[907] loss: 0.496\n",
      "[908] loss: 0.413\n",
      "[909] loss: 0.480\n",
      "[910] loss: 0.452\n",
      "[911] loss: 0.479\n",
      "[912] loss: 0.428\n",
      "[913] loss: 0.432\n",
      "[914] loss: 0.443\n",
      "[915] loss: 0.415\n",
      "[916] loss: 0.484\n",
      "[917] loss: 0.466\n",
      "[918] loss: 0.434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[919] loss: 0.454\n",
      "[920] loss: 0.474\n",
      "[921] loss: 0.418\n",
      "[922] loss: 0.425\n",
      "[923] loss: 0.417\n",
      "[924] loss: 0.432\n",
      "[925] loss: 0.450\n",
      "[926] loss: 0.450\n",
      "[927] loss: 0.538\n",
      "[928] loss: 0.402\n",
      "[929] loss: 0.449\n",
      "[930] loss: 0.448\n",
      "[931] loss: 0.398\n",
      "[932] loss: 0.512\n",
      "[933] loss: 0.435\n",
      "[934] loss: 0.409\n",
      "[935] loss: 0.465\n",
      "[936] loss: 0.460\n",
      "[937] loss: 0.422\n",
      "[938] loss: 0.415\n",
      "[939] loss: 0.426\n",
      "[940] loss: 0.437\n",
      "[941] loss: 0.375\n",
      "[942] loss: 0.461\n",
      "[943] loss: 0.399\n",
      "[944] loss: 0.402\n",
      "[945] loss: 0.445\n",
      "[946] loss: 0.463\n",
      "[947] loss: 0.425\n",
      "[948] loss: 0.420\n",
      "[949] loss: 0.434\n",
      "[950] loss: 0.427\n",
      "[951] loss: 0.406\n",
      "[952] loss: 0.433\n",
      "[953] loss: 0.483\n",
      "[954] loss: 0.462\n",
      "[955] loss: 0.434\n",
      "[956] loss: 0.395\n",
      "[957] loss: 0.436\n",
      "[958] loss: 0.398\n",
      "[959] loss: 0.372\n",
      "[960] loss: 0.434\n",
      "[961] loss: 0.458\n",
      "[962] loss: 0.418\n",
      "[963] loss: 0.451\n",
      "[964] loss: 0.437\n",
      "[965] loss: 0.397\n",
      "[966] loss: 0.372\n",
      "[967] loss: 0.427\n",
      "[968] loss: 0.427\n",
      "[969] loss: 0.377\n",
      "[970] loss: 0.377\n",
      "[971] loss: 0.368\n",
      "[972] loss: 0.429\n",
      "[973] loss: 0.442\n",
      "[974] loss: 0.400\n",
      "[975] loss: 0.477\n",
      "[976] loss: 0.378\n",
      "[977] loss: 0.433\n",
      "[978] loss: 0.372\n",
      "[979] loss: 0.433\n",
      "[980] loss: 0.419\n",
      "[981] loss: 0.408\n",
      "[982] loss: 0.349\n",
      "[983] loss: 0.437\n",
      "[984] loss: 0.406\n",
      "[985] loss: 0.366\n",
      "[986] loss: 0.357\n",
      "[987] loss: 0.464\n",
      "[988] loss: 0.396\n",
      "[989] loss: 0.395\n",
      "[990] loss: 0.351\n",
      "[991] loss: 0.391\n",
      "[992] loss: 0.417\n",
      "[993] loss: 0.452\n",
      "[994] loss: 0.319\n",
      "[995] loss: 0.386\n",
      "[996] loss: 0.399\n",
      "[997] loss: 0.385\n",
      "[998] loss: 0.416\n",
      "[999] loss: 0.430\n",
      "[1000] loss: 0.403\n",
      "[1001] loss: 0.353\n",
      "[1002] loss: 0.399\n",
      "[1003] loss: 0.402\n",
      "[1004] loss: 0.399\n",
      "[1005] loss: 0.392\n",
      "[1006] loss: 0.419\n",
      "[1007] loss: 0.385\n",
      "[1008] loss: 0.363\n",
      "[1009] loss: 0.406\n",
      "[1010] loss: 0.472\n",
      "[1011] loss: 0.331\n",
      "[1012] loss: 0.381\n",
      "[1013] loss: 0.349\n",
      "[1014] loss: 0.332\n",
      "[1015] loss: 0.323\n",
      "[1016] loss: 0.432\n",
      "[1017] loss: 0.364\n",
      "[1018] loss: 0.464\n",
      "[1019] loss: 0.328\n",
      "[1020] loss: 0.373\n",
      "[1021] loss: 0.462\n",
      "[1022] loss: 0.361\n",
      "[1023] loss: 0.372\n",
      "[1024] loss: 0.410\n",
      "[1025] loss: 0.309\n",
      "[1026] loss: 0.400\n",
      "[1027] loss: 0.385\n",
      "[1028] loss: 0.343\n",
      "[1029] loss: 0.359\n",
      "[1030] loss: 0.347\n",
      "[1031] loss: 0.426\n",
      "[1032] loss: 0.392\n",
      "[1033] loss: 0.416\n",
      "[1034] loss: 0.318\n",
      "[1035] loss: 0.379\n",
      "[1036] loss: 0.454\n",
      "[1037] loss: 0.338\n",
      "[1038] loss: 0.405\n",
      "[1039] loss: 0.287\n",
      "[1040] loss: 0.408\n",
      "[1041] loss: 0.365\n",
      "[1042] loss: 0.348\n",
      "[1043] loss: 0.422\n",
      "[1044] loss: 0.362\n",
      "[1045] loss: 0.308\n",
      "[1046] loss: 0.324\n",
      "[1047] loss: 0.327\n",
      "[1048] loss: 0.336\n",
      "[1049] loss: 0.330\n",
      "[1050] loss: 0.468\n",
      "[1051] loss: 0.390\n",
      "[1052] loss: 0.366\n",
      "[1053] loss: 0.383\n",
      "[1054] loss: 0.362\n",
      "[1055] loss: 0.316\n",
      "[1056] loss: 0.361\n",
      "[1057] loss: 0.397\n",
      "[1058] loss: 0.425\n",
      "[1059] loss: 0.342\n",
      "[1060] loss: 0.295\n",
      "[1061] loss: 0.387\n",
      "[1062] loss: 0.344\n",
      "[1063] loss: 0.363\n",
      "[1064] loss: 0.327\n",
      "[1065] loss: 0.311\n",
      "[1066] loss: 0.367\n",
      "[1067] loss: 0.316\n",
      "[1068] loss: 0.385\n",
      "[1069] loss: 0.367\n",
      "[1070] loss: 0.305\n",
      "[1071] loss: 0.368\n",
      "[1072] loss: 0.441\n",
      "[1073] loss: 0.337\n",
      "[1074] loss: 0.328\n",
      "[1075] loss: 0.353\n",
      "[1076] loss: 0.353\n",
      "[1077] loss: 0.315\n",
      "[1078] loss: 0.338\n",
      "[1079] loss: 0.338\n",
      "[1080] loss: 0.341\n",
      "[1081] loss: 0.345\n",
      "[1082] loss: 0.334\n",
      "[1083] loss: 0.323\n",
      "[1084] loss: 0.338\n",
      "[1085] loss: 0.430\n",
      "[1086] loss: 0.322\n",
      "[1087] loss: 0.427\n",
      "[1088] loss: 0.258\n",
      "[1089] loss: 0.274\n",
      "[1090] loss: 0.386\n",
      "[1091] loss: 0.354\n",
      "[1092] loss: 0.291\n",
      "[1093] loss: 0.482\n",
      "[1094] loss: 0.251\n",
      "[1095] loss: 0.299\n",
      "[1096] loss: 0.278\n",
      "[1097] loss: 0.434\n",
      "[1098] loss: 0.281\n",
      "[1099] loss: 0.398\n",
      "[1100] loss: 0.294\n",
      "[1101] loss: 0.320\n",
      "[1102] loss: 0.295\n",
      "[1103] loss: 0.266\n",
      "[1104] loss: 0.378\n",
      "[1105] loss: 0.351\n",
      "[1106] loss: 0.377\n",
      "[1107] loss: 0.309\n",
      "[1108] loss: 0.345\n",
      "[1109] loss: 0.268\n",
      "[1110] loss: 0.446\n",
      "[1111] loss: 0.328\n",
      "[1112] loss: 0.265\n",
      "[1113] loss: 0.404\n",
      "[1114] loss: 0.316\n",
      "[1115] loss: 0.315\n",
      "[1116] loss: 0.283\n",
      "[1117] loss: 0.262\n",
      "[1118] loss: 0.344\n",
      "[1119] loss: 0.230\n",
      "[1120] loss: 0.377\n",
      "[1121] loss: 0.288\n",
      "[1122] loss: 0.379\n",
      "[1123] loss: 0.486\n",
      "[1124] loss: 0.303\n",
      "[1125] loss: 0.372\n",
      "[1126] loss: 0.336\n",
      "[1127] loss: 0.328\n",
      "[1128] loss: 0.325\n",
      "[1129] loss: 0.226\n",
      "[1130] loss: 0.304\n",
      "[1131] loss: 0.251\n",
      "[1132] loss: 0.410\n",
      "[1133] loss: 0.313\n",
      "[1134] loss: 0.373\n",
      "[1135] loss: 0.292\n",
      "[1136] loss: 0.309\n",
      "[1137] loss: 0.255\n",
      "[1138] loss: 0.291\n",
      "[1139] loss: 0.285\n",
      "[1140] loss: 0.357\n",
      "[1141] loss: 0.276\n",
      "[1142] loss: 0.214\n",
      "[1143] loss: 0.323\n",
      "[1144] loss: 0.388\n",
      "[1145] loss: 0.317\n",
      "[1146] loss: 0.357\n",
      "[1147] loss: 0.284\n",
      "[1148] loss: 0.383\n",
      "[1149] loss: 0.260\n",
      "[1150] loss: 0.323\n",
      "[1151] loss: 0.240\n",
      "[1152] loss: 0.255\n",
      "[1153] loss: 0.393\n",
      "[1154] loss: 0.345\n",
      "[1155] loss: 0.312\n",
      "[1156] loss: 0.247\n",
      "[1157] loss: 0.290\n",
      "[1158] loss: 0.336\n",
      "[1159] loss: 0.305\n",
      "[1160] loss: 0.286\n",
      "[1161] loss: 0.389\n",
      "[1162] loss: 0.277\n",
      "[1163] loss: 0.246\n",
      "[1164] loss: 0.311\n",
      "[1165] loss: 0.309\n",
      "[1166] loss: 0.311\n",
      "[1167] loss: 0.323\n",
      "[1168] loss: 0.310\n",
      "[1169] loss: 0.330\n",
      "[1170] loss: 0.311\n",
      "[1171] loss: 0.266\n",
      "[1172] loss: 0.300\n",
      "[1173] loss: 0.254\n",
      "[1174] loss: 0.365\n",
      "[1175] loss: 0.301\n",
      "[1176] loss: 0.232\n",
      "[1177] loss: 0.304\n",
      "[1178] loss: 0.245\n",
      "[1179] loss: 0.331\n",
      "[1180] loss: 0.289\n",
      "[1181] loss: 0.276\n",
      "[1182] loss: 0.513\n",
      "[1183] loss: 0.219\n",
      "[1184] loss: 0.279\n",
      "[1185] loss: 0.226\n",
      "[1186] loss: 0.247\n",
      "[1187] loss: 0.324\n",
      "[1188] loss: 0.306\n",
      "[1189] loss: 0.247\n",
      "[1190] loss: 0.278\n",
      "[1191] loss: 0.317\n",
      "[1192] loss: 0.464\n",
      "[1193] loss: 0.256\n",
      "[1194] loss: 0.206\n",
      "[1195] loss: 0.187\n",
      "[1196] loss: 0.286\n",
      "[1197] loss: 0.344\n",
      "[1198] loss: 0.250\n",
      "[1199] loss: 0.310\n",
      "[1200] loss: 0.239\n",
      "[1201] loss: 0.218\n",
      "[1202] loss: 0.313\n",
      "[1203] loss: 0.298\n",
      "[1204] loss: 0.274\n",
      "[1205] loss: 0.307\n",
      "[1206] loss: 0.288\n",
      "[1207] loss: 0.319\n",
      "[1208] loss: 0.297\n",
      "[1209] loss: 0.190\n",
      "[1210] loss: 0.377\n",
      "[1211] loss: 0.328\n",
      "[1212] loss: 0.244\n",
      "[1213] loss: 0.238\n",
      "[1214] loss: 0.377\n",
      "[1215] loss: 0.257\n",
      "[1216] loss: 0.202\n",
      "[1217] loss: 0.336\n",
      "[1218] loss: 0.301\n",
      "[1219] loss: 0.321\n",
      "[1220] loss: 0.188\n",
      "[1221] loss: 0.324\n",
      "[1222] loss: 0.291\n",
      "[1223] loss: 0.176\n",
      "[1224] loss: 0.320\n",
      "[1225] loss: 0.221\n",
      "[1226] loss: 0.178\n",
      "[1227] loss: 0.243\n",
      "[1228] loss: 0.177\n",
      "[1229] loss: 0.450\n",
      "[1230] loss: 0.402\n",
      "[1231] loss: 0.258\n",
      "[1232] loss: 0.175\n",
      "[1233] loss: 0.169\n",
      "[1234] loss: 0.239\n",
      "[1235] loss: 0.419\n",
      "[1236] loss: 0.398\n",
      "[1237] loss: 0.312\n",
      "[1238] loss: 0.206\n",
      "[1239] loss: 0.171\n",
      "[1240] loss: 0.166\n",
      "[1241] loss: 0.329\n",
      "[1242] loss: 0.291\n",
      "[1243] loss: 0.386\n",
      "[1244] loss: 0.174\n",
      "[1245] loss: 0.253\n",
      "[1246] loss: 0.228\n",
      "[1247] loss: 0.190\n",
      "[1248] loss: 0.166\n",
      "[1249] loss: 0.164\n",
      "[1250] loss: 0.312\n",
      "[1251] loss: 0.459\n",
      "[1252] loss: 0.288\n",
      "[1253] loss: 0.294\n",
      "[1254] loss: 0.167\n",
      "[1255] loss: 0.241\n",
      "[1256] loss: 0.311\n",
      "[1257] loss: 0.306\n",
      "[1258] loss: 0.169\n",
      "[1259] loss: 0.262\n",
      "[1260] loss: 0.304\n",
      "[1261] loss: 0.266\n",
      "[1262] loss: 0.162\n",
      "[1263] loss: 0.330\n",
      "[1264] loss: 0.252\n",
      "[1265] loss: 0.282\n",
      "[1266] loss: 0.337\n",
      "[1267] loss: 0.163\n",
      "[1268] loss: 0.160\n",
      "[1269] loss: 0.156\n",
      "[1270] loss: 0.158\n",
      "[1271] loss: 0.451\n",
      "[1272] loss: 0.315\n",
      "[1273] loss: 0.401\n",
      "[1274] loss: 0.281\n",
      "[1275] loss: 0.161\n",
      "[1276] loss: 0.225\n",
      "[1277] loss: 0.166\n",
      "[1278] loss: 0.272\n",
      "[1279] loss: 0.328\n",
      "[1280] loss: 0.446\n",
      "[1281] loss: 0.294\n",
      "[1282] loss: 0.214\n",
      "[1283] loss: 0.155\n",
      "[1284] loss: 0.202\n",
      "[1285] loss: 0.245\n",
      "[1286] loss: 0.154\n",
      "[1287] loss: 0.151\n",
      "[1288] loss: 0.149\n",
      "[1289] loss: 0.148\n",
      "[1290] loss: 0.148\n",
      "[1291] loss: 0.335\n",
      "[1292] loss: 0.475\n",
      "[1293] loss: 0.328\n",
      "[1294] loss: 0.252\n",
      "[1295] loss: 0.257\n",
      "[1296] loss: 0.174\n",
      "[1297] loss: 0.528\n",
      "[1298] loss: 0.154\n",
      "[1299] loss: 0.149\n",
      "[1300] loss: 0.147\n",
      "[1301] loss: 0.148\n",
      "[1302] loss: 0.395\n",
      "[1303] loss: 0.422\n",
      "[1304] loss: 0.332\n",
      "[1305] loss: 0.435\n",
      "[1306] loss: 0.372\n",
      "[1307] loss: 0.188\n",
      "[1308] loss: 0.195\n",
      "[1309] loss: 0.216\n",
      "[1310] loss: 0.240\n",
      "[1311] loss: 0.147\n",
      "[1312] loss: 0.144\n",
      "[1313] loss: 0.143\n",
      "[1314] loss: 0.142\n",
      "[1315] loss: 0.290\n",
      "[1316] loss: 0.427\n",
      "[1317] loss: 0.150\n",
      "[1318] loss: 0.145\n",
      "[1319] loss: 0.141\n",
      "[1320] loss: 0.139\n",
      "[1321] loss: 0.300\n",
      "[1322] loss: 0.428\n",
      "[1323] loss: 0.213\n",
      "[1324] loss: 0.191\n",
      "[1325] loss: 0.216\n",
      "[1326] loss: 0.377\n",
      "[1327] loss: 0.428\n",
      "[1328] loss: 0.184\n",
      "[1329] loss: 0.144\n",
      "[1330] loss: 0.139\n",
      "[1331] loss: 0.139\n",
      "[1332] loss: 0.137\n",
      "[1333] loss: 0.254\n",
      "[1334] loss: 0.306\n",
      "[1335] loss: 0.240\n",
      "[1336] loss: 0.274\n",
      "[1337] loss: 0.142\n",
      "[1338] loss: 0.137\n",
      "[1339] loss: 0.269\n",
      "[1340] loss: 0.139\n",
      "[1341] loss: 0.134\n",
      "[1342] loss: 0.255\n",
      "[1343] loss: 0.577\n",
      "[1344] loss: 0.341\n",
      "[1345] loss: 0.332\n",
      "[1346] loss: 0.142\n",
      "[1347] loss: 0.136\n",
      "[1348] loss: 0.134\n",
      "[1349] loss: 0.133\n",
      "[1350] loss: 0.132\n",
      "[1351] loss: 0.131\n",
      "[1352] loss: 0.130\n",
      "[1353] loss: 0.130\n",
      "[1354] loss: 0.457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1355] loss: 0.558\n",
      "[1356] loss: 0.360\n",
      "[1357] loss: 0.210\n",
      "[1358] loss: 0.134\n",
      "[1359] loss: 0.242\n",
      "[1360] loss: 0.132\n",
      "[1361] loss: 0.130\n",
      "[1362] loss: 0.129\n",
      "[1363] loss: 0.207\n",
      "[1364] loss: 0.217\n",
      "[1365] loss: 0.467\n",
      "[1366] loss: 0.171\n",
      "[1367] loss: 0.132\n",
      "[1368] loss: 0.129\n",
      "[1369] loss: 0.127\n",
      "[1370] loss: 0.126\n",
      "[1371] loss: 0.125\n",
      "[1372] loss: 0.125\n",
      "[1373] loss: 0.126\n",
      "[1374] loss: 0.321\n",
      "[1375] loss: 0.704\n",
      "[1376] loss: 0.473\n",
      "[1377] loss: 0.205\n",
      "[1378] loss: 0.134\n",
      "[1379] loss: 0.128\n",
      "[1380] loss: 0.126\n",
      "[1381] loss: 0.125\n",
      "[1382] loss: 0.222\n",
      "[1383] loss: 0.405\n",
      "[1384] loss: 0.366\n",
      "[1385] loss: 0.254\n",
      "[1386] loss: 0.199\n",
      "[1387] loss: 0.127\n",
      "[1388] loss: 0.125\n",
      "[1389] loss: 0.123\n",
      "[1390] loss: 0.122\n",
      "[1391] loss: 0.121\n",
      "[1392] loss: 0.121\n",
      "[1393] loss: 0.121\n",
      "[1394] loss: 0.301\n",
      "[1395] loss: 0.335\n",
      "[1396] loss: 0.512\n",
      "[1397] loss: 0.299\n",
      "[1398] loss: 0.307\n",
      "[1399] loss: 0.128\n",
      "[1400] loss: 0.123\n",
      "[1401] loss: 0.392\n",
      "[1402] loss: 0.457\n",
      "[1403] loss: 0.332\n",
      "[1404] loss: 0.128\n",
      "[1405] loss: 0.124\n",
      "[1406] loss: 0.122\n",
      "[1407] loss: 0.121\n",
      "[1408] loss: 0.119\n",
      "[1409] loss: 0.119\n",
      "[1410] loss: 0.118\n",
      "[1411] loss: 0.200\n",
      "[1412] loss: 0.261\n",
      "[1413] loss: 0.137\n",
      "[1414] loss: 0.521\n",
      "[1415] loss: 0.206\n",
      "[1416] loss: 0.121\n",
      "[1417] loss: 0.118\n",
      "[1418] loss: 0.117\n",
      "[1419] loss: 0.117\n",
      "[1420] loss: 0.217\n",
      "[1421] loss: 0.129\n",
      "[1422] loss: 0.117\n",
      "[1423] loss: 0.115\n",
      "[1424] loss: 0.115\n",
      "[1425] loss: 0.249\n",
      "[1426] loss: 0.609\n",
      "[1427] loss: 0.200\n",
      "[1428] loss: 0.118\n",
      "[1429] loss: 0.115\n",
      "[1430] loss: 0.115\n",
      "[1431] loss: 0.113\n",
      "[1432] loss: 0.113\n",
      "[1433] loss: 0.112\n",
      "[1434] loss: 0.398\n",
      "[1435] loss: 0.268\n",
      "[1436] loss: 0.340\n",
      "[1437] loss: 0.191\n",
      "[1438] loss: 0.116\n",
      "[1439] loss: 0.113\n",
      "[1440] loss: 0.112\n",
      "[1441] loss: 0.111\n",
      "[1442] loss: 0.110\n",
      "[1443] loss: 0.110\n",
      "[1444] loss: 0.110\n",
      "[1445] loss: 0.716\n",
      "[1446] loss: 0.367\n",
      "[1447] loss: 0.267\n",
      "[1448] loss: 0.117\n",
      "[1449] loss: 0.113\n",
      "[1450] loss: 0.111\n",
      "[1451] loss: 0.326\n",
      "[1452] loss: 0.130\n",
      "[1453] loss: 0.574\n",
      "[1454] loss: 0.470\n",
      "[1455] loss: 0.584\n",
      "[1456] loss: 0.485\n",
      "[1457] loss: 0.336\n",
      "[1458] loss: 0.383\n",
      "[1459] loss: 0.156\n",
      "[1460] loss: 0.314\n",
      "[1461] loss: 0.271\n",
      "[1462] loss: 0.221\n",
      "[1463] loss: 0.136\n",
      "[1464] loss: 0.126\n",
      "[1465] loss: 0.123\n",
      "[1466] loss: 0.121\n",
      "[1467] loss: 0.119\n",
      "[1468] loss: 0.118\n",
      "[1469] loss: 0.198\n",
      "[1470] loss: 0.125\n",
      "[1471] loss: 0.267\n",
      "[1472] loss: 0.299\n",
      "[1473] loss: 0.199\n",
      "[1474] loss: 0.119\n",
      "[1475] loss: 0.117\n",
      "[1476] loss: 0.115\n",
      "[1477] loss: 0.114\n",
      "[1478] loss: 0.113\n",
      "[1479] loss: 0.112\n",
      "[1480] loss: 0.111\n",
      "[1481] loss: 0.112\n",
      "[1482] loss: 0.111\n",
      "[1483] loss: 0.482\n",
      "[1484] loss: 0.466\n",
      "[1485] loss: 0.186\n",
      "[1486] loss: 0.371\n",
      "[1487] loss: 0.123\n",
      "[1488] loss: 0.282\n",
      "[1489] loss: 0.116\n",
      "[1490] loss: 0.112\n",
      "[1491] loss: 0.111\n",
      "[1492] loss: 0.110\n",
      "[1493] loss: 0.183\n",
      "[1494] loss: 0.169\n",
      "[1495] loss: 0.500\n",
      "[1496] loss: 0.183\n",
      "[1497] loss: 0.225\n",
      "[1498] loss: 0.111\n",
      "[1499] loss: 0.108\n",
      "[1500] loss: 0.106\n",
      "[1501] loss: 0.105\n",
      "[1502] loss: 0.104\n",
      "[1503] loss: 0.104\n",
      "[1504] loss: 0.103\n",
      "[1505] loss: 0.102\n",
      "[1506] loss: 0.102\n",
      "[1507] loss: 0.101\n",
      "[1508] loss: 0.101\n",
      "[1509] loss: 0.100\n",
      "[1510] loss: 0.100\n",
      "[1511] loss: 0.100\n",
      "[1512] loss: 0.099\n",
      "[1513] loss: 0.099\n",
      "[1514] loss: 0.099\n",
      "[1515] loss: 0.098\n",
      "[1516] loss: 0.098\n",
      "[1517] loss: 0.807\n",
      "[1518] loss: 0.660\n",
      "[1519] loss: 0.542\n",
      "[1520] loss: 0.306\n",
      "[1521] loss: 0.185\n",
      "[1522] loss: 0.108\n",
      "[1523] loss: 0.104\n",
      "[1524] loss: 0.102\n",
      "[1525] loss: 0.101\n",
      "[1526] loss: 0.100\n",
      "[1527] loss: 0.099\n",
      "[1528] loss: 0.098\n",
      "[1529] loss: 0.098\n",
      "[1530] loss: 0.098\n",
      "[1531] loss: 0.097\n",
      "[1532] loss: 0.096\n",
      "[1533] loss: 0.096\n",
      "[1534] loss: 0.096\n",
      "[1535] loss: 0.096\n",
      "[1536] loss: 0.096\n",
      "[1537] loss: 0.095\n",
      "[1538] loss: 0.094\n",
      "[1539] loss: 0.094\n",
      "[1540] loss: 0.094\n",
      "[1541] loss: 0.094\n",
      "[1542] loss: 0.094\n",
      "[1543] loss: 0.723\n",
      "[1544] loss: 0.739\n",
      "[1545] loss: 0.466\n",
      "[1546] loss: 0.211\n",
      "[1547] loss: 0.105\n",
      "[1548] loss: 0.100\n",
      "[1549] loss: 0.098\n",
      "[1550] loss: 0.097\n",
      "[1551] loss: 0.096\n",
      "[1552] loss: 0.095\n",
      "[1553] loss: 0.095\n",
      "[1554] loss: 0.094\n",
      "[1555] loss: 0.094\n",
      "[1556] loss: 0.093\n",
      "[1557] loss: 0.093\n",
      "[1558] loss: 0.092\n",
      "[1559] loss: 0.092\n",
      "[1560] loss: 0.092\n",
      "[1561] loss: 0.091\n",
      "[1562] loss: 0.091\n",
      "[1563] loss: 0.091\n",
      "[1564] loss: 0.091\n",
      "[1565] loss: 0.090\n",
      "[1566] loss: 0.090\n",
      "[1567] loss: 0.349\n",
      "[1568] loss: 0.778\n",
      "[1569] loss: 0.608\n",
      "[1570] loss: 0.462\n",
      "[1571] loss: 0.388\n",
      "[1572] loss: 0.106\n",
      "[1573] loss: 0.099\n",
      "[1574] loss: 0.096\n",
      "[1575] loss: 0.095\n",
      "[1576] loss: 0.094\n",
      "[1577] loss: 0.093\n",
      "[1578] loss: 0.092\n",
      "[1579] loss: 0.092\n",
      "[1580] loss: 0.091\n",
      "[1581] loss: 0.091\n",
      "[1582] loss: 0.090\n",
      "[1583] loss: 0.090\n",
      "[1584] loss: 0.090\n",
      "[1585] loss: 0.090\n",
      "[1586] loss: 0.089\n",
      "[1587] loss: 0.089\n",
      "[1588] loss: 0.088\n",
      "[1589] loss: 0.088\n",
      "[1590] loss: 0.785\n",
      "[1591] loss: 0.609\n",
      "[1592] loss: 0.161\n",
      "[1593] loss: 0.096\n",
      "[1594] loss: 0.093\n",
      "[1595] loss: 0.091\n",
      "[1596] loss: 0.090\n",
      "[1597] loss: 0.090\n",
      "[1598] loss: 0.089\n",
      "[1599] loss: 0.089\n",
      "[1600] loss: 0.088\n",
      "[1601] loss: 0.088\n",
      "[1602] loss: 0.087\n",
      "[1603] loss: 0.087\n",
      "[1604] loss: 0.087\n",
      "[1605] loss: 0.086\n",
      "[1606] loss: 0.086\n",
      "[1607] loss: 0.086\n",
      "[1608] loss: 0.086\n",
      "[1609] loss: 0.086\n",
      "[1610] loss: 0.085\n",
      "[1611] loss: 0.086\n",
      "[1612] loss: 0.085\n",
      "[1613] loss: 0.898\n",
      "[1614] loss: 0.674\n",
      "[1615] loss: 0.651\n",
      "[1616] loss: 0.595\n",
      "[1617] loss: 0.347\n",
      "[1618] loss: 0.530\n",
      "[1619] loss: 0.251\n",
      "[1620] loss: 0.343\n",
      "[1621] loss: 0.109\n",
      "[1622] loss: 0.100\n",
      "[1623] loss: 0.096\n",
      "[1624] loss: 0.094\n",
      "[1625] loss: 0.092\n",
      "[1626] loss: 0.091\n",
      "[1627] loss: 0.090\n",
      "[1628] loss: 0.089\n",
      "[1629] loss: 0.088\n",
      "[1630] loss: 0.088\n",
      "[1631] loss: 0.087\n",
      "[1632] loss: 0.087\n",
      "[1633] loss: 0.086\n",
      "[1634] loss: 0.086\n",
      "[1635] loss: 0.086\n",
      "[1636] loss: 0.085\n",
      "[1637] loss: 0.085\n",
      "[1638] loss: 0.084\n",
      "[1639] loss: 0.084\n",
      "[1640] loss: 0.084\n",
      "[1641] loss: 0.084\n",
      "[1642] loss: 0.083\n",
      "[1643] loss: 0.083\n",
      "[1644] loss: 0.083\n",
      "[1645] loss: 0.083\n",
      "[1646] loss: 0.082\n",
      "[1647] loss: 0.082\n",
      "[1648] loss: 0.082\n",
      "[1649] loss: 0.082\n",
      "[1650] loss: 0.082\n",
      "[1651] loss: 0.327\n",
      "[1652] loss: 0.816\n",
      "[1653] loss: 0.652\n",
      "[1654] loss: 0.773\n",
      "[1655] loss: 0.531\n",
      "[1656] loss: 0.426\n",
      "[1657] loss: 0.274\n",
      "[1658] loss: 0.124\n",
      "[1659] loss: 0.283\n",
      "[1660] loss: 0.175\n",
      "[1661] loss: 0.097\n",
      "[1662] loss: 0.094\n",
      "[1663] loss: 0.092\n",
      "[1664] loss: 0.091\n",
      "[1665] loss: 0.090\n",
      "[1666] loss: 0.089\n",
      "[1667] loss: 0.088\n",
      "[1668] loss: 0.087\n",
      "[1669] loss: 0.087\n",
      "[1670] loss: 0.086\n",
      "[1671] loss: 0.086\n",
      "[1672] loss: 0.085\n",
      "[1673] loss: 0.085\n",
      "[1674] loss: 0.085\n",
      "[1675] loss: 0.084\n",
      "[1676] loss: 0.084\n",
      "[1677] loss: 0.084\n",
      "[1678] loss: 0.083\n",
      "[1679] loss: 0.083\n",
      "[1680] loss: 0.083\n",
      "[1681] loss: 0.083\n",
      "[1682] loss: 0.082\n",
      "[1683] loss: 0.082\n",
      "[1684] loss: 0.082\n",
      "[1685] loss: 0.082\n",
      "[1686] loss: 0.232\n",
      "[1687] loss: 0.787\n",
      "[1688] loss: 0.367\n",
      "[1689] loss: 0.101\n",
      "[1690] loss: 0.086\n",
      "[1691] loss: 0.083\n",
      "[1692] loss: 0.082\n",
      "[1693] loss: 0.081\n",
      "[1694] loss: 0.081\n",
      "[1695] loss: 0.080\n",
      "[1696] loss: 0.080\n",
      "[1697] loss: 0.079\n",
      "[1698] loss: 0.079\n",
      "[1699] loss: 0.078\n",
      "[1700] loss: 0.078\n",
      "[1701] loss: 0.078\n",
      "[1702] loss: 0.078\n",
      "[1703] loss: 0.078\n",
      "[1704] loss: 0.077\n",
      "[1705] loss: 0.077\n",
      "[1706] loss: 0.077\n",
      "[1707] loss: 0.077\n",
      "[1708] loss: 0.076\n",
      "[1709] loss: 0.076\n",
      "[1710] loss: 0.076\n",
      "[1711] loss: 0.076\n",
      "[1712] loss: 0.076\n",
      "[1713] loss: 0.363\n",
      "[1714] loss: 0.858\n",
      "[1715] loss: 0.682\n",
      "[1716] loss: 0.570\n",
      "[1717] loss: 0.217\n",
      "[1718] loss: 0.088\n",
      "[1719] loss: 0.084\n",
      "[1720] loss: 0.082\n",
      "[1721] loss: 0.081\n",
      "[1722] loss: 0.080\n",
      "[1723] loss: 0.079\n",
      "[1724] loss: 0.078\n",
      "[1725] loss: 0.078\n",
      "[1726] loss: 0.077\n",
      "[1727] loss: 0.077\n",
      "[1728] loss: 0.077\n",
      "[1729] loss: 0.076\n",
      "[1730] loss: 0.076\n",
      "[1731] loss: 0.076\n",
      "[1732] loss: 0.075\n",
      "[1733] loss: 0.075\n",
      "[1734] loss: 0.075\n",
      "[1735] loss: 0.075\n",
      "[1736] loss: 0.075\n",
      "[1737] loss: 0.074\n",
      "[1738] loss: 0.074\n",
      "[1739] loss: 0.074\n",
      "[1740] loss: 0.074\n",
      "[1741] loss: 0.074\n",
      "[1742] loss: 0.073\n",
      "[1743] loss: 0.073\n",
      "[1744] loss: 0.073\n",
      "[1745] loss: 0.319\n",
      "[1746] loss: 0.897\n",
      "[1747] loss: 0.652\n",
      "[1748] loss: 0.307\n",
      "[1749] loss: 0.236\n",
      "[1750] loss: 0.085\n",
      "[1751] loss: 0.080\n",
      "[1752] loss: 0.079\n",
      "[1753] loss: 0.077\n",
      "[1754] loss: 0.076\n",
      "[1755] loss: 0.076\n",
      "[1756] loss: 0.075\n",
      "[1757] loss: 0.075\n",
      "[1758] loss: 0.075\n",
      "[1759] loss: 0.074\n",
      "[1760] loss: 0.074\n",
      "[1761] loss: 0.074\n",
      "[1762] loss: 0.073\n",
      "[1763] loss: 0.073\n",
      "[1764] loss: 0.073\n",
      "[1765] loss: 0.073\n",
      "[1766] loss: 0.072\n",
      "[1767] loss: 0.072\n",
      "[1768] loss: 0.072\n",
      "[1769] loss: 0.072\n",
      "[1770] loss: 0.072\n",
      "[1771] loss: 0.071\n",
      "[1772] loss: 0.071\n",
      "[1773] loss: 0.071\n",
      "[1774] loss: 0.071\n",
      "[1775] loss: 0.071\n",
      "[1776] loss: 0.071\n",
      "[1777] loss: 0.070\n",
      "[1778] loss: 0.072\n",
      "[1779] loss: 0.070\n",
      "[1780] loss: 0.070\n",
      "[1781] loss: 0.070\n",
      "[1782] loss: 0.070\n",
      "[1783] loss: 0.070\n",
      "[1784] loss: 0.069\n",
      "[1785] loss: 0.069\n",
      "[1786] loss: 0.069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1787] loss: 0.069\n",
      "[1788] loss: 0.069\n",
      "[1789] loss: 0.069\n",
      "[1790] loss: 0.069\n",
      "[1791] loss: 0.068\n",
      "[1792] loss: 0.068\n",
      "[1793] loss: 0.068\n",
      "[1794] loss: 0.068\n",
      "[1795] loss: 0.068\n",
      "[1796] loss: 0.068\n",
      "[1797] loss: 0.068\n",
      "[1798] loss: 0.068\n",
      "[1799] loss: 0.067\n",
      "[1800] loss: 0.067\n",
      "[1801] loss: 0.067\n",
      "[1802] loss: 0.992\n",
      "[1803] loss: 0.818\n",
      "[1804] loss: 0.750\n",
      "[1805] loss: 0.599\n",
      "[1806] loss: 0.499\n",
      "[1807] loss: 0.522\n",
      "[1808] loss: 0.278\n",
      "[1809] loss: 0.444\n",
      "[1810] loss: 0.099\n",
      "[1811] loss: 0.083\n",
      "[1812] loss: 0.079\n",
      "[1813] loss: 0.077\n",
      "[1814] loss: 0.076\n",
      "[1815] loss: 0.074\n",
      "[1816] loss: 0.074\n",
      "[1817] loss: 0.073\n",
      "[1818] loss: 0.072\n",
      "[1819] loss: 0.072\n",
      "[1820] loss: 0.071\n",
      "[1821] loss: 0.071\n",
      "[1822] loss: 0.071\n",
      "[1823] loss: 0.070\n",
      "[1824] loss: 0.070\n",
      "[1825] loss: 0.070\n",
      "[1826] loss: 0.069\n",
      "[1827] loss: 0.069\n",
      "[1828] loss: 0.069\n",
      "[1829] loss: 0.069\n",
      "[1830] loss: 0.069\n",
      "[1831] loss: 0.068\n",
      "[1832] loss: 0.068\n",
      "[1833] loss: 0.068\n",
      "[1834] loss: 0.068\n",
      "[1835] loss: 0.068\n",
      "[1836] loss: 0.067\n",
      "[1837] loss: 0.067\n",
      "[1838] loss: 0.067\n",
      "[1839] loss: 0.067\n",
      "[1840] loss: 0.067\n",
      "[1841] loss: 0.067\n",
      "[1842] loss: 0.066\n",
      "[1843] loss: 0.066\n",
      "[1844] loss: 0.066\n",
      "[1845] loss: 0.066\n",
      "[1846] loss: 0.066\n",
      "[1847] loss: 0.066\n",
      "[1848] loss: 0.066\n",
      "[1849] loss: 0.065\n",
      "[1850] loss: 0.065\n",
      "[1851] loss: 0.316\n",
      "[1852] loss: 0.882\n",
      "[1853] loss: 0.749\n",
      "[1854] loss: 0.643\n",
      "[1855] loss: 0.592\n",
      "[1856] loss: 0.508\n",
      "[1857] loss: 0.382\n",
      "[1858] loss: 0.102\n",
      "[1859] loss: 0.080\n",
      "[1860] loss: 0.076\n",
      "[1861] loss: 0.074\n",
      "[1862] loss: 0.073\n",
      "[1863] loss: 0.072\n",
      "[1864] loss: 0.071\n",
      "[1865] loss: 0.070\n",
      "[1866] loss: 0.070\n",
      "[1867] loss: 0.069\n",
      "[1868] loss: 0.069\n",
      "[1869] loss: 0.068\n",
      "[1870] loss: 0.068\n",
      "[1871] loss: 0.068\n",
      "[1872] loss: 0.068\n",
      "[1873] loss: 0.067\n",
      "[1874] loss: 0.067\n",
      "[1875] loss: 0.067\n",
      "[1876] loss: 0.067\n",
      "[1877] loss: 0.066\n",
      "[1878] loss: 0.066\n",
      "[1879] loss: 0.066\n",
      "[1880] loss: 0.066\n",
      "[1881] loss: 0.066\n",
      "[1882] loss: 0.065\n",
      "[1883] loss: 0.065\n",
      "[1884] loss: 0.065\n",
      "[1885] loss: 0.065\n",
      "[1886] loss: 0.065\n",
      "[1887] loss: 0.065\n",
      "[1888] loss: 0.065\n",
      "[1889] loss: 0.064\n",
      "[1890] loss: 0.064\n",
      "[1891] loss: 0.064\n",
      "[1892] loss: 0.064\n",
      "[1893] loss: 0.064\n",
      "[1894] loss: 0.064\n",
      "[1895] loss: 0.064\n",
      "[1896] loss: 0.063\n",
      "[1897] loss: 0.063\n",
      "[1898] loss: 0.063\n",
      "[1899] loss: 0.063\n",
      "[1900] loss: 0.063\n",
      "[1901] loss: 0.063\n",
      "[1902] loss: 0.063\n",
      "[1903] loss: 0.063\n",
      "[1904] loss: 0.063\n",
      "[1905] loss: 0.062\n",
      "[1906] loss: 0.062\n",
      "[1907] loss: 0.430\n",
      "[1908] loss: 0.935\n",
      "[1909] loss: 0.651\n",
      "[1910] loss: 0.675\n",
      "[1911] loss: 0.290\n",
      "[1912] loss: 0.577\n",
      "[1913] loss: 0.443\n",
      "[1914] loss: 0.086\n",
      "[1915] loss: 0.076\n",
      "[1916] loss: 0.073\n",
      "[1917] loss: 0.071\n",
      "[1918] loss: 0.069\n",
      "[1919] loss: 0.068\n",
      "[1920] loss: 0.068\n",
      "[1921] loss: 0.067\n",
      "[1922] loss: 0.067\n",
      "[1923] loss: 0.066\n",
      "[1924] loss: 0.066\n",
      "[1925] loss: 0.065\n",
      "[1926] loss: 0.065\n",
      "[1927] loss: 0.065\n",
      "[1928] loss: 0.065\n",
      "[1929] loss: 0.064\n",
      "[1930] loss: 0.064\n",
      "[1931] loss: 0.064\n",
      "[1932] loss: 0.064\n",
      "[1933] loss: 0.063\n",
      "[1934] loss: 0.063\n",
      "[1935] loss: 0.063\n",
      "[1936] loss: 0.063\n",
      "[1937] loss: 0.063\n",
      "[1938] loss: 0.063\n",
      "[1939] loss: 0.063\n",
      "[1940] loss: 0.062\n",
      "[1941] loss: 0.062\n",
      "[1942] loss: 0.062\n",
      "[1943] loss: 0.062\n",
      "[1944] loss: 0.062\n",
      "[1945] loss: 0.062\n",
      "[1946] loss: 0.062\n",
      "[1947] loss: 0.061\n",
      "[1948] loss: 0.061\n",
      "[1949] loss: 0.061\n",
      "[1950] loss: 0.061\n",
      "[1951] loss: 0.061\n",
      "[1952] loss: 0.061\n",
      "[1953] loss: 0.061\n",
      "[1954] loss: 0.061\n",
      "[1955] loss: 0.060\n",
      "[1956] loss: 0.060\n",
      "[1957] loss: 0.060\n",
      "[1958] loss: 0.060\n",
      "[1959] loss: 0.060\n",
      "[1960] loss: 0.060\n",
      "[1961] loss: 0.060\n",
      "[1962] loss: 0.060\n",
      "[1963] loss: 0.060\n",
      "[1964] loss: 0.060\n",
      "[1965] loss: 0.060\n",
      "[1966] loss: 0.059\n",
      "[1967] loss: 0.059\n",
      "[1968] loss: 0.059\n",
      "[1969] loss: 0.059\n",
      "[1970] loss: 0.059\n",
      "[1971] loss: 0.059\n",
      "[1972] loss: 0.059\n",
      "[1973] loss: 0.571\n",
      "[1974] loss: 0.919\n",
      "[1975] loss: 0.810\n",
      "[1976] loss: 0.791\n",
      "[1977] loss: 0.656\n",
      "[1978] loss: 0.447\n",
      "[1979] loss: 0.379\n",
      "[1980] loss: 0.444\n",
      "[1981] loss: 0.442\n",
      "[1982] loss: 0.088\n",
      "[1983] loss: 0.339\n",
      "[1984] loss: 0.077\n",
      "[1985] loss: 0.072\n",
      "[1986] loss: 0.070\n",
      "[1987] loss: 0.069\n",
      "[1988] loss: 0.067\n",
      "[1989] loss: 0.066\n",
      "[1990] loss: 0.066\n",
      "[1991] loss: 0.065\n",
      "[1992] loss: 0.065\n",
      "[1993] loss: 0.064\n",
      "[1994] loss: 0.064\n",
      "[1995] loss: 0.063\n",
      "[1996] loss: 0.063\n",
      "[1997] loss: 0.063\n",
      "[1998] loss: 0.063\n",
      "[1999] loss: 0.062\n",
      "[2000] loss: 0.062\n",
      "[2001] loss: 0.062\n",
      "[2002] loss: 0.062\n",
      "[2003] loss: 0.061\n",
      "[2004] loss: 0.061\n",
      "[2005] loss: 0.061\n",
      "[2006] loss: 0.061\n",
      "[2007] loss: 0.061\n",
      "[2008] loss: 0.061\n",
      "[2009] loss: 0.060\n",
      "[2010] loss: 0.060\n",
      "[2011] loss: 0.060\n",
      "[2012] loss: 0.060\n",
      "[2013] loss: 0.060\n",
      "[2014] loss: 0.060\n",
      "[2015] loss: 0.060\n",
      "[2016] loss: 0.059\n",
      "[2017] loss: 0.059\n",
      "[2018] loss: 0.059\n",
      "[2019] loss: 0.059\n",
      "[2020] loss: 0.059\n",
      "[2021] loss: 0.059\n",
      "[2022] loss: 0.059\n",
      "[2023] loss: 0.059\n",
      "[2024] loss: 0.058\n",
      "[2025] loss: 0.058\n",
      "[2026] loss: 0.058\n",
      "[2027] loss: 0.058\n",
      "[2028] loss: 0.058\n",
      "[2029] loss: 0.058\n",
      "[2030] loss: 0.058\n",
      "[2031] loss: 0.058\n",
      "[2032] loss: 0.058\n",
      "[2033] loss: 0.058\n",
      "[2034] loss: 0.057\n",
      "[2035] loss: 0.057\n",
      "[2036] loss: 0.057\n",
      "[2037] loss: 0.057\n",
      "[2038] loss: 0.057\n",
      "[2039] loss: 0.057\n",
      "[2040] loss: 0.057\n",
      "[2041] loss: 0.057\n",
      "[2042] loss: 0.057\n",
      "[2043] loss: 0.057\n",
      "[2044] loss: 0.057\n",
      "[2045] loss: 0.056\n",
      "[2046] loss: 0.056\n",
      "[2047] loss: 0.056\n",
      "[2048] loss: 0.056\n",
      "[2049] loss: 0.056\n",
      "[2050] loss: 0.056\n",
      "[2051] loss: 0.056\n",
      "[2052] loss: 0.056\n",
      "[2053] loss: 0.056\n",
      "[2054] loss: 0.056\n",
      "[2055] loss: 0.056\n",
      "[2056] loss: 0.056\n",
      "[2057] loss: 0.056\n",
      "[2058] loss: 0.055\n",
      "[2059] loss: 0.056\n",
      "[2060] loss: 0.055\n",
      "[2061] loss: 0.055\n",
      "[2062] loss: 0.055\n",
      "[2063] loss: 0.055\n",
      "[2064] loss: 0.055\n",
      "[2065] loss: 0.055\n",
      "[2066] loss: 0.055\n",
      "[2067] loss: 0.055\n",
      "[2068] loss: 0.055\n",
      "[2069] loss: 0.055\n",
      "[2070] loss: 0.054\n",
      "[2071] loss: 0.054\n",
      "[2072] loss: 0.054\n",
      "[2073] loss: 0.054\n",
      "[2074] loss: 0.054\n",
      "[2075] loss: 0.054\n",
      "[2076] loss: 0.054\n",
      "[2077] loss: 0.054\n",
      "[2078] loss: 0.054\n",
      "[2079] loss: 0.432\n",
      "[2080] loss: 1.047\n",
      "[2081] loss: 0.840\n",
      "[2082] loss: 0.785\n",
      "[2083] loss: 0.729\n",
      "[2084] loss: 0.668\n",
      "[2085] loss: 0.592\n",
      "[2086] loss: 0.455\n",
      "[2087] loss: 0.516\n",
      "[2088] loss: 0.412\n",
      "[2089] loss: 0.385\n",
      "[2090] loss: 0.084\n",
      "[2091] loss: 0.073\n",
      "[2092] loss: 0.069\n",
      "[2093] loss: 0.067\n",
      "[2094] loss: 0.066\n",
      "[2095] loss: 0.064\n",
      "[2096] loss: 0.063\n",
      "[2097] loss: 0.063\n",
      "[2098] loss: 0.062\n",
      "[2099] loss: 0.061\n",
      "[2100] loss: 0.061\n",
      "[2101] loss: 0.060\n",
      "[2102] loss: 0.060\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6a247be27830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-8050d2d5ed3e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs, train_loader, losses)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d480e33359dc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_model(model, 1500, train_loader, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Cross Entropy durante o Treinamento\")\n",
    "plt.tight_layout()\n",
    "_ = plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(results):\n",
    "    results = results.cpu().detach().numpy().tolist()[0]\n",
    "    return results.index(max(results))\n",
    "\n",
    "def evaluate(model, dataset):\n",
    "    inputs_list = []\n",
    "    labels_list = []\n",
    "    for _, (inputs, labels) in enumerate(dataset):\n",
    "        inputs_list.append(inputs.to(device))\n",
    "        labels_list.append(labels.to(device))\n",
    "\n",
    "    acuracia = 0\n",
    "    results = [0,0,0,0,0,0,0,0,0,0]\n",
    "    for i, (inputs, labels) in enumerate(zip(inputs_list, labels_list), 0):\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        result = one_hot(y_pred)\n",
    "\n",
    "        if result == labels.item():\n",
    "            acuracia += 1\n",
    "\n",
    "        results[result] += 1\n",
    "        \n",
    "    print(acuracia / len(dataset) * 100, \"%\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=1)\n",
    "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do Modelo no dataset de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4c852c457a72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-8b76cc84f22f>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dataset)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d480e33359dc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mthreshold\u001b[0;34m(input, threshold, value, inplace)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do Modelo (conjunto de teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
