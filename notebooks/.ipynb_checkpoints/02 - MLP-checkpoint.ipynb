{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos deste trabalho\n",
    "- Familiarizar-se com a biblioteca PyTorch\n",
    "- Definir arquiteturas MLP simples em PyTorch\n",
    "- Treinar utilizando CIFAR10, testando diferentes arquiteturas, parâmetros, funções de loss e otimizadores\n",
    "- Comparar os resultados obtidos utilizando apenas Perpceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Carregar os datasets\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "dataset_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=200)\n",
    "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a arquitetura MLP\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.activation_function = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32)\n",
    "        x = self.activation_function(self.fc1(x))\n",
    "        x = self.activation_function(self.fc2(x))\n",
    "        x = self.activation_function(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "MLP(\n",
      "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (activation_function): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "# Definir otimizador e loss\n",
    "# Nota: testar outros otimizadores e funções de loss (em particular cross entropy)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2, weight_decay=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "print(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar o treinamento aqui\n",
    "losses = []\n",
    "def train_model(model, epochs, train_loader, losses):\n",
    "    inputs_list = []\n",
    "    labels_list = []\n",
    "    for _, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs_list.append(inputs.to(device))\n",
    "        labels_list.append(labels.to(device))\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(zip(inputs_list, labels_list), 0):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(inputs)\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            running_loss += loss.item()\n",
    "            if i % 250 == 249:\n",
    "                print('[%d] loss: %.3f' %\n",
    "                      (epoch + 1, running_loss / 250))\n",
    "                losses.append(running_loss / 250)\n",
    "                running_loss = 0.0\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainamento do Modelo\n",
    "- Ativação: ReLu\n",
    "- Loss: CrossEntropy\n",
    "- Hidden: 3\n",
    "- Regularizador: L2\n",
    "- Taxa de Aprendizado = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 2.303\n",
      "[2] loss: 2.301\n",
      "[3] loss: 2.299\n",
      "[4] loss: 2.297\n",
      "[5] loss: 2.296\n",
      "[6] loss: 2.294\n",
      "[7] loss: 2.293\n",
      "[8] loss: 2.292\n",
      "[9] loss: 2.290\n",
      "[10] loss: 2.289\n",
      "[11] loss: 2.288\n",
      "[12] loss: 2.286\n",
      "[13] loss: 2.285\n",
      "[14] loss: 2.284\n",
      "[15] loss: 2.282\n",
      "[16] loss: 2.281\n",
      "[17] loss: 2.279\n",
      "[18] loss: 2.278\n",
      "[19] loss: 2.276\n",
      "[20] loss: 2.274\n",
      "[21] loss: 2.273\n",
      "[22] loss: 2.271\n",
      "[23] loss: 2.269\n",
      "[24] loss: 2.267\n",
      "[25] loss: 2.266\n",
      "[26] loss: 2.264\n",
      "[27] loss: 2.262\n",
      "[28] loss: 2.260\n",
      "[29] loss: 2.258\n",
      "[30] loss: 2.256\n",
      "[31] loss: 2.254\n",
      "[32] loss: 2.252\n",
      "[33] loss: 2.250\n",
      "[34] loss: 2.248\n",
      "[35] loss: 2.246\n",
      "[36] loss: 2.245\n",
      "[37] loss: 2.243\n",
      "[38] loss: 2.241\n",
      "[39] loss: 2.239\n",
      "[40] loss: 2.237\n",
      "[41] loss: 2.235\n",
      "[42] loss: 2.233\n",
      "[43] loss: 2.231\n",
      "[44] loss: 2.228\n",
      "[45] loss: 2.226\n",
      "[46] loss: 2.223\n",
      "[47] loss: 2.221\n",
      "[48] loss: 2.219\n",
      "[49] loss: 2.217\n",
      "[50] loss: 2.215\n",
      "[51] loss: 2.212\n",
      "[52] loss: 2.210\n",
      "[53] loss: 2.208\n",
      "[54] loss: 2.206\n",
      "[55] loss: 2.205\n",
      "[56] loss: 2.203\n",
      "[57] loss: 2.201\n",
      "[58] loss: 2.199\n",
      "[59] loss: 2.197\n",
      "[60] loss: 2.196\n",
      "[61] loss: 2.194\n",
      "[62] loss: 2.192\n",
      "[63] loss: 2.190\n",
      "[64] loss: 2.189\n",
      "[65] loss: 2.187\n",
      "[66] loss: 2.186\n",
      "[67] loss: 2.184\n",
      "[68] loss: 2.183\n",
      "[69] loss: 2.181\n",
      "[70] loss: 2.180\n",
      "[71] loss: 2.178\n",
      "[72] loss: 2.177\n",
      "[73] loss: 2.175\n",
      "[74] loss: 2.174\n",
      "[75] loss: 2.172\n",
      "[76] loss: 2.171\n",
      "[77] loss: 2.170\n",
      "[78] loss: 2.168\n",
      "[79] loss: 2.167\n",
      "[80] loss: 2.165\n",
      "[81] loss: 2.164\n",
      "[82] loss: 2.163\n",
      "[83] loss: 2.161\n",
      "[84] loss: 2.160\n",
      "[85] loss: 2.159\n",
      "[86] loss: 2.157\n",
      "[87] loss: 2.156\n",
      "[88] loss: 2.155\n",
      "[89] loss: 2.153\n",
      "[90] loss: 2.152\n",
      "[91] loss: 2.151\n",
      "[92] loss: 2.149\n",
      "[93] loss: 2.148\n",
      "[94] loss: 2.147\n",
      "[95] loss: 2.146\n",
      "[96] loss: 2.145\n",
      "[97] loss: 2.143\n",
      "[98] loss: 2.142\n",
      "[99] loss: 2.141\n",
      "[100] loss: 2.140\n",
      "[101] loss: 2.139\n",
      "[102] loss: 2.138\n",
      "[103] loss: 2.137\n",
      "[104] loss: 2.136\n",
      "[105] loss: 2.135\n",
      "[106] loss: 2.134\n",
      "[107] loss: 2.133\n",
      "[108] loss: 2.132\n",
      "[109] loss: 2.131\n",
      "[110] loss: 2.130\n",
      "[111] loss: 2.129\n",
      "[112] loss: 2.128\n",
      "[113] loss: 2.127\n",
      "[114] loss: 2.126\n",
      "[115] loss: 2.125\n",
      "[116] loss: 2.124\n",
      "[117] loss: 2.124\n",
      "[118] loss: 2.123\n",
      "[119] loss: 2.122\n",
      "[120] loss: 2.121\n",
      "[121] loss: 2.121\n",
      "[122] loss: 2.120\n",
      "[123] loss: 2.119\n",
      "[124] loss: 2.118\n",
      "[125] loss: 2.118\n",
      "[126] loss: 2.117\n",
      "[127] loss: 2.116\n",
      "[128] loss: 2.116\n",
      "[129] loss: 2.115\n",
      "[130] loss: 2.114\n",
      "[131] loss: 2.114\n",
      "[132] loss: 2.113\n",
      "[133] loss: 2.112\n",
      "[134] loss: 2.112\n",
      "[135] loss: 2.111\n",
      "[136] loss: 2.111\n",
      "[137] loss: 2.110\n",
      "[138] loss: 2.109\n",
      "[139] loss: 2.109\n",
      "[140] loss: 2.108\n",
      "[141] loss: 2.108\n",
      "[142] loss: 2.107\n",
      "[143] loss: 2.107\n",
      "[144] loss: 2.106\n",
      "[145] loss: 2.105\n",
      "[146] loss: 2.105\n",
      "[147] loss: 2.104\n",
      "[148] loss: 2.104\n",
      "[149] loss: 2.103\n",
      "[150] loss: 2.103\n",
      "[151] loss: 2.102\n",
      "[152] loss: 2.102\n",
      "[153] loss: 2.101\n",
      "[154] loss: 2.101\n",
      "[155] loss: 2.100\n",
      "[156] loss: 2.100\n",
      "[157] loss: 2.099\n",
      "[158] loss: 2.098\n",
      "[159] loss: 2.098\n",
      "[160] loss: 2.097\n",
      "[161] loss: 2.096\n",
      "[162] loss: 2.096\n",
      "[163] loss: 2.094\n",
      "[164] loss: 2.092\n",
      "[165] loss: 2.090\n",
      "[166] loss: 2.089\n",
      "[167] loss: 2.088\n",
      "[168] loss: 2.087\n",
      "[169] loss: 2.086\n",
      "[170] loss: 2.085\n",
      "[171] loss: 2.085\n",
      "[172] loss: 2.084\n",
      "[173] loss: 2.083\n",
      "[174] loss: 2.083\n",
      "[175] loss: 2.082\n",
      "[176] loss: 2.081\n",
      "[177] loss: 2.081\n",
      "[178] loss: 2.080\n",
      "[179] loss: 2.080\n",
      "[180] loss: 2.079\n",
      "[181] loss: 2.079\n",
      "[182] loss: 2.078\n",
      "[183] loss: 2.077\n",
      "[184] loss: 2.077\n",
      "[185] loss: 2.076\n",
      "[186] loss: 2.076\n",
      "[187] loss: 2.075\n",
      "[188] loss: 2.075\n",
      "[189] loss: 2.074\n",
      "[190] loss: 2.074\n",
      "[191] loss: 2.073\n",
      "[192] loss: 2.073\n",
      "[193] loss: 2.073\n",
      "[194] loss: 2.072\n",
      "[195] loss: 2.072\n",
      "[196] loss: 2.071\n",
      "[197] loss: 2.071\n",
      "[198] loss: 2.070\n",
      "[199] loss: 2.070\n",
      "[200] loss: 2.069\n",
      "[201] loss: 2.069\n",
      "[202] loss: 2.068\n",
      "[203] loss: 2.068\n",
      "[204] loss: 2.067\n",
      "[205] loss: 2.067\n",
      "[206] loss: 2.067\n",
      "[207] loss: 2.066\n",
      "[208] loss: 2.066\n",
      "[209] loss: 2.065\n",
      "[210] loss: 2.065\n",
      "[211] loss: 2.064\n",
      "[212] loss: 2.064\n",
      "[213] loss: 2.063\n",
      "[214] loss: 2.063\n",
      "[215] loss: 2.063\n",
      "[216] loss: 2.062\n",
      "[217] loss: 2.062\n",
      "[218] loss: 2.061\n",
      "[219] loss: 2.060\n",
      "[220] loss: 2.055\n",
      "[221] loss: 2.053\n",
      "[222] loss: 2.051\n",
      "[223] loss: 2.050\n",
      "[224] loss: 2.049\n",
      "[225] loss: 2.048\n",
      "[226] loss: 2.042\n",
      "[227] loss: 2.033\n",
      "[228] loss: 2.028\n",
      "[229] loss: 2.025\n",
      "[230] loss: 2.023\n",
      "[231] loss: 2.021\n",
      "[232] loss: 2.019\n",
      "[233] loss: 2.017\n",
      "[234] loss: 2.015\n",
      "[235] loss: 2.014\n",
      "[236] loss: 2.012\n",
      "[237] loss: 2.011\n",
      "[238] loss: 2.010\n",
      "[239] loss: 2.008\n",
      "[240] loss: 2.007\n",
      "[241] loss: 2.006\n",
      "[242] loss: 2.005\n",
      "[243] loss: 2.004\n",
      "[244] loss: 2.003\n",
      "[245] loss: 2.002\n",
      "[246] loss: 2.001\n",
      "[247] loss: 2.000\n",
      "[248] loss: 1.999\n",
      "[249] loss: 1.998\n",
      "[250] loss: 1.997\n",
      "[251] loss: 1.996\n",
      "[252] loss: 1.995\n",
      "[253] loss: 1.994\n",
      "[254] loss: 1.993\n",
      "[255] loss: 1.992\n",
      "[256] loss: 1.991\n",
      "[257] loss: 1.990\n",
      "[258] loss: 1.989\n",
      "[259] loss: 1.988\n",
      "[260] loss: 1.987\n",
      "[261] loss: 1.986\n",
      "[262] loss: 1.985\n",
      "[263] loss: 1.984\n",
      "[264] loss: 1.983\n",
      "[265] loss: 1.982\n",
      "[266] loss: 1.981\n",
      "[267] loss: 1.980\n",
      "[268] loss: 1.979\n",
      "[269] loss: 1.978\n",
      "[270] loss: 1.977\n",
      "[271] loss: 1.976\n",
      "[272] loss: 1.976\n",
      "[273] loss: 1.975\n",
      "[274] loss: 1.974\n",
      "[275] loss: 1.973\n",
      "[276] loss: 1.972\n",
      "[277] loss: 1.971\n",
      "[278] loss: 1.970\n",
      "[279] loss: 1.969\n",
      "[280] loss: 1.968\n",
      "[281] loss: 1.967\n",
      "[282] loss: 1.966\n",
      "[283] loss: 1.965\n",
      "[284] loss: 1.964\n",
      "[285] loss: 1.963\n",
      "[286] loss: 1.962\n",
      "[287] loss: 1.961\n",
      "[288] loss: 1.960\n",
      "[289] loss: 1.959\n",
      "[290] loss: 1.958\n",
      "[291] loss: 1.957\n",
      "[292] loss: 1.956\n",
      "[293] loss: 1.955\n",
      "[294] loss: 1.954\n",
      "[295] loss: 1.953\n",
      "[296] loss: 1.952\n",
      "[297] loss: 1.951\n",
      "[298] loss: 1.950\n",
      "[299] loss: 1.949\n",
      "[300] loss: 1.948\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "# Treinamento por 300 Épocas\n",
    "train_model(model, 300, train_loader, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Cross Entropy durante o Treinamento\")\n",
    "plt.tight_layout()\n",
    "_ = plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(results):\n",
    "    results = results.cpu().detach().numpy().tolist()[0]\n",
    "    return results.index(max(results))\n",
    "\n",
    "def evaluate(model, dataset):\n",
    "    inputs_list = []\n",
    "    labels_list = []\n",
    "    for _, (inputs, labels) in enumerate(dataset):\n",
    "        inputs_list.append(inputs.to(device))\n",
    "        labels_list.append(labels.to(device))\n",
    "\n",
    "    acuracia = 0\n",
    "    results = [0,0,0,0,0,0,0,0,0,0]\n",
    "    for i, (inputs, labels) in enumerate(zip(inputs_list, labels_list), 0):\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        result = one_hot(y_pred)\n",
    "\n",
    "        if result == labels.item():\n",
    "            acuracia += 1\n",
    "\n",
    "        results[result] += 1\n",
    "        \n",
    "    print(acuracia / len(dataset) * 100, \"%\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=1)\n",
    "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do Modelo no dataset de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.881999999999998 %\n",
      "[5093, 5488, 6643, 3094, 4560, 6352, 2321, 3279, 7076, 6094]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do Modelo (conjunto de teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.65 %\n",
      "[1003, 1110, 1421, 627, 864, 1214, 465, 651, 1473, 1172]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> É possível observar um desempenho muito melhor que utilizar apenas uma camada de perceptrons. Mas podemos deixar o modelo treinar por mais tempo para ver o que acontece... </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 1.947\n",
      "[2] loss: 1.946\n",
      "[3] loss: 1.945\n",
      "[4] loss: 1.944\n",
      "[5] loss: 1.943\n",
      "[6] loss: 1.942\n",
      "[7] loss: 1.941\n",
      "[8] loss: 1.940\n",
      "[9] loss: 1.939\n",
      "[10] loss: 1.939\n",
      "[11] loss: 1.938\n",
      "[12] loss: 1.937\n",
      "[13] loss: 1.936\n",
      "[14] loss: 1.935\n",
      "[15] loss: 1.934\n",
      "[16] loss: 1.933\n",
      "[17] loss: 1.932\n",
      "[18] loss: 1.931\n",
      "[19] loss: 1.930\n",
      "[20] loss: 1.929\n",
      "[21] loss: 1.928\n",
      "[22] loss: 1.927\n",
      "[23] loss: 1.926\n",
      "[24] loss: 1.925\n",
      "[25] loss: 1.924\n",
      "[26] loss: 1.923\n",
      "[27] loss: 1.922\n",
      "[28] loss: 1.921\n",
      "[29] loss: 1.920\n",
      "[30] loss: 1.920\n",
      "[31] loss: 1.919\n",
      "[32] loss: 1.918\n",
      "[33] loss: 1.917\n",
      "[34] loss: 1.916\n",
      "[35] loss: 1.915\n",
      "[36] loss: 1.914\n",
      "[37] loss: 1.913\n",
      "[38] loss: 1.912\n",
      "[39] loss: 1.911\n",
      "[40] loss: 1.910\n",
      "[41] loss: 1.910\n",
      "[42] loss: 1.909\n",
      "[43] loss: 1.908\n",
      "[44] loss: 1.907\n",
      "[45] loss: 1.906\n",
      "[46] loss: 1.905\n",
      "[47] loss: 1.904\n",
      "[48] loss: 1.903\n",
      "[49] loss: 1.902\n",
      "[50] loss: 1.902\n",
      "[51] loss: 1.901\n",
      "[52] loss: 1.900\n",
      "[53] loss: 1.899\n",
      "[54] loss: 1.898\n",
      "[55] loss: 1.897\n",
      "[56] loss: 1.896\n",
      "[57] loss: 1.896\n",
      "[58] loss: 1.895\n",
      "[59] loss: 1.894\n",
      "[60] loss: 1.893\n",
      "[61] loss: 1.892\n",
      "[62] loss: 1.891\n",
      "[63] loss: 1.890\n",
      "[64] loss: 1.890\n",
      "[65] loss: 1.889\n",
      "[66] loss: 1.888\n",
      "[67] loss: 1.887\n",
      "[68] loss: 1.886\n",
      "[69] loss: 1.885\n",
      "[70] loss: 1.885\n",
      "[71] loss: 1.884\n",
      "[72] loss: 1.883\n",
      "[73] loss: 1.882\n",
      "[74] loss: 1.881\n",
      "[75] loss: 1.880\n",
      "[76] loss: 1.880\n",
      "[77] loss: 1.879\n",
      "[78] loss: 1.878\n",
      "[79] loss: 1.877\n",
      "[80] loss: 1.876\n",
      "[81] loss: 1.875\n",
      "[82] loss: 1.875\n",
      "[83] loss: 1.874\n",
      "[84] loss: 1.873\n",
      "[85] loss: 1.872\n",
      "[86] loss: 1.871\n",
      "[87] loss: 1.871\n",
      "[88] loss: 1.870\n",
      "[89] loss: 1.869\n",
      "[90] loss: 1.868\n",
      "[91] loss: 1.867\n",
      "[92] loss: 1.867\n",
      "[93] loss: 1.866\n",
      "[94] loss: 1.865\n",
      "[95] loss: 1.864\n",
      "[96] loss: 1.863\n",
      "[97] loss: 1.863\n",
      "[98] loss: 1.862\n",
      "[99] loss: 1.861\n",
      "[100] loss: 1.860\n",
      "[101] loss: 1.859\n",
      "[102] loss: 1.859\n",
      "[103] loss: 1.858\n",
      "[104] loss: 1.857\n",
      "[105] loss: 1.856\n",
      "[106] loss: 1.855\n",
      "[107] loss: 1.855\n",
      "[108] loss: 1.854\n",
      "[109] loss: 1.853\n",
      "[110] loss: 1.852\n",
      "[111] loss: 1.852\n",
      "[112] loss: 1.851\n",
      "[113] loss: 1.850\n",
      "[114] loss: 1.849\n",
      "[115] loss: 1.848\n",
      "[116] loss: 1.848\n",
      "[117] loss: 1.847\n",
      "[118] loss: 1.846\n",
      "[119] loss: 1.845\n",
      "[120] loss: 1.845\n",
      "[121] loss: 1.844\n",
      "[122] loss: 1.843\n",
      "[123] loss: 1.842\n",
      "[124] loss: 1.842\n",
      "[125] loss: 1.841\n",
      "[126] loss: 1.840\n",
      "[127] loss: 1.839\n",
      "[128] loss: 1.838\n",
      "[129] loss: 1.838\n",
      "[130] loss: 1.837\n",
      "[131] loss: 1.836\n",
      "[132] loss: 1.835\n",
      "[133] loss: 1.835\n",
      "[134] loss: 1.834\n",
      "[135] loss: 1.833\n",
      "[136] loss: 1.832\n",
      "[137] loss: 1.832\n",
      "[138] loss: 1.831\n",
      "[139] loss: 1.830\n",
      "[140] loss: 1.829\n",
      "[141] loss: 1.829\n",
      "[142] loss: 1.828\n",
      "[143] loss: 1.827\n",
      "[144] loss: 1.826\n",
      "[145] loss: 1.826\n",
      "[146] loss: 1.825\n",
      "[147] loss: 1.824\n",
      "[148] loss: 1.824\n",
      "[149] loss: 1.823\n",
      "[150] loss: 1.822\n",
      "[151] loss: 1.821\n",
      "[152] loss: 1.821\n",
      "[153] loss: 1.820\n",
      "[154] loss: 1.819\n",
      "[155] loss: 1.818\n",
      "[156] loss: 1.818\n",
      "[157] loss: 1.817\n",
      "[158] loss: 1.816\n",
      "[159] loss: 1.815\n",
      "[160] loss: 1.815\n",
      "[161] loss: 1.814\n",
      "[162] loss: 1.813\n",
      "[163] loss: 1.813\n",
      "[164] loss: 1.812\n",
      "[165] loss: 1.811\n",
      "[166] loss: 1.810\n",
      "[167] loss: 1.810\n",
      "[168] loss: 1.809\n",
      "[169] loss: 1.808\n",
      "[170] loss: 1.808\n",
      "[171] loss: 1.807\n",
      "[172] loss: 1.806\n",
      "[173] loss: 1.805\n",
      "[174] loss: 1.805\n",
      "[175] loss: 1.804\n",
      "[176] loss: 1.803\n",
      "[177] loss: 1.803\n",
      "[178] loss: 1.802\n",
      "[179] loss: 1.801\n",
      "[180] loss: 1.800\n",
      "[181] loss: 1.800\n",
      "[182] loss: 1.799\n",
      "[183] loss: 1.798\n",
      "[184] loss: 1.798\n",
      "[185] loss: 1.797\n",
      "[186] loss: 1.796\n",
      "[187] loss: 1.796\n",
      "[188] loss: 1.795\n",
      "[189] loss: 1.794\n",
      "[190] loss: 1.794\n",
      "[191] loss: 1.793\n",
      "[192] loss: 1.792\n",
      "[193] loss: 1.791\n",
      "[194] loss: 1.791\n",
      "[195] loss: 1.790\n",
      "[196] loss: 1.789\n",
      "[197] loss: 1.789\n",
      "[198] loss: 1.788\n",
      "[199] loss: 1.787\n",
      "[200] loss: 1.787\n",
      "[201] loss: 1.786\n",
      "[202] loss: 1.785\n",
      "[203] loss: 1.785\n",
      "[204] loss: 1.784\n",
      "[205] loss: 1.783\n",
      "[206] loss: 1.783\n",
      "[207] loss: 1.782\n",
      "[208] loss: 1.781\n",
      "[209] loss: 1.781\n",
      "[210] loss: 1.780\n",
      "[211] loss: 1.779\n",
      "[212] loss: 1.779\n",
      "[213] loss: 1.778\n",
      "[214] loss: 1.777\n",
      "[215] loss: 1.777\n",
      "[216] loss: 1.776\n",
      "[217] loss: 1.775\n",
      "[218] loss: 1.775\n",
      "[219] loss: 1.774\n",
      "[220] loss: 1.773\n",
      "[221] loss: 1.773\n",
      "[222] loss: 1.772\n",
      "[223] loss: 1.771\n",
      "[224] loss: 1.771\n",
      "[225] loss: 1.770\n",
      "[226] loss: 1.769\n",
      "[227] loss: 1.769\n",
      "[228] loss: 1.768\n",
      "[229] loss: 1.767\n",
      "[230] loss: 1.767\n",
      "[231] loss: 1.766\n",
      "[232] loss: 1.765\n",
      "[233] loss: 1.765\n",
      "[234] loss: 1.764\n",
      "[235] loss: 1.763\n",
      "[236] loss: 1.763\n",
      "[237] loss: 1.762\n",
      "[238] loss: 1.761\n",
      "[239] loss: 1.761\n",
      "[240] loss: 1.760\n",
      "[241] loss: 1.760\n",
      "[242] loss: 1.759\n",
      "[243] loss: 1.758\n",
      "[244] loss: 1.758\n",
      "[245] loss: 1.757\n",
      "[246] loss: 1.756\n",
      "[247] loss: 1.756\n",
      "[248] loss: 1.755\n",
      "[249] loss: 1.754\n",
      "[250] loss: 1.754\n",
      "[251] loss: 1.753\n",
      "[252] loss: 1.752\n",
      "[253] loss: 1.752\n",
      "[254] loss: 1.751\n",
      "[255] loss: 1.751\n",
      "[256] loss: 1.750\n",
      "[257] loss: 1.749\n",
      "[258] loss: 1.749\n",
      "[259] loss: 1.748\n",
      "[260] loss: 1.747\n",
      "[261] loss: 1.747\n",
      "[262] loss: 1.746\n",
      "[263] loss: 1.746\n",
      "[264] loss: 1.745\n",
      "[265] loss: 1.744\n",
      "[266] loss: 1.744\n",
      "[267] loss: 1.743\n",
      "[268] loss: 1.742\n",
      "[269] loss: 1.742\n",
      "[270] loss: 1.741\n",
      "[271] loss: 1.741\n",
      "[272] loss: 1.740\n",
      "[273] loss: 1.739\n",
      "[274] loss: 1.739\n",
      "[275] loss: 1.738\n",
      "[276] loss: 1.737\n",
      "[277] loss: 1.737\n",
      "[278] loss: 1.736\n",
      "[279] loss: 1.736\n",
      "[280] loss: 1.735\n",
      "[281] loss: 1.734\n",
      "[282] loss: 1.734\n",
      "[283] loss: 1.733\n",
      "[284] loss: 1.733\n",
      "[285] loss: 1.732\n",
      "[286] loss: 1.731\n",
      "[287] loss: 1.731\n",
      "[288] loss: 1.730\n",
      "[289] loss: 1.729\n",
      "[290] loss: 1.729\n",
      "[291] loss: 1.728\n",
      "[292] loss: 1.728\n",
      "[293] loss: 1.727\n",
      "[294] loss: 1.726\n",
      "[295] loss: 1.726\n",
      "[296] loss: 1.725\n",
      "[297] loss: 1.725\n",
      "[298] loss: 1.724\n",
      "[299] loss: 1.723\n",
      "[300] loss: 1.723\n",
      "[301] loss: 1.722\n",
      "[302] loss: 1.722\n",
      "[303] loss: 1.721\n",
      "[304] loss: 1.720\n",
      "[305] loss: 1.720\n",
      "[306] loss: 1.719\n",
      "[307] loss: 1.719\n",
      "[308] loss: 1.718\n",
      "[309] loss: 1.717\n",
      "[310] loss: 1.717\n",
      "[311] loss: 1.716\n",
      "[312] loss: 1.716\n",
      "[313] loss: 1.715\n",
      "[314] loss: 1.714\n",
      "[315] loss: 1.714\n",
      "[316] loss: 1.713\n",
      "[317] loss: 1.713\n",
      "[318] loss: 1.712\n",
      "[319] loss: 1.711\n",
      "[320] loss: 1.711\n",
      "[321] loss: 1.710\n",
      "[322] loss: 1.710\n",
      "[323] loss: 1.709\n",
      "[324] loss: 1.709\n",
      "[325] loss: 1.708\n",
      "[326] loss: 1.707\n",
      "[327] loss: 1.707\n",
      "[328] loss: 1.706\n",
      "[329] loss: 1.706\n",
      "[330] loss: 1.705\n",
      "[331] loss: 1.704\n",
      "[332] loss: 1.704\n",
      "[333] loss: 1.703\n",
      "[334] loss: 1.703\n",
      "[335] loss: 1.702\n",
      "[336] loss: 1.702\n",
      "[337] loss: 1.701\n",
      "[338] loss: 1.700\n",
      "[339] loss: 1.700\n",
      "[340] loss: 1.699\n",
      "[341] loss: 1.699\n",
      "[342] loss: 1.698\n",
      "[343] loss: 1.697\n",
      "[344] loss: 1.697\n",
      "[345] loss: 1.696\n",
      "[346] loss: 1.696\n",
      "[347] loss: 1.695\n",
      "[348] loss: 1.695\n",
      "[349] loss: 1.694\n",
      "[350] loss: 1.693\n",
      "[351] loss: 1.693\n",
      "[352] loss: 1.692\n",
      "[353] loss: 1.692\n",
      "[354] loss: 1.691\n",
      "[355] loss: 1.691\n",
      "[356] loss: 1.690\n",
      "[357] loss: 1.689\n",
      "[358] loss: 1.689\n",
      "[359] loss: 1.688\n",
      "[360] loss: 1.688\n",
      "[361] loss: 1.687\n",
      "[362] loss: 1.687\n",
      "[363] loss: 1.686\n",
      "[364] loss: 1.685\n",
      "[365] loss: 1.685\n",
      "[366] loss: 1.684\n",
      "[367] loss: 1.684\n",
      "[368] loss: 1.683\n",
      "[369] loss: 1.683\n",
      "[370] loss: 1.682\n",
      "[371] loss: 1.681\n",
      "[372] loss: 1.681\n",
      "[373] loss: 1.680\n",
      "[374] loss: 1.680\n",
      "[375] loss: 1.679\n",
      "[376] loss: 1.679\n",
      "[377] loss: 1.678\n",
      "[378] loss: 1.678\n",
      "[379] loss: 1.677\n",
      "[380] loss: 1.676\n",
      "[381] loss: 1.676\n",
      "[382] loss: 1.675\n",
      "[383] loss: 1.675\n",
      "[384] loss: 1.674\n",
      "[385] loss: 1.674\n",
      "[386] loss: 1.673\n",
      "[387] loss: 1.672\n",
      "[388] loss: 1.672\n",
      "[389] loss: 1.671\n",
      "[390] loss: 1.671\n",
      "[391] loss: 1.670\n",
      "[392] loss: 1.670\n",
      "[393] loss: 1.669\n",
      "[394] loss: 1.669\n",
      "[395] loss: 1.668\n",
      "[396] loss: 1.667\n",
      "[397] loss: 1.667\n",
      "[398] loss: 1.666\n",
      "[399] loss: 1.666\n",
      "[400] loss: 1.665\n",
      "[401] loss: 1.665\n",
      "[402] loss: 1.664\n",
      "[403] loss: 1.664\n",
      "[404] loss: 1.663\n",
      "[405] loss: 1.663\n",
      "[406] loss: 1.662\n",
      "[407] loss: 1.661\n",
      "[408] loss: 1.661\n",
      "[409] loss: 1.660\n",
      "[410] loss: 1.660\n",
      "[411] loss: 1.659\n",
      "[412] loss: 1.659\n",
      "[413] loss: 1.658\n",
      "[414] loss: 1.658\n",
      "[415] loss: 1.657\n",
      "[416] loss: 1.657\n",
      "[417] loss: 1.656\n",
      "[418] loss: 1.655\n",
      "[419] loss: 1.655\n",
      "[420] loss: 1.654\n",
      "[421] loss: 1.654\n",
      "[422] loss: 1.653\n",
      "[423] loss: 1.653\n",
      "[424] loss: 1.652\n",
      "[425] loss: 1.652\n",
      "[426] loss: 1.651\n",
      "[427] loss: 1.651\n",
      "[428] loss: 1.650\n",
      "[429] loss: 1.650\n",
      "[430] loss: 1.649\n",
      "[431] loss: 1.649\n",
      "[432] loss: 1.648\n",
      "[433] loss: 1.647\n",
      "[434] loss: 1.647\n",
      "[435] loss: 1.646\n",
      "[436] loss: 1.646\n",
      "[437] loss: 1.645\n",
      "[438] loss: 1.645\n",
      "[439] loss: 1.644\n",
      "[440] loss: 1.644\n",
      "[441] loss: 1.643\n",
      "[442] loss: 1.643\n",
      "[443] loss: 1.642\n",
      "[444] loss: 1.642\n",
      "[445] loss: 1.641\n",
      "[446] loss: 1.641\n",
      "[447] loss: 1.640\n",
      "[448] loss: 1.640\n",
      "[449] loss: 1.639\n",
      "[450] loss: 1.639\n",
      "[451] loss: 1.638\n",
      "[452] loss: 1.637\n",
      "[453] loss: 1.637\n",
      "[454] loss: 1.636\n",
      "[455] loss: 1.636\n",
      "[456] loss: 1.635\n",
      "[457] loss: 1.635\n",
      "[458] loss: 1.634\n",
      "[459] loss: 1.634\n",
      "[460] loss: 1.633\n",
      "[461] loss: 1.633\n",
      "[462] loss: 1.632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[463] loss: 1.632\n",
      "[464] loss: 1.631\n",
      "[465] loss: 1.631\n",
      "[466] loss: 1.630\n",
      "[467] loss: 1.630\n",
      "[468] loss: 1.629\n",
      "[469] loss: 1.629\n",
      "[470] loss: 1.628\n",
      "[471] loss: 1.628\n",
      "[472] loss: 1.627\n",
      "[473] loss: 1.627\n",
      "[474] loss: 1.626\n",
      "[475] loss: 1.626\n",
      "[476] loss: 1.625\n",
      "[477] loss: 1.625\n",
      "[478] loss: 1.624\n",
      "[479] loss: 1.624\n",
      "[480] loss: 1.623\n",
      "[481] loss: 1.623\n",
      "[482] loss: 1.622\n",
      "[483] loss: 1.622\n",
      "[484] loss: 1.621\n",
      "[485] loss: 1.621\n",
      "[486] loss: 1.620\n",
      "[487] loss: 1.620\n",
      "[488] loss: 1.619\n",
      "[489] loss: 1.619\n",
      "[490] loss: 1.618\n",
      "[491] loss: 1.618\n",
      "[492] loss: 1.617\n",
      "[493] loss: 1.617\n",
      "[494] loss: 1.616\n",
      "[495] loss: 1.616\n",
      "[496] loss: 1.615\n",
      "[497] loss: 1.615\n",
      "[498] loss: 1.614\n",
      "[499] loss: 1.614\n",
      "[500] loss: 1.613\n",
      "[501] loss: 1.613\n",
      "[502] loss: 1.612\n",
      "[503] loss: 1.612\n",
      "[504] loss: 1.611\n",
      "[505] loss: 1.611\n",
      "[506] loss: 1.610\n",
      "[507] loss: 1.610\n",
      "[508] loss: 1.609\n",
      "[509] loss: 1.609\n",
      "[510] loss: 1.608\n",
      "[511] loss: 1.608\n",
      "[512] loss: 1.607\n",
      "[513] loss: 1.607\n",
      "[514] loss: 1.606\n",
      "[515] loss: 1.606\n",
      "[516] loss: 1.605\n",
      "[517] loss: 1.605\n",
      "[518] loss: 1.604\n",
      "[519] loss: 1.604\n",
      "[520] loss: 1.603\n",
      "[521] loss: 1.603\n",
      "[522] loss: 1.602\n",
      "[523] loss: 1.602\n",
      "[524] loss: 1.601\n",
      "[525] loss: 1.601\n",
      "[526] loss: 1.600\n",
      "[527] loss: 1.600\n",
      "[528] loss: 1.599\n",
      "[529] loss: 1.599\n",
      "[530] loss: 1.598\n",
      "[531] loss: 1.598\n",
      "[532] loss: 1.597\n",
      "[533] loss: 1.597\n",
      "[534] loss: 1.596\n",
      "[535] loss: 1.596\n",
      "[536] loss: 1.595\n",
      "[537] loss: 1.595\n",
      "[538] loss: 1.594\n",
      "[539] loss: 1.594\n",
      "[540] loss: 1.593\n",
      "[541] loss: 1.593\n",
      "[542] loss: 1.593\n",
      "[543] loss: 1.592\n",
      "[544] loss: 1.592\n",
      "[545] loss: 1.591\n",
      "[546] loss: 1.591\n",
      "[547] loss: 1.590\n",
      "[548] loss: 1.590\n",
      "[549] loss: 1.589\n",
      "[550] loss: 1.589\n",
      "[551] loss: 1.588\n",
      "[552] loss: 1.588\n",
      "[553] loss: 1.587\n",
      "[554] loss: 1.587\n",
      "[555] loss: 1.586\n",
      "[556] loss: 1.586\n",
      "[557] loss: 1.585\n",
      "[558] loss: 1.585\n",
      "[559] loss: 1.584\n",
      "[560] loss: 1.584\n",
      "[561] loss: 1.583\n",
      "[562] loss: 1.583\n",
      "[563] loss: 1.583\n",
      "[564] loss: 1.582\n",
      "[565] loss: 1.582\n",
      "[566] loss: 1.581\n",
      "[567] loss: 1.581\n",
      "[568] loss: 1.580\n",
      "[569] loss: 1.580\n",
      "[570] loss: 1.579\n",
      "[571] loss: 1.579\n",
      "[572] loss: 1.578\n",
      "[573] loss: 1.578\n",
      "[574] loss: 1.577\n",
      "[575] loss: 1.577\n",
      "[576] loss: 1.576\n",
      "[577] loss: 1.576\n",
      "[578] loss: 1.575\n",
      "[579] loss: 1.575\n",
      "[580] loss: 1.575\n",
      "[581] loss: 1.574\n",
      "[582] loss: 1.574\n",
      "[583] loss: 1.573\n",
      "[584] loss: 1.573\n",
      "[585] loss: 1.572\n",
      "[586] loss: 1.572\n",
      "[587] loss: 1.571\n",
      "[588] loss: 1.571\n",
      "[589] loss: 1.570\n",
      "[590] loss: 1.570\n",
      "[591] loss: 1.569\n",
      "[592] loss: 1.569\n",
      "[593] loss: 1.569\n",
      "[594] loss: 1.568\n",
      "[595] loss: 1.568\n",
      "[596] loss: 1.567\n",
      "[597] loss: 1.567\n",
      "[598] loss: 1.566\n",
      "[599] loss: 1.566\n",
      "[600] loss: 1.565\n",
      "[601] loss: 1.565\n",
      "[602] loss: 1.564\n",
      "[603] loss: 1.564\n",
      "[604] loss: 1.563\n",
      "[605] loss: 1.563\n",
      "[606] loss: 1.563\n",
      "[607] loss: 1.562\n",
      "[608] loss: 1.562\n",
      "[609] loss: 1.561\n",
      "[610] loss: 1.561\n",
      "[611] loss: 1.560\n",
      "[612] loss: 1.560\n",
      "[613] loss: 1.559\n",
      "[614] loss: 1.559\n",
      "[615] loss: 1.558\n",
      "[616] loss: 1.558\n",
      "[617] loss: 1.557\n",
      "[618] loss: 1.557\n",
      "[619] loss: 1.557\n",
      "[620] loss: 1.556\n",
      "[621] loss: 1.556\n",
      "[622] loss: 1.555\n",
      "[623] loss: 1.555\n",
      "[624] loss: 1.554\n",
      "[625] loss: 1.554\n",
      "[626] loss: 1.553\n",
      "[627] loss: 1.553\n",
      "[628] loss: 1.552\n",
      "[629] loss: 1.552\n",
      "[630] loss: 1.552\n",
      "[631] loss: 1.551\n",
      "[632] loss: 1.551\n",
      "[633] loss: 1.550\n",
      "[634] loss: 1.550\n",
      "[635] loss: 1.549\n",
      "[636] loss: 1.549\n",
      "[637] loss: 1.548\n",
      "[638] loss: 1.548\n",
      "[639] loss: 1.547\n",
      "[640] loss: 1.547\n",
      "[641] loss: 1.547\n",
      "[642] loss: 1.546\n",
      "[643] loss: 1.546\n",
      "[644] loss: 1.545\n",
      "[645] loss: 1.545\n",
      "[646] loss: 1.544\n",
      "[647] loss: 1.544\n",
      "[648] loss: 1.543\n",
      "[649] loss: 1.543\n",
      "[650] loss: 1.543\n",
      "[651] loss: 1.542\n",
      "[652] loss: 1.542\n",
      "[653] loss: 1.541\n",
      "[654] loss: 1.541\n",
      "[655] loss: 1.540\n",
      "[656] loss: 1.540\n",
      "[657] loss: 1.539\n",
      "[658] loss: 1.539\n",
      "[659] loss: 1.538\n",
      "[660] loss: 1.538\n",
      "[661] loss: 1.538\n",
      "[662] loss: 1.537\n",
      "[663] loss: 1.537\n",
      "[664] loss: 1.536\n",
      "[665] loss: 1.536\n",
      "[666] loss: 1.535\n",
      "[667] loss: 1.535\n",
      "[668] loss: 1.534\n",
      "[669] loss: 1.534\n",
      "[670] loss: 1.534\n",
      "[671] loss: 1.533\n",
      "[672] loss: 1.533\n",
      "[673] loss: 1.532\n",
      "[674] loss: 1.532\n",
      "[675] loss: 1.531\n",
      "[676] loss: 1.531\n",
      "[677] loss: 1.530\n",
      "[678] loss: 1.530\n",
      "[679] loss: 1.530\n",
      "[680] loss: 1.529\n",
      "[681] loss: 1.529\n",
      "[682] loss: 1.528\n",
      "[683] loss: 1.528\n",
      "[684] loss: 1.527\n",
      "[685] loss: 1.527\n",
      "[686] loss: 1.526\n",
      "[687] loss: 1.526\n",
      "[688] loss: 1.526\n",
      "[689] loss: 1.525\n",
      "[690] loss: 1.525\n",
      "[691] loss: 1.524\n",
      "[692] loss: 1.524\n",
      "[693] loss: 1.523\n",
      "[694] loss: 1.523\n",
      "[695] loss: 1.523\n",
      "[696] loss: 1.522\n",
      "[697] loss: 1.522\n",
      "[698] loss: 1.521\n",
      "[699] loss: 1.521\n",
      "[700] loss: 1.520\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=200)\n",
    "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=200)\n",
    "\n",
    "losses = []\n",
    "# Treinamento por mais 700 Épocas\n",
    "train_model(model, 700, train_loader, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Cross Entropy durante o Treinamento\")\n",
    "plt.tight_layout()\n",
    "_ = plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=1)\n",
    "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do modelo nos datasets de Treino e Teste, respectivamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.745999999999995 %\n",
      "[4304, 4742, 4866, 4342, 4933, 4772, 6283, 4669, 5135, 5954]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.29 %\n",
      "[832, 905, 1077, 865, 981, 972, 1253, 867, 1044, 1204]\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Podemos observar que o modelo melhorou muito seu desempenho com os dados de treino, mas não com os de teste (inclusive piorou).</h3>\n",
    "\n",
    "<h2> E se reduzirmos a complexidade do nosso modelo?</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a arquitetura MLP\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32, 80)\n",
    "        self.fc2 = nn.Linear(80, 10)\n",
    "        self.activation_function = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32)\n",
    "        x = self.activation_function(self.fc1(x))\n",
    "        x = self.activation_function(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 2.285\n",
      "[2] loss: 2.251\n",
      "[3] loss: 2.224\n",
      "[4] loss: 2.206\n",
      "[5] loss: 2.194\n",
      "[6] loss: 2.174\n",
      "[7] loss: 2.160\n",
      "[8] loss: 2.149\n",
      "[9] loss: 2.130\n",
      "[10] loss: 2.103\n",
      "[11] loss: 2.086\n",
      "[12] loss: 2.075\n",
      "[13] loss: 2.066\n",
      "[14] loss: 2.059\n",
      "[15] loss: 2.053\n",
      "[16] loss: 2.038\n",
      "[17] loss: 2.016\n",
      "[18] loss: 2.008\n",
      "[19] loss: 2.001\n",
      "[20] loss: 1.995\n",
      "[21] loss: 1.988\n",
      "[22] loss: 1.981\n",
      "[23] loss: 1.975\n",
      "[24] loss: 1.968\n",
      "[25] loss: 1.961\n",
      "[26] loss: 1.954\n",
      "[27] loss: 1.947\n",
      "[28] loss: 1.940\n",
      "[29] loss: 1.934\n",
      "[30] loss: 1.927\n",
      "[31] loss: 1.921\n",
      "[32] loss: 1.914\n",
      "[33] loss: 1.908\n",
      "[34] loss: 1.902\n",
      "[35] loss: 1.896\n",
      "[36] loss: 1.891\n",
      "[37] loss: 1.885\n",
      "[38] loss: 1.879\n",
      "[39] loss: 1.874\n",
      "[40] loss: 1.869\n",
      "[41] loss: 1.863\n",
      "[42] loss: 1.858\n",
      "[43] loss: 1.853\n",
      "[44] loss: 1.848\n",
      "[45] loss: 1.844\n",
      "[46] loss: 1.839\n",
      "[47] loss: 1.834\n",
      "[48] loss: 1.830\n",
      "[49] loss: 1.826\n",
      "[50] loss: 1.822\n",
      "[51] loss: 1.818\n",
      "[52] loss: 1.814\n",
      "[53] loss: 1.810\n",
      "[54] loss: 1.806\n",
      "[55] loss: 1.802\n",
      "[56] loss: 1.799\n",
      "[57] loss: 1.795\n",
      "[58] loss: 1.792\n",
      "[59] loss: 1.788\n",
      "[60] loss: 1.785\n",
      "[61] loss: 1.782\n",
      "[62] loss: 1.779\n",
      "[63] loss: 1.775\n",
      "[64] loss: 1.772\n",
      "[65] loss: 1.769\n",
      "[66] loss: 1.766\n",
      "[67] loss: 1.763\n",
      "[68] loss: 1.760\n",
      "[69] loss: 1.757\n",
      "[70] loss: 1.754\n",
      "[71] loss: 1.752\n",
      "[72] loss: 1.749\n",
      "[73] loss: 1.746\n",
      "[74] loss: 1.743\n",
      "[75] loss: 1.741\n",
      "[76] loss: 1.738\n",
      "[77] loss: 1.735\n",
      "[78] loss: 1.733\n",
      "[79] loss: 1.730\n",
      "[80] loss: 1.728\n",
      "[81] loss: 1.725\n",
      "[82] loss: 1.723\n",
      "[83] loss: 1.720\n",
      "[84] loss: 1.718\n",
      "[85] loss: 1.716\n",
      "[86] loss: 1.713\n",
      "[87] loss: 1.711\n",
      "[88] loss: 1.709\n",
      "[89] loss: 1.707\n",
      "[90] loss: 1.704\n",
      "[91] loss: 1.702\n",
      "[92] loss: 1.700\n",
      "[93] loss: 1.698\n",
      "[94] loss: 1.696\n",
      "[95] loss: 1.694\n",
      "[96] loss: 1.692\n",
      "[97] loss: 1.690\n",
      "[98] loss: 1.688\n",
      "[99] loss: 1.686\n",
      "[100] loss: 1.684\n",
      "[101] loss: 1.682\n",
      "[102] loss: 1.680\n",
      "[103] loss: 1.678\n",
      "[104] loss: 1.676\n",
      "[105] loss: 1.674\n",
      "[106] loss: 1.672\n",
      "[107] loss: 1.670\n",
      "[108] loss: 1.668\n",
      "[109] loss: 1.667\n",
      "[110] loss: 1.665\n",
      "[111] loss: 1.663\n",
      "[112] loss: 1.661\n",
      "[113] loss: 1.659\n",
      "[114] loss: 1.657\n",
      "[115] loss: 1.656\n",
      "[116] loss: 1.654\n",
      "[117] loss: 1.652\n",
      "[118] loss: 1.650\n",
      "[119] loss: 1.649\n",
      "[120] loss: 1.647\n",
      "[121] loss: 1.645\n",
      "[122] loss: 1.643\n",
      "[123] loss: 1.641\n",
      "[124] loss: 1.640\n",
      "[125] loss: 1.638\n",
      "[126] loss: 1.636\n",
      "[127] loss: 1.635\n",
      "[128] loss: 1.633\n",
      "[129] loss: 1.631\n",
      "[130] loss: 1.630\n",
      "[131] loss: 1.628\n",
      "[132] loss: 1.626\n",
      "[133] loss: 1.625\n",
      "[134] loss: 1.623\n",
      "[135] loss: 1.621\n",
      "[136] loss: 1.620\n",
      "[137] loss: 1.618\n",
      "[138] loss: 1.617\n",
      "[139] loss: 1.615\n",
      "[140] loss: 1.613\n",
      "[141] loss: 1.612\n",
      "[142] loss: 1.610\n",
      "[143] loss: 1.609\n",
      "[144] loss: 1.607\n",
      "[145] loss: 1.605\n",
      "[146] loss: 1.604\n",
      "[147] loss: 1.602\n",
      "[148] loss: 1.601\n",
      "[149] loss: 1.599\n",
      "[150] loss: 1.598\n",
      "[151] loss: 1.596\n",
      "[152] loss: 1.595\n",
      "[153] loss: 1.593\n",
      "[154] loss: 1.592\n",
      "[155] loss: 1.591\n",
      "[156] loss: 1.589\n",
      "[157] loss: 1.588\n",
      "[158] loss: 1.586\n",
      "[159] loss: 1.585\n",
      "[160] loss: 1.584\n",
      "[161] loss: 1.582\n",
      "[162] loss: 1.581\n",
      "[163] loss: 1.579\n",
      "[164] loss: 1.578\n",
      "[165] loss: 1.577\n",
      "[166] loss: 1.575\n",
      "[167] loss: 1.574\n",
      "[168] loss: 1.573\n",
      "[169] loss: 1.572\n",
      "[170] loss: 1.570\n",
      "[171] loss: 1.569\n",
      "[172] loss: 1.568\n",
      "[173] loss: 1.566\n",
      "[174] loss: 1.565\n",
      "[175] loss: 1.564\n",
      "[176] loss: 1.563\n",
      "[177] loss: 1.561\n",
      "[178] loss: 1.560\n",
      "[179] loss: 1.559\n",
      "[180] loss: 1.558\n",
      "[181] loss: 1.557\n",
      "[182] loss: 1.555\n",
      "[183] loss: 1.554\n",
      "[184] loss: 1.553\n",
      "[185] loss: 1.552\n",
      "[186] loss: 1.551\n",
      "[187] loss: 1.549\n",
      "[188] loss: 1.548\n",
      "[189] loss: 1.547\n",
      "[190] loss: 1.546\n",
      "[191] loss: 1.545\n",
      "[192] loss: 1.544\n",
      "[193] loss: 1.543\n",
      "[194] loss: 1.542\n",
      "[195] loss: 1.540\n",
      "[196] loss: 1.539\n",
      "[197] loss: 1.538\n",
      "[198] loss: 1.537\n",
      "[199] loss: 1.536\n",
      "[200] loss: 1.535\n",
      "[201] loss: 1.534\n",
      "[202] loss: 1.533\n",
      "[203] loss: 1.532\n",
      "[204] loss: 1.531\n",
      "[205] loss: 1.530\n",
      "[206] loss: 1.529\n",
      "[207] loss: 1.528\n",
      "[208] loss: 1.527\n",
      "[209] loss: 1.526\n",
      "[210] loss: 1.525\n",
      "[211] loss: 1.524\n",
      "[212] loss: 1.523\n",
      "[213] loss: 1.522\n",
      "[214] loss: 1.521\n",
      "[215] loss: 1.520\n",
      "[216] loss: 1.519\n",
      "[217] loss: 1.518\n",
      "[218] loss: 1.517\n",
      "[219] loss: 1.516\n",
      "[220] loss: 1.515\n",
      "[221] loss: 1.514\n",
      "[222] loss: 1.513\n",
      "[223] loss: 1.512\n",
      "[224] loss: 1.511\n",
      "[225] loss: 1.510\n",
      "[226] loss: 1.509\n",
      "[227] loss: 1.508\n",
      "[228] loss: 1.507\n",
      "[229] loss: 1.506\n",
      "[230] loss: 1.505\n",
      "[231] loss: 1.504\n",
      "[232] loss: 1.503\n",
      "[233] loss: 1.503\n",
      "[234] loss: 1.502\n",
      "[235] loss: 1.501\n",
      "[236] loss: 1.500\n",
      "[237] loss: 1.499\n",
      "[238] loss: 1.498\n",
      "[239] loss: 1.497\n",
      "[240] loss: 1.496\n",
      "[241] loss: 1.495\n",
      "[242] loss: 1.495\n",
      "[243] loss: 1.494\n",
      "[244] loss: 1.493\n",
      "[245] loss: 1.492\n",
      "[246] loss: 1.491\n",
      "[247] loss: 1.490\n",
      "[248] loss: 1.489\n",
      "[249] loss: 1.489\n",
      "[250] loss: 1.488\n",
      "[251] loss: 1.487\n",
      "[252] loss: 1.486\n",
      "[253] loss: 1.485\n",
      "[254] loss: 1.484\n",
      "[255] loss: 1.484\n",
      "[256] loss: 1.483\n",
      "[257] loss: 1.482\n",
      "[258] loss: 1.481\n",
      "[259] loss: 1.480\n",
      "[260] loss: 1.479\n",
      "[261] loss: 1.479\n",
      "[262] loss: 1.478\n",
      "[263] loss: 1.477\n",
      "[264] loss: 1.476\n",
      "[265] loss: 1.475\n",
      "[266] loss: 1.475\n",
      "[267] loss: 1.474\n",
      "[268] loss: 1.473\n",
      "[269] loss: 1.472\n",
      "[270] loss: 1.471\n",
      "[271] loss: 1.471\n",
      "[272] loss: 1.470\n",
      "[273] loss: 1.469\n",
      "[274] loss: 1.468\n",
      "[275] loss: 1.467\n",
      "[276] loss: 1.467\n",
      "[277] loss: 1.466\n",
      "[278] loss: 1.465\n",
      "[279] loss: 1.464\n",
      "[280] loss: 1.463\n",
      "[281] loss: 1.463\n",
      "[282] loss: 1.462\n",
      "[283] loss: 1.461\n",
      "[284] loss: 1.460\n",
      "[285] loss: 1.460\n",
      "[286] loss: 1.459\n",
      "[287] loss: 1.458\n",
      "[288] loss: 1.457\n",
      "[289] loss: 1.457\n",
      "[290] loss: 1.456\n",
      "[291] loss: 1.455\n",
      "[292] loss: 1.454\n",
      "[293] loss: 1.454\n",
      "[294] loss: 1.453\n",
      "[295] loss: 1.452\n",
      "[296] loss: 1.451\n",
      "[297] loss: 1.451\n",
      "[298] loss: 1.450\n",
      "[299] loss: 1.449\n",
      "[300] loss: 1.448\n",
      "[301] loss: 1.448\n",
      "[302] loss: 1.447\n",
      "[303] loss: 1.446\n",
      "[304] loss: 1.445\n",
      "[305] loss: 1.445\n",
      "[306] loss: 1.444\n",
      "[307] loss: 1.443\n",
      "[308] loss: 1.443\n",
      "[309] loss: 1.442\n",
      "[310] loss: 1.441\n",
      "[311] loss: 1.440\n",
      "[312] loss: 1.440\n",
      "[313] loss: 1.439\n",
      "[314] loss: 1.438\n",
      "[315] loss: 1.438\n",
      "[316] loss: 1.437\n",
      "[317] loss: 1.436\n",
      "[318] loss: 1.435\n",
      "[319] loss: 1.435\n",
      "[320] loss: 1.434\n",
      "[321] loss: 1.433\n",
      "[322] loss: 1.433\n",
      "[323] loss: 1.432\n",
      "[324] loss: 1.431\n",
      "[325] loss: 1.431\n",
      "[326] loss: 1.430\n",
      "[327] loss: 1.429\n",
      "[328] loss: 1.429\n",
      "[329] loss: 1.428\n",
      "[330] loss: 1.427\n",
      "[331] loss: 1.427\n",
      "[332] loss: 1.426\n",
      "[333] loss: 1.425\n",
      "[334] loss: 1.425\n",
      "[335] loss: 1.424\n",
      "[336] loss: 1.423\n",
      "[337] loss: 1.423\n",
      "[338] loss: 1.422\n",
      "[339] loss: 1.421\n"
     ]
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2, weight_decay=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=200)\n",
    "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=200)\n",
    "\n",
    "losses = []\n",
    "train_model(model, 1000, train_loader, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Cross Entropy durante o Treinamento\")\n",
    "plt.tight_layout()\n",
    "_ = plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=1)\n",
    "test_loader = DataLoader(dataset=dataset_test, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Nota-se que o desempenho se mantem próximo ao anterior, com uma diferença menor no desempenho entre treino e teste, e com um modelo mais simples <h5>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
